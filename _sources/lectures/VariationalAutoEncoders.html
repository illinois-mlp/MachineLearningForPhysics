

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Variational AutoEncoders &#8212; PHYS 498 MLP</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_sources/lectures/VariationalAutoEncoders';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Generative Adversarial Networks" href="GenerativeAdversarialNetworks.html" />
    <link rel="prev" title="Autoencoders" href="AutoEncoders.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="PHYS 498 MLP - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="PHYS 498 MLP - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    <span style="color:Blue">Machine Learning for Physics</span>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_01.html"><span style="color: blue;"><b>Introduction to Data Science</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1cQJycGyQ07qSOoeskr6GjjTD3byIkxDbdi228NRgFeU/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="JupyterNumpy.html">Jupyter Notebooks and Numerical Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="Pandas.html">Handling Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Visualization.html">Visualizing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Clustering.html">Finding Structure in Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Dimensionality.html">Measuring and Reducing Dimensionality</a></li>
<li class="toctree-l2"><a class="reference internal" href="Nonlinear.html">Adapting Linear Methods to Non-Linear Data and Kernel Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_01.html">Homework 01: Introduction to Data Science</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_02.html"><span style="color: blue;"><b>Probability Theory and Density Estimation</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1o9tM9ppKZWIa9B3WIHy5JDF4myR5NkO02W6WAlyiTSg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityTheory.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityDistributions.html">Important Probability Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="DensityEstimation.html">Estimating Probability Density from Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_02.html">Homework 02: Probability Theory and Density Estimation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_03.html"><span style="color: blue;"><b>Bayesian Statistics I</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1h2SMuH-Z5a_OE6UMDbFjysEiL2NmYT1tGww6VF5jzsA/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Statistics.html">Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="BayesianInference.html">Bayesian Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="MarkovChainMonteCarlo.html">Markov Chain Monte Carlo in Practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="MarkovChains.html">Stochastic Processes and Markov-Chain Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_03.html">Homework 03: Bayesian Statistics and Markov Chains</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_04.html"><span style="color: blue;"><b>Bayesian Statistics II</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/18bft9_CiBLjjBy0MHvT_vN7E95kfakvhm_7d7WKHXyY/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ModelSelection.html">Bayesian Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="VariationalInference.html">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="Optimization.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="CrossValidation.html">Cross Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_04.html">Homework 04: Metropolis-Hastings and Cross Validation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_05.html"><span style="color: blue;"><b>Introduction to Artificial Intelligence</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1by3-6jDEorKi7_WEr6PTMfEBE8f4xrS94fNdtSuATVg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="SupervisedLearning.html">Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="Learning.html">Artificial Intelligence and Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ArtificialNeuralNetworks.html">Artificial Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="DeepLearning.html">Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_05.html">Homework 05: Artificial Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_06.html"><span style="color: blue;"><b>Convolutional and Recurrent Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1cDFVtEVGLaWd4256OShSb3Roto0x4y4GwG6LkhVozg0/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ConvolutionalRecurrentNeuralNetworks.html">Convolutional and Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_06.html">Homework 06: Forecasting Projectile Motion with Recurrent Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_07.html"><span style="color: blue;"><b>Geometric Deep Learning and Graph Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1jK61M3QGH7bxFU7TMBm16G3YDb-7HOFtgNdqlj3Gs38/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="GraphNeuralNetworks.html">Geometric Deep Learning and Graph Neural Networks</a></li>





</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_08.html"><span style="color: blue;"><b>Attention Mechanism and Transformers</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1ZHuK7TopASFSoyUoELKeCGT8bullhtSLcEkrp4ZueGg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Attention.html">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="Transformers.html">Transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="VisionTransformer.html">Vision Transformer</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_01.html"><span style="color: blue;"><b>Project 01</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_HiggsTauTau.html">Higgs Boson Decaying to Tau Leptons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_ExoticParticles.html">Searching for Exotic Particles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_GalaxyZoo.html">Galaxy Zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_NuclearGeometryQGP.html">Nuclear Geometry and Characterization of the Quark Gluon Plasma</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_AberratedImages.html">Aberrated Image Recovery of Ultracold Atoms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_DarkEnergySurvey.html">Dark Energy Survey</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_GravitationalWaves.html">Detection of Gravitational Waves</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../Week_09.html"><span style="color: blue;"><b>Generative Modeling</b></span></a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1h13YeUjtTU_WHLxghxFBBQJO3uRr1GtsIyO4DVZviJo/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="GenerativeModeling.html">Generative Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="AutoEncoders.html">Autoencoders</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Variational AutoEncoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="GenerativeAdversarialNetworks.html">Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="Diffusion.html">Diffusion Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="NormalizingFlows.html">Normalizing Flows</a></li>
<li class="toctree-l2"><a class="reference internal" href="SimulationBasedInference.html">Simulation Based Inference</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_10.html"><span style="color: blue;"><b>Reinforcement Learning</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1EsW71u3hdNdXyhlDfkmOX__9c4wJZUee_pjlsmlv_Vg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ReinforcementLearning.html">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_07.html">Homework 07: Reinforcement Learning: Implementing a Deep Q-Network</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_11.html"><span style="color: blue;"><b>AI Explainablility and Uncertainty Quantification</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1ydzY7IEYzALTR6ez5gvwwKDduf_7wUtZddq0SUSuvI0/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="AIExplainabilityUncertaintyQuantification.html">AI Explainability and Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_08.html">Homework 08: Detecting Distribution Shift on MNIST using Bayesian Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_12.html"><span style="color: blue;"><b>Unsupervised Learning and Anomaly Detection</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1ydzY7IEYzALTR6ez5gvwwKDduf_7wUtZddq0SUSuvI0/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="UnsupervisedLearningAnomalyDetection.html">Unsupervised Learning and Anomaly Detection</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_13.html"><span style="color: blue;"><b>Physics Informed Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1C-Z8b6WP5rE8yohZQdSxyH8O_bHIbllq97QhEJYyh0w/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="PhysicsInformedNeuralNetworks.html">Physics Informed Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="LearningTheSchrodingerEquation.html">Solving the Time Dependent Schrodinger Equation with Physics-Informed Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="SymbolicRegression.html">Introduction to Symbolic Regression</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_02.html"><span style="color: blue;"><b>Project 02</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_AnisotropyQGP.html">Anisotropy in the Quark Gluon Plasma</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_AberratedImages.html">Aberrated Image Recovery of Ultracold Atoms</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_14.html"><span style="color: blue;"><b>Learning from the Machines</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1hkfaU7JVy1f5S8jURZvTY67KRzP7I8zGRf4Zku_bpM4/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="LearningPhysicsMachines.html">Learning Physics from the Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_15.html"><span style="color: blue;"><b>Future of AI and Physics: What Lies Ahead?</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1eB1qCn5J07D5he_DCpkBiKjbVjdI6OexUzMQ351GCaI/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="LookingForward.html">Future of AI and Physics: What Lies Ahead?</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/illinois-mlp/MachineLearningForPhysics/blob/main/_sources/lectures/VariationalAutoEncoders.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/illinois-mlp/MachineLearningForPhysics" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/_sources/lectures/VariationalAutoEncoders.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Variational AutoEncoders</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-what-are-generative-models-span"><span style="color:Orange">What are Generative Models?</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-vanilla-autoencoders-as-a-generative-model-span"><span style="color:Orange">Vanilla Autoencoders as a Generative Model</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-variational-autoencoders-regularized-latent-spaces-span"><span style="color:Orange">Variational AutoEncoders: Regularized Latent Spaces</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-how-do-we-force-latents-to-be-gaussian-span"><span style="color:Orange">How do we Force Latents to be Gaussian?</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-imposing-standard-normal-distributions-on-our-latent-variables-span"><span style="color:Orange">Imposing Standard Normal Distributions on our Latent Variables</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-and-thats-it-almost-span"><span style="color:Orange">And thats it! (Almost)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-last-thing-logvariance-span"><span style="color:Orange">Last Thing - LogVariance</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-recap-span"><span style="color:LightGreen">Recap</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-helper-code-span"><span style="color:Orange">Helper Code</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-load-mnist-data-span"><span style="color:Orange">Load MNIST data</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-building-a-simple-linear-variational-autoencoder-span"><span style="color:Orange">Building a Simple Linear Variational Autoencoder</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-define-the-loss-function-span"><span style="color:Orange">Define the Loss Function</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-training-script-span"><span style="color:Orange">Training Script</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-comparing-latents-span"><span style="color:Orange">Comparing Latents</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-interpolate-the-latent-space-span"><span style="color:Orange">Interpolate the Latent Space</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-generating-new-data-span"><span style="color:Orange">Generating New Data</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-lets-build-a-convolutional-variational-autoencoder-span"><span style="color:Orange">Lets Build A Convolutional Variational AutoEncoder</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-lets-plot-the-latents-with-tsne-span"><span style="color:Orange">Lets Plot the Latents with TSNE</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-taking-a-closer-look-at-reconstruction-span"><span style="color:Orange">Taking a Closer Look at Reconstruction</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="variational-autoencoders">
<h1>Variational AutoEncoders<a class="headerlink" href="#variational-autoencoders" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">()</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span><span class="p">;</span> <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">wget_data</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">local_path</span><span class="o">=</span><span class="s1">&#39;./tmp_data&#39;</span><span class="p">):</span>
  <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="n">p</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">([</span><span class="s2">&quot;wget&quot;</span><span class="p">,</span> <span class="s2">&quot;-nc&quot;</span><span class="p">,</span> <span class="s2">&quot;-P&quot;</span><span class="p">,</span> <span class="n">local_path</span><span class="p">,</span> <span class="n">url</span><span class="p">],</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;UTF-8&#39;</span><span class="p">)</span>
  <span class="n">rc</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">while</span> <span class="n">rc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="n">rc</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>As we showed in this <a class="reference external" href="https://illinois-mlp.github.io/MachineLearningForPhysics/_sources/lectures/AutoEncoders.html">notebook</a>, AutoEncoders are an extremely effective way for mapping high dimensional and complex datasets to lower dimensional latents. There were again two parts to an AutoEncoder:</p>
<ul class="simple">
<li><p><span style="color:Violet">Encoder</span>: Take data and compress down to the lower dimensional form</p></li>
<li><p><span style="color:Violet">Decoder</span>: Take the lower dimensional data and attempt to reconstruct the original uncompressed data</p></li>
</ul>
<p>If all you care about is encoding, then this method is probably sufficient. You can definitely write larger and more complex Autoencoders but the prinipals from our explorations stay the same. On the other hand, what if you want to generate new samples?</p>
<section id="span-style-color-orange-what-are-generative-models-span">
<h2><span style="color:Orange">What are Generative Models?</span><a class="headerlink" href="#span-style-color-orange-what-are-generative-models-span" title="Permalink to this heading">#</a></h2>
<p>There are typically two classes of models. <span style="color:Violet">Discriminative Models</span> are typically architectures that take in data and predict something about it like a classification or regression task. More specifically, this means discriminative models are learning something along the lines of <span class="math notranslate nohighlight">\(P(y=prediction|x=data)\)</span>, to predict the probabilty of some output <span class="math notranslate nohighlight">\(y\)</span> given some input data <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p><span style="color:Violet">Generative models</span> on the other hand learn the data distribution <span class="math notranslate nohighlight">\(P(x)\)</span> directly. If we can paramaterize the distribution of the data using a Neural Network, then we can grab samples from that distribution to actually <em><strong>generate data</strong></em>. How did we do this with AutoEncoders before?</p>
</section>
<section id="span-style-color-orange-vanilla-autoencoders-as-a-generative-model-span">
<h2><span style="color:Orange">Vanilla Autoencoders as a Generative Model</span><a class="headerlink" href="#span-style-color-orange-vanilla-autoencoders-as-a-generative-model-span" title="Permalink to this heading">#</a></h2>
<p>We were briefly able to look at this in the <a class="reference external" href="https://illinois-mlp.github.io/MachineLearningForPhysics/_sources/lectures/AutoEncoders.html">AutoEncoders notebook</a>, but lets recap the imporant elements.</p>
<p>What we have below is a plot of the testing MNIST data compressed to 2 dimensional latents. This means the Encoder took the 784 number in MNIST and compressed them down into just 2 numbers, and whats really neat is this compression forces clustering between similar numbers to occur (numbers that look similar occupy same pockets of the larger vector space).</p>
<p><a class="reference internal" href="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-uncompact_vanilla_autoencoder_latents.png"><img alt="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-uncompact_vanilla_autoencoder_latents.png" class="align-center" src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-uncompact_vanilla_autoencoder_latents.png" style="width: 800px;" /></a></img><br></p>
<p>From this, if I wanted to generate an image of the number 1, I could pick a coordinate like (-3, 2) which is roughly the location of our cloud of image points for digit 1. These two numbers could be passed to the Decoder of the AutoEncoder and it should generate a sample that looks like a 1.</p>
<p>This method does have its own set of problems though, typically related to the uncompact nature of our embeddings. The cluster embeddings look almost like they are being stretched out from the middle, and there are large pockets of white space between cluters. In these pockets, the model cannot be expected to generate any reasonable generation, as the data has not been mapped there. Here is an example of the interpolated space of generating images from a grid of latent embeddings:</p>
<p><a class="reference internal" href="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-vanilla_autoencoder_latent_interpolation.png"><img alt="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-vanilla_autoencoder_latent_interpolation.png" class="align-center" src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-vanilla_autoencoder_latent_interpolation.png" style="width: 800px;" /></a></img><br></p>
<p>Do you notice how:</p>
<ul class="simple">
<li><p>In certain pockets of area in our grid, the generates images look really bad</p></li>
<li><p>We dont have smooth transition between the classes of images</p></li>
</ul>
<p>This is because we have absolutely no regularization on our latent space - it is allowed to be whatever it needs to be to minimize our MSE Reconstruction Loss.</p>
<p>What we want is to be able to sample from a distribution of latent variables and then pass those samples to our decoder to generate data. If we are going to be sampling from a distribution, we have to pick which one we want, and the simplicity of normal distributions make them a natural choice.</p>
<p><em><strong><span style="color:Violet">Key point</span></strong></em>: We will force our model to learn <span class="math notranslate nohighlight">\(P(x)\)</span> by forcing the reconstruction loss <strong>AND</strong> making sure to map our latents to a Standard Normal distribution rather an unrestricted arbritrary one.</p>
<p>If our latents are normally distributed, we know exactly the properties of it and our life becomes easier when we want to use it for some generative tasks.</p>
</section>
<section id="span-style-color-orange-variational-autoencoders-regularized-latent-spaces-span">
<h2><span style="color:Orange">Variational AutoEncoders: Regularized Latent Spaces</span><a class="headerlink" href="#span-style-color-orange-variational-autoencoders-regularized-latent-spaces-span" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-variational-autoencoder-visual.png"><img alt="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-variational-autoencoder-visual.png" class="align-center" src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AutoEncoders-variational-autoencoder-visual.png" style="width: 1000px;" /></a></img><br></p>
<p><a class="reference external" href="https://medium.com/dataseries/variational-autoencoder-with-pytorch-2d359cbf027b">Image Source</a></p>
<p>So we have set the goal. What we want is a model that can perform the reconstruction task like before, with the added restriction that the latent space must be close to a normal distribution. This means, we are taking high dimensional and complex dataset and mapping to a simple gaussian! Now this wont be a regular gaussian distribution, but rather a MultiVariate gaussian.</p>
<p><em><strong><span style="color:Violet">Quick Review of Gaussians</strong></em></span></p>
<p>What is our gaussian distribution?</p>
<div class="math notranslate nohighlight">
\[ \Large
f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\]</div>
<p>A gaussian distribution is defined by (parameterized by) <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>, and so a 1D gaussian has a single <span class="math notranslate nohighlight">\(\mu\)</span> and single <span class="math notranslate nohighlight">\(\sigma\)</span> and we can generate single numbers from this that would be normally distributed. We will be looking at the special case of a Gaussian distribution known as the Standard Normal distribution that has <span class="math notranslate nohighlight">\(\mu = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma = 1\)</span>. The problem here is, if we took data and compressed our data down to a latent with a dimension of just a single number, we probably wont have very good performance. In this case, we want to compress down to a couple of numbers, but that vector of numbers should be normally distributed, and is sampled from a Standard MultiVariate Gaussian distribution. A MultiVariate gaussian is defined by a vector <span class="math notranslate nohighlight">\(\vec{\mu}_d\)</span> that define the center of the distribution in N-D space, a covariance matrix <span class="math notranslate nohighlight">\(\Sigma_{dxd}\)</span> tells us the variance of each variable and covariance of each pair of variables, and the sampled data will be <span class="math notranslate nohighlight">\(\vec{x}_d\)</span>. More specifically they look like:</p>
<div class="math notranslate nohighlight">
\[\vec{x}_d = [x_1, x_2, ... x_d]\]</div>
<div class="math notranslate nohighlight">
\[\vec{\mu}_d = [\mu_1 = mean(x_1), \mu_2 = mean(x_2), ... \mu_d = mean(x_d)]\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\sigma_{d\times d} = \begin{bmatrix} var(x_1) &amp; cov(x_1, x_2) &amp; \dots &amp; cov(x_1, x_d) \\ cov(x_2, x_1) &amp; var(x_2) &amp;  &amp; \vdots \\ \vdots &amp;  &amp; \ddots &amp;  \\ cov(x_1, x_d) &amp; \dots &amp;  &amp; var(x_d) \end{bmatrix}\end{split}\]</div>
<p>To further simplify this, for a Standard Normal MultiVariate Gaussian distribution, <span class="math notranslate nohighlight">\(\vec{\mu}_d = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma_{d\times d}\)</span> will be the Identity matrix!</p>
</section>
<section id="span-style-color-orange-how-do-we-force-latents-to-be-gaussian-span">
<h2><span style="color:Orange">How do we Force Latents to be Gaussian?</span><a class="headerlink" href="#span-style-color-orange-how-do-we-force-latents-to-be-gaussian-span" title="Permalink to this heading">#</a></h2>
<p>So we have our problem statement, take high dimensional data, map it to a low dimensional gaussian distribution, sample from this gaussian distribution, and then reconstruct the original high dimensional data. But how do we do this? There are two things we have to do:</p>
<ul class="simple">
<li><p><span style="color:LightGreen">Reconstruction Loss</span>: Make sure the output to the model looks like the input to the model. This is already done through the reconstruction loss <span class="math notranslate nohighlight">\(\cal{L}_{\rm MSE}\)</span>.</p></li>
<li><p><span style="color:LightGreen">KL Divergence Loss</span>: Measure the distance of our latent distribution from a Standard Normal distribution and minimize it.</p></li>
</ul>
<p>To understand Variational Autoencoders, one needs a firm grasp of the KL Divergence and Variation Inference. What follows is consise review of these concepts from the <a class="reference external" href="https://illinois-mlp.github.io/MachineLearningForPhysics/_sources/lectures/VariationalInference.html">Variational Inference notebook</a>.</p>
<p><em><strong><span style="color:Violet">Quick Review of the Kullback-Leibler Divergence</strong></em></span></p>
<p>The Kullback-Leibler (KL) Divergence is a measure of entropy (or difference) between two probability distributions. But what is Entropy? Entropy is a measure of the amount of “information” in data and is typically written as:</p>
<div class="math notranslate nohighlight">
\[ \Large
H = -\sum_{i=1}^Np(x_i)\cdot \log p(x_i)
\]</div>
<p>You can think of this like the Expected Value of information in an event. KL Divergence is then the difference in information between two separate distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>. Well then we can just write that as:</p>
<div class="math notranslate nohighlight">
\[ \Large
D_{KL}(Q||P) = \sum_{i=1}^Np(x_i) \cdot (\log p(x_i) - \log q(x_i)) = \sum_{i=1}^Np(x_i) \cdot \log \frac{p(x_i)}{q(x_i)}
\]</div>
<p>We can quickly implement this too, just so we can test it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>

<span class="n">kl_pq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="o">/</span><span class="n">Q</span><span class="p">))</span>
<span class="n">kl_qp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Q</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Q</span><span class="o">/</span><span class="n">P</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;D(P|Q):&quot;</span><span class="p">,</span> <span class="n">kl_pq</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;D(Q|P):&quot;</span><span class="p">,</span> <span class="n">kl_qp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>D(P|Q): 0.7941600448957673
D(Q|P): 1.0325534177382862
</pre></div>
</div>
</div>
</div>
<p>We can see very clearly that KL Divergence is not a symmetrical function, i.e. <span class="math notranslate nohighlight">\(D(P|Q) \neq D(Q|P)\)</span>. But this should work! We now have a metric that can compute the difference between distributions. To incorporate this into our training, we will have to enter the world of Variational Inference.</p>
<p><em><strong><span style="color:Violet">Quick Review of Variational Inference</strong></em></span></p>
<p><strong>Note:</strong> The math we are about to do is a bit scary looking, I promise its not. I will try to be as granular as possible to not skip any steps, but I would highly recommend you write the derivation as you read it to make sure you understand the probability and algebraic manipulations I am doing! Here are some links that we also really helpful for me to reference as I was going through all this.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://medium.com/retina-ai-health-inc/variational-inference-derivation-of-the-variational-autoencoder-vae-loss-function-a-true-story-3543a3dc67ee">Variational Inference &amp; Derivation of the Variational Autoencoder (VAE) Loss Function: A True Story</a></p></li>
<li><p><a class="reference external" href="https://gokererdogan.github.io/2017/08/15/variational-autoencoder-explained/">Variational Autoencoder Explained</a></p></li>
</ul>
<p><strong>Onto the Derivation!</strong></p>
<p>In Bayesian Statistics, we typically have this form:</p>
<div class="math notranslate nohighlight">
\[ \large
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\]</div>
<p>Each piece of this actually has a name!</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A|B)\)</span>: Posterior Probability, the probability of A occuring knowing B had occured</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A)\)</span>: Likelihood Function, the probability of B occuring knowing A had occured</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A)\)</span>: Prior Probability, some initial belief you had about A before any evidence from data is utilized</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B)\)</span>: Marginal (Evidence) Probability, summing across all possible A in the joint distribution <span class="math notranslate nohighlight">\(P(A,B)\)</span></p></li>
</ul>
<p>In our case, we will use <span class="math notranslate nohighlight">\(P(z|x)\)</span> where <span class="math notranslate nohighlight">\(z\)</span> is our latent (compressed) variables and <span class="math notranslate nohighlight">\(x\)</span> is the data. This means we can write our encoder as <span class="math notranslate nohighlight">\(P(z|x)\)</span> which is just our posterior, and the decoder <span class="math notranslate nohighlight">\(P(x|z)\)</span> which is our likelihood function. But we face yet another problem!</p>
<p>Remember, the original problem we are trying to solve is <span class="math notranslate nohighlight">\(P(x)\)</span>, so how can we get that from our conditionals? Well we can do the following manipulation:</p>
<div class="math notranslate nohighlight">
\[ \large
P(x|z) = \frac{P(x,z)}{P(z)}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
P(x,z) = P(x|z) \cdot P(z)
\]</div>
<p>We can then take the marginal over z of the joint distribution to get just <span class="math notranslate nohighlight">\(P(x)\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \large
P(x) = \int_{z}{}P(x|z) \cdot P(z)
\]</div>
<p>And this intergral is impossible, there is no way we could feasibily compute the integral over the entire complex latent space. So in our Bayes Rule formulation:</p>
<div class="math notranslate nohighlight">
\[ \large
P(z|x) = \frac{P(x|z)P(z)}{P(x)}
\]</div>
<p>The denominator is totally unsolveable! Therefore, because we cannot compute the true posterior distribution, we will attempt to approximate it with a simpler known distribution, which is precisely the process of Variational Inference! In our case we will pick gaussian as our simpler distribution, and if we can learn to map the data to a latent dimension that follows a normal distribution, that will solve our variational inference problem. Again going back to our KL divergence then, we want to then minimize the KL divergence between our predicted distribution from the Neural Network and a normal distribution.</p>
<p>Lets update our notation a bit so it follows what you typically see online</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(q_{\theta}(z_i|x)\)</span>: The distribution of our latents <span class="math notranslate nohighlight">\(z_i\)</span> given <span class="math notranslate nohighlight">\(x\)</span>, approximated by a neural network with parameters <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p_(z_i|x)\)</span>: The simpler distribution we selected to approximate with (Standard Multivariate Normal)</p></li>
</ul>
<p>Our goal can the be written as:</p>
<div class="math notranslate nohighlight">
\[ \large
\mathop{\min}\limits_{\theta}D_{KL}(q_{\theta}(z_i|x)||p(z_i|x))
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)}{p(z_i|x)}
\]</div>
<p>We can then use conditional probability rule and rewrite:</p>
<div class="math notranslate nohighlight">
\[ \large
p(z_i|x) = \frac{p(z_i,x)}{p(x)}
\]</div>
<p>So we then substitute into our KL Divergence:</p>
<div class="math notranslate nohighlight">
\[ \large
=\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)}{p(z_i|x)}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)p(x)}{p(z_i,x)}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)}{p(z_i,x)} + \sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log p(x)
\]</div>
<p>In the second sum <span class="math notranslate nohighlight">\(\log p(x)\)</span> is a constant and has nothing to do with <span class="math notranslate nohighlight">\(z\)</span>. Therefore we can pull it out!</p>
<div class="math notranslate nohighlight">
\[ \large
=\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)}{p(z_i,x)} + \log p(x) \cdot \sum_{i=1}^Nq_{\theta}(z_i|x)
\]</div>
<p>Again, in the second sum, notice that we are adding up over all the <span class="math notranslate nohighlight">\(z\)</span> values, well the sum of of a probability distribution over its entire support (in our case over all discrete latents <span class="math notranslate nohighlight">\(z\)</span>) will just <strong>add to 1</strong>! So we are then left with the expression:</p>
<div class="math notranslate nohighlight">
\[ \large
D_{KL}(q_{\theta}(z_i|x)||p(z_i|x)) =(\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)}{p(z_i,x)}) + \log p(x)
\]</div>
<p>Using our formula relating joint distributions to conditional distributions, we can write both:</p>
<div class="math notranslate nohighlight">
\[ \large
p(z_i,x) = p(z_i|x)p(x) = p(x|z_i)p(z_i)
\]</div>
<p>In this case, we definitely prefer the second form <span class="math notranslate nohighlight">\(p(x|z_i)p(z_i)\)</span> as everything in there is computable! (There are no nasty integrals over the unknown latent space <span class="math notranslate nohighlight">\(z\)</span>) The first form has a <span class="math notranslate nohighlight">\(p(x)\)</span> which again would have been an impossible integral. So lets substitute this in:</p>
<div class="math notranslate nohighlight">
\[ \large
D_{KL}(q_{\theta}(z_i|x)||p(z_i|x)) =(\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)}{p(x|z_i)p(z_i)}) + \log p(x)
\]</div>
<p>Lets rearrange the terms a bit:</p>
<div class="math notranslate nohighlight">
\[ \large
- (\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{q_{\theta}(z_i|x)}{p(x|z_i)p(z_i)}) = \log p(x) - D_{KL}(q_{\theta}(z_i|x)||p(z_i|x))
\]</div>
<p>We can get rid of the negative on the left by flipping the fraction in the logarithm:</p>
<div class="math notranslate nohighlight">
\[ \large
\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{p(x|z_i)p(z_i)}{q_{\theta}(z_i|x)} = \log p(x) - D_{KL}(q_{\theta}(z_i|x)||p(z_i|x))
\]</div>
<p>A property of <span class="math notranslate nohighlight">\(D_{KL}\)</span> is that it is never less than 0, you can see the quick proof for that <a class="reference external" href="https://statproofbook.github.io/P/kl-nonneg.html">here</a>. Lets simplify our long expression to be <span class="math notranslate nohighlight">\(A = B - C\)</span>. We can tell two things from here:</p>
<ul class="simple">
<li><p>If we want to minimize C (our placeholder for KL divergence), that is the same as maximizing A. Therefore our optimization problem changes from <span class="math notranslate nohighlight">\(\mathop{\min}\limits_{\theta}D_{KL}(q_{\theta}(z_i|x)||p(z_i|x))\)</span> to <span class="math notranslate nohighlight">\(\mathop{\max}\limits_{\theta} \sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{p(x|z_i)p(z_i)}{q_{\theta}(z_i|x)}\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(C\)</span> must be greater than or equal to 0 due to the properties of KL Divergence, then <span class="math notranslate nohighlight">\(A\)</span> must always be less than <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
</ul>
<p>Using these properties, lets rewrite the expression:</p>
<div class="math notranslate nohighlight">
\[ \large
\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{p(x|z_i)p(z_i)}{q_{\theta}(z_i|x)} \leq \log p(x)
\]</div>
<p>Lets take a look at our two conditional probabilities we have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(q_{\theta}(z_i|x)\)</span>: Use a Neural Network paramaterized by <span class="math notranslate nohighlight">\(\theta\)</span> to predict the latent <span class="math notranslate nohighlight">\(z_i\)</span>, this is handled by the encoder and is our <strong>Posterior Distribution</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(p(x|z_i)\)</span>: This is our reverse process and the <strong>Likelihood Function</strong> that also has to be learned jointly by our Decoder! Lets let the decoder by a Neural Network paramaterized by the variable <span class="math notranslate nohighlight">\(\phi\)</span></p></li>
</ul>
<p>Therefore our Encoder is <span class="math notranslate nohighlight">\(q_{\theta}(z_i|x)\)</span> and the Decoder is <span class="math notranslate nohighlight">\(p_{\phi}(x|z_i)\)</span>. So our final form for the inequality is:</p>
<div class="math notranslate nohighlight">
\[ \large
\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{p_{\phi}(x|z_i)p(z_i)}{q_{\theta}(z_i|x)} \leq \log p(x)
\]</div>
<p>Our optimization goal is to maximize the left side of the equation, which happens to be the lower bound of our <span class="math notranslate nohighlight">\(\log p(x)\)</span>. Therefore it is known as the <strong>Evidence Lower Bound</strong> or ELBO of our original data distribution. As we maximize our loss, we will equivalently be maximizing the likelihood of the data distribution as well!</p>
<p>Therefore our final form for our optimization is:</p>
<div class="math notranslate nohighlight">
\[ \large
\mathop{\max}\limits_{\theta, \phi}\sum_{i=1}^Nq_{\theta}(z_i|x) \cdot \log \frac{p_{\phi}(x|z_i)p(z_i)}{q_{\theta}(z_i|x)}
\]</div>
</section>
<section id="span-style-color-orange-imposing-standard-normal-distributions-on-our-latent-variables-span">
<h2><span style="color:Orange">Imposing Standard Normal Distributions on our Latent Variables</span><a class="headerlink" href="#span-style-color-orange-imposing-standard-normal-distributions-on-our-latent-variables-span" title="Permalink to this heading">#</a></h2>
<p>Everything we have seen so far is the derivation of Variational Inference, and everything is expressed in terms of <span class="math notranslate nohighlight">\(q_{\theta}(z_i|x)\)</span> and <span class="math notranslate nohighlight">\(p_{\phi}(x|z_i)\)</span>. Remember, it is our decision what distribution we want to use for these latent variables, and typically we use Normal Distributions.</p>
<p>Lets write everything as Univariate Normal Distributions for now to keep the notation shorter, but the derivation will expand to Multivariate gaussians pretty easily. This derivation is pretty long but just step through it line by line.</p>
<p>First things first, we want to make our latent distribution <span class="math notranslate nohighlight">\(q_{\theta}(z_i|x)\)</span> close to normal, so we will compute the KL divergence between <span class="math notranslate nohighlight">\(q_{\theta}(z_i|x)\)</span> and some arbritary normal distribution <span class="math notranslate nohighlight">\(p(z)\)</span>.</p>
<p>Normal distributions are represented by a mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Lets allow <span class="math notranslate nohighlight">\(q_{\theta}(z|x)\)</span> to have mean <span class="math notranslate nohighlight">\(\mu_q\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_q^2\)</span> and allow <span class="math notranslate nohighlight">\(p(z)\)</span> to have mean <span class="math notranslate nohighlight">\(\mu_p\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_p^2\)</span>. Then we can say:</p>
<div class="math notranslate nohighlight">
\[ \large
q_{\theta}(z|x) \sim \frac{1}{\sigma_q \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu_q}{\sigma_q}\right)^2}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
p(z) \sim \frac{1}{\sigma_p \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu_p}{\sigma_p}\right)^2}
\]</div>
<p>Lets plug these expressions in for our <span class="math notranslate nohighlight">\(D_{KL}\)</span> formula from before and then simplify!</p>
<div class="math notranslate nohighlight">
\[ \large
D_{KL}(q_{\theta}(z|x)||p(z)) = \sum_{z}q_{\theta}(z|x) \cdot \log \frac{q_{\theta}(z|x)}{p(z|x)}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \sum_{z}\frac{1}{\sigma_q \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu_q}{\sigma_q}\right)^2} \cdot \log \frac{\frac{1}{\sigma_q \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu_q}{\sigma_q}\right)^2}}{\frac{1}{\sigma_p \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu_p}{\sigma_p}\right)^2}}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \sum_{z}\frac{1}{\sigma_q \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu_q}{\sigma_q}\right)^2} \cdot \log \frac{\sigma_p e^{-\frac{1}{2}\left(\frac{x-\mu_q}{\sigma_q}\right)^2}}{\sigma_q e^{-\frac{1}{2}\left(\frac{x-\mu_p}{\sigma_p}\right)^2}}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \sum_{z}\frac{1}{\sigma_q \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu_q}{\sigma_q}\right)^2} \cdot \left(\log \frac{\sigma_p}{\sigma_q} - \frac{(x-\mu_q)^2}{2\sigma_q^2} + \frac{(x-\mu_p)^2}{2\sigma_p^2}\right)
\]</div>
<p>The normal distribution for <span class="math notranslate nohighlight">\(q\)</span> in front has a constant so we can pull it out:</p>
<div class="math notranslate nohighlight">
\[ \large
= \frac{1}{\sigma_q \sqrt{2\pi} } \sum_{z} e^{-\frac{1}{2}\left(\frac{x-\mu_q}{\sigma_q}\right)^2} \cdot \left(\log \frac{\sigma_p}{\sigma_q} - \frac{(x-\mu_q)^2}{2\sigma_q^2} + \frac{(x-\mu_p)^2}{2\sigma_p^2}\right)
\]</div>
<p>What does this look like? We are weighting <span class="math notranslate nohighlight">\(\left(\log \frac{\sigma_p}{\sigma_q} - \frac{(x-\mu_q)^2}{2\sigma_q^2} + \frac{(x-\mu_p)^2}{2\sigma_p^2}\right)\)</span> by the gaussian kernel for <span class="math notranslate nohighlight">\(q(x|z)\)</span> so that is just an Expected Value! Now remember again, this whole expression is our KL Divergence, and our goal is to minimize the KL Divergence value. That means we can ignore the constant in front as well. (Minimizing <span class="math notranslate nohighlight">\(AD_{KL}\)</span> is the same as just minimizing <span class="math notranslate nohighlight">\(D_{KL}\)</span>)</p>
<div class="math notranslate nohighlight">
\[ \large
=\mathbb{E}_q\left(\log \frac{\sigma_p}{\sigma_q} - \frac{(x-\mu_q)^2}{2\sigma_q^2} + \frac{(x-\mu_p)^2}{2\sigma_p^2}\right)
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=\log \frac{\sigma_p}{\sigma_q} - \frac{1}{2\sigma_q^2}\mathbb{E}_q(x-\mu_q)^2 + \frac{1}{2\sigma_p^2}\mathbb{E}_q(x-\mu_p)^2
\]</div>
<p>The Variance appears! Recall that <span class="math notranslate nohighlight">\(Var(x) = \sigma^2 = \mathbb{E}[(x-\mu)^2]\)</span>. So we can replace the expected value in our derivation between the distribution of <span class="math notranslate nohighlight">\(q\)</span> and the <span class="math notranslate nohighlight">\(\mu_q\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \large
=\log \frac{\sigma_p}{\sigma_q} - \frac{\sigma_q^2}{2\sigma_q^2}+ \frac{1}{2\sigma_p}\mathbb{E}_q(x-\mu_p)^2
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \log \frac{\sigma_p}{\sigma_q} - \frac{1}{2}+ \frac{1}{2\sigma_p^2}\mathbb{E}_q(x-\mu_p)^2
\]</div>
<p>Now we need to deal with <span class="math notranslate nohighlight">\(\mathbb{E}_q(x-\mu_p)^2\)</span> using a simple algebraic trick and again, doing a ton of expanding and simplification:</p>
<div class="math notranslate nohighlight">
\[ \large
\mathbb{E}_q(x-\mu_p)^2
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \mathbb{E}_q((x-\mu_q)+(\mu_q-\mu_p))^2
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \mathbb{E}_q((x-\mu_q)^2+ 2(x-\mu_q)(\mu_q-\mu_p) + (\mu_q-\mu_p)^2)
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \mathbb{E}_q[(x-\mu_q)^2]+ 2\mathbb{E}_q(x-\mu_q)(\mu_q-\mu_p) + \mathbb{E}_q(\mu_q-\mu_p)^2
\]</div>
<p>Again we can replace our first expected value as it is the variance formula, and the last expected value is just a constant of the difference in <span class="math notranslate nohighlight">\(\mu_q-\mu_p\)</span> (Expected value of a constant is just a constant, the only random variable we have is <span class="math notranslate nohighlight">\(x\)</span>)</p>
<div class="math notranslate nohighlight">
\[ \large
= \sigma_q^2+ 2\mathbb{E}_q[(x-\mu_q)(\mu_q-\mu_p)] + (\mu_q-\mu_p)^2
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \sigma_q^2+ (\mu_q-\mu_p)^2+2(\mu_q-\mu_p)\mathbb{E}_q[(x-\mu_q)]
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \sigma_q^2+ (\mu_q-\mu_p)^2+2(\mu_q-\mu_p)(\mathbb{E}_q[x] - \mathbb{E}_q[\mu_q])
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \sigma_q^2+ (\mu_q-\mu_p)^2+2(\mu_q-\mu_p)(\mu_q - \mu_q)
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \sigma_q^2+ (\mu_q-\mu_p)^2
\]</div>
<p>Lets plug it all back into our original equation!</p>
<div class="math notranslate nohighlight">
\[ \large
\log \frac{\sigma_p}{\sigma_q} - \frac{1}{2}+ \frac{1}{2\sigma_p^2}\mathbb{E}_q(x-\mu_p)^2
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \log \frac{\sigma_p}{\sigma_q} - \frac{1}{2}+ \frac{\sigma_q^2+ (\mu_q-\mu_p)^2}{2\sigma_p^2}
\]</div>
<p>Now if you remember, the distribution <span class="math notranslate nohighlight">\(p(z)\)</span> we want to map to is a Standard Normal distribution with <span class="math notranslate nohighlight">\(\mu_p = 0\)</span> and <span class="math notranslate nohighlight">\(\sigma_p = 1\)</span>. So lets plug those in as well!</p>
<div class="math notranslate nohighlight">
\[ \large
= \log \frac{1}{\sigma_q} - \frac{1}{2}+ \frac{\sigma_q^2+ \mu_q^2}{2}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \frac{1}{2}\log \frac{1}{\sigma_q^2} - \frac{1}{2}+ \frac{\sigma_q^2+ \mu_q^2}{2}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= \frac{1}{2}(\log \frac{1}{\sigma_q^2} - 1 + \sigma_q^2+ \mu_q^2)
\]</div>
<p>Remember this equation from before for our Evidence Lower Bound?</p>
<div class="math notranslate nohighlight">
\[ \large
\sum_{z}q_{\theta}(z|x) \cdot \log \frac{p_{\phi}(x|z)p(z)}{q_{\theta}(z|x)} \leq \log p(x)
\]</div>
<p>We can then take our ELBO and split it up to make it some loss function including KL Divergence and Reconstruction loss:</p>
<div class="math notranslate nohighlight">
\[ \large
\sum_{z}q_{\theta}(z|x) \cdot \log \frac{p_{\phi}(x|z)p(z)}{q_{\theta}(z|x)}
\]</div>
<div class="math notranslate nohighlight">
\[ \large =\sum_{z}q_{\theta}(z|x) \cdot \log \frac{p_{\phi}(x|z)p(z)}{q_{\theta}(z|x)}
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=\sum_{z}q_{\theta}(z|x) \cdot \log \frac{p(z)}{q_{\theta}(z|x)} + \sum_{z}q_{\theta}(z|x)\log p_{\phi}(x|z)
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=-\sum_{z}q_{\theta}(z|x) \cdot \log \frac{q_{\theta}(z|x)}{p(z)} + \mathbb{E}_q[\log p_{\phi}(x|z)]
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=-D_{KL}(q_{\theta}(z|x)||p(z)) + \mathbb{E}_q[\log p_{\phi}(x|z)]
\]</div>
<div class="math notranslate nohighlight">
\[ \large
= -D_{KL} + Reconstruction
\]</div>
<p>Therefore this is our Negative KL Divergence plus the Expected Value of our <span class="math notranslate nohighlight">\(P(x|z)\)</span> which is our reconstruction loss. We want to maximize this total value, so lets plug in our expression we derived earlier for the KL Drivergence between <span class="math notranslate nohighlight">\(q(z|x)\)</span> and a standard normal distribution.</p>
<div class="math notranslate nohighlight">
\[ \large
=-D_{KL}(q_{\theta}(z|x)||p(z)) + \mathbb{E}_q[\log p_{\phi}(x|z)]
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=-\frac{1}{2}(\log \frac{1}{\sigma_q^2} - 1 + \sigma_q^2+ \mu_q^2) + \mathbb{E}_q[\log p_{\phi}(x|z)]
\]</div>
<div class="math notranslate nohighlight">
\[ \large
=\frac{1}{2}(\log \sigma_q^2 + 1 - \sigma_q^2 - \mu_q^2) + \mathbb{E}_q[\log p_{\phi}(x|z)]
\]</div>
<p>That is the final function that we want to maximize, so we can force the Evidence Lower Bound to go up, and therfore cause <span class="math notranslate nohighlight">\(p(x)\)</span> to also go up. Although typically in deep learning, we perform gradient descent not ascent. So maximizing a function is the equivalent to minimizng the negative of that function, so our final loss for our model is:</p>
<div class="math notranslate nohighlight">
\[ \large
=\mathop{\min}\limits_{\theta, \phi}-\frac{1}{2}(\log \sigma_q^2 + 1 - \sigma_q^2 - \mu_q^2) - \mathbb{E}_q[\log p_{\phi}(x|z)]
\]</div>
</section>
<section id="span-style-color-orange-and-thats-it-almost-span">
<h2><span style="color:Orange">And thats it! (Almost)</span><a class="headerlink" href="#span-style-color-orange-and-thats-it-almost-span" title="Permalink to this heading">#</a></h2>
<p>We have derived the final form of our loss function and here are the components of it that we need:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma_q^2\)</span>: Use a Neural Network paramaterized by <span class="math notranslate nohighlight">\(\theta\)</span> to compute the <span style="color:Violet">Variance</span> of a sample</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_q^2\)</span>: Use a Neural Network paramaterized by <span class="math notranslate nohighlight">\(\theta\)</span> to compute the <span style="color:Violet">Mean</span> of a sample</p></li>
<li><p><span class="math notranslate nohighlight">\(\log p_{\phi}(x|z)\)</span>: Reconstruction loss, but <strong>theres a problem!</strong>, what is <span class="math notranslate nohighlight">\(z\)</span>?</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(z\)</span> is our latent variable (of some given dimension) that is <strong>SAMPLED</strong> from our distribution. Our model will learn the parameters of our gaussian (mean and variance) and then <span class="math notranslate nohighlight">\(z\)</span> will be a <strong>RANDOM SAMPLE</strong> from a normal distribution defined by the predicted parameters.</p>
</section>
<section id="span-style-color-orange-last-thing-logvariance-span">
<h2><span style="color:Orange">Last Thing - LogVariance</span><a class="headerlink" href="#span-style-color-orange-last-thing-logvariance-span" title="Permalink to this heading">#</a></h2>
<p>This has nothing to do with theory but rather stability and just general practice. Like we showed before, our Neural Network will predict the Mean <span class="math notranslate nohighlight">\(\mu\)</span> and the Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. But just for stability, the model will actually predict the Log Variance rather than variance directly. It probably doesn’t matter all that much but I will just go this way as its whats more people use. But this means we need to keep a few small things in mind.</p>
<p>When doing the reparameterization trick, we wanted to multiply by the sqare root of our variance prediction. Because we have the Log Variance <span class="math notranslate nohighlight">\(\log \sigma^2\)</span> to compute our scaling factor <span class="math notranslate nohighlight">\(\sigma\)</span> we will have to do:</p>
<div class="math notranslate nohighlight">
\[ \Large
e^{0.5*\log(\sigma^2)} = e^{\log(\sqrt{\sigma^2})} = e^{\log\sigma} = \sigma
\]</div>
<p>Also our loss function we want to minimize also includes some Log Variance values so it works out to be marginally more convenient in the end to do it this way!</p>
<section id="span-style-color-lightgreen-recap-span">
<h3><span style="color:LightGreen">Recap</span><a class="headerlink" href="#span-style-color-lightgreen-recap-span" title="Permalink to this heading">#</a></h3>
<p>That was a lot of derivation! None of it should be particularly difficult, you just have to go through it step by step to make sure you understand the purpose and architecture of the model. The main takeaway is our loss function is:</p>
<div class="math notranslate nohighlight">
\[ \Large
\cal{L}_{\rm KL} =\mathop{\min}\limits_{\theta, \phi}-\frac{1}{2}(\log \sigma_q^2 + 1 - \sigma_q^2 - \mu_q^2) - \mathbb{E}_q[\log p_{\phi}(x|z)]
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\mu_q\)</span> and <span class="math notranslate nohighlight">\(\log \sigma_q^2\)</span> are predicted by our Neural Network and are the parameters of our Gaussian Distribution.</p>
</section>
</section>
<section id="span-style-color-orange-helper-code-span">
<h2><span style="color:Orange">Helper Code</span><a class="headerlink" href="#span-style-color-orange-helper-code-span" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm.auto</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>

<span class="c1">### Stuff to Visualize the Latent Space ###</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">celluloid</span><span class="w"> </span><span class="kn">import</span> <span class="n">Camera</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">HTML</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">########################################</span>
<span class="c1">######### VANILLA AUTOENCODER ##########</span>
<span class="c1">########################################</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LinearVanillaAutoEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bottleneck_size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">bottleneck_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">bottleneck_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1">### Flatten Image to Vector ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1">### Pass Through Encoder ###</span>
        <span class="n">enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">### Pass Through Decoder ###</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>

        <span class="c1">### Put Decoded Image Back to Original Shape ###</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="n">dec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">enc</span><span class="p">,</span> <span class="n">dec</span>
        
<span class="k">class</span><span class="w"> </span><span class="nc">ConvolutionalAutoEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels_bottleneck</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span> <span class="o">=</span> <span class="n">channels_bottleneck</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span> 
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>

            <span class="c1">### Convolutional Encoding ###</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> 

            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> 

            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> 
            
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">conv_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">conv_enc</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward_dec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">conv_dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">conv_dec</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_enc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_dec</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">enc</span><span class="p">,</span> <span class="n">dec</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">build_embedding_plot</span><span class="p">(</span><span class="n">encoding</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>

    <span class="n">encoding</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">encoding</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">])</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;class&quot;</span><span class="p">)</span>
    <span class="n">encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">grouper</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">encoding</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">grouper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">interpolate_space</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>

    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">x_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_steps</span><span class="p">)</span>
    <span class="n">y_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_steps</span><span class="p">)</span>

    <span class="n">points</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_space</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_space</span><span class="p">:</span>
            <span class="n">points</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>

    <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1">### Pass Through Model Decoder and Reshape ###</span>
    <span class="n">dec</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward_dec</span><span class="p">(</span><span class="n">points</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">dec</span> <span class="o">=</span> <span class="n">dec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="n">dec</span> <span class="o">=</span> <span class="n">dec</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_steps</span><span class="p">,</span><span class="n">num_steps</span><span class="p">,</span> <span class="o">*</span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">num_steps</span><span class="p">,</span><span class="n">num_steps</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
            
            <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dec</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
            
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-orange-load-mnist-data-span">
<h2><span style="color:Orange">Load MNIST data</span><a class="headerlink" href="#span-style-color-orange-load-mnist-data-span" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wget_data</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/data/MNIST/t10k-images-idx3-ubyte&#39;</span><span class="p">,</span> <span class="s1">&#39;tmp_data/MNIST/raw&#39;</span><span class="p">)</span>
<span class="n">wget_data</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/data/MNIST/t10k-labels-idx1-ubyte&#39;</span><span class="p">,</span>  <span class="s1">&#39;tmp_data/MNIST/raw&#39;</span><span class="p">)</span>
<span class="n">wget_data</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/data/MNIST/train-images-idx3-ubyte&#39;</span><span class="p">,</span> <span class="s1">&#39;tmp_data/MNIST/raw&#39;</span><span class="p">)</span>
<span class="n">wget_data</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/data/MNIST/train-labels-idx1-ubyte&#39;</span><span class="p">,</span> <span class="s1">&#39;tmp_data/MNIST/raw&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File ‘tmp_data/MNIST/raw/t10k-images-idx3-ubyte’ already there; not retrieving.
File ‘tmp_data/MNIST/raw/t10k-labels-idx1-ubyte’ already there; not retrieving.
File ‘tmp_data/MNIST/raw/train-images-idx3-ubyte’ already there; not retrieving.
File ‘tmp_data/MNIST/raw/train-labels-idx1-ubyte’ already there; not retrieving.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Seed Everything ###</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">### Prep Dataset ###</span>
<span class="n">tensor_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./tmp_data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">tensor_transforms</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./tmp_data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">tensor_transforms</span><span class="p">)</span>

<span class="c1">### Set Device ###</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-orange-building-a-simple-linear-variational-autoencoder-span">
<h2><span style="color:Orange">Building a Simple Linear Variational Autoencoder</span><a class="headerlink" href="#span-style-color-orange-building-a-simple-linear-variational-autoencoder-span" title="Permalink to this heading">#</a></h2>
<p>We will take the regular AutoEecoder we wrote in our previous lecture and make the necessary changes to make it a Variational Autoencoder! As you will see, that was a lot of math to justify this small change!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LinearVariationalAutoEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1">#########################################################</span>
        <span class="c1">### The New Layers Added in from Original AutoEncoder ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fn_mu</span> <span class="o">=</span>  <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fn_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="c1">#########################################################</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">#############################################</span>
        <span class="c1">### Compute Mu and Sigma ###</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fn_mu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fn_logvar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1">### Sample with Reparamaterization Trick ###</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sigma</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">noise</span>
        <span class="c1">############################################</span>
        
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward_dec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1">### Flatten Image to Vector ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1">### Pass Through Encoder ###</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_enc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">### Pass Sampled Data Through Decoder ###</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="c1">### Put Decoded Image Back to Original Shape ###</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="n">dec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">dec</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

<span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearVariationalAutoEncoder</span><span class="p">()</span>
<span class="n">encoded</span><span class="p">,</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">rand</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-orange-define-the-loss-function-span">
<h2><span style="color:Orange">Define the Loss Function</span><a class="headerlink" href="#span-style-color-orange-define-the-loss-function-span" title="Permalink to this heading">#</a></h2>
<p>The purpose of a VAE model is to simultaneous improve reconstruction loss and ensure the latent space is close to a standard normal distribution. We will also add a weight to the KL Divergence term so we can see what the model does when we place high emphasis vs low emphasis on our latent space regularization.</p>
<p><em><strong><span style="color:Violet">This is Super Important</span>:</strong></em> You will see this loss implemented in different ways and its really important to keep in mind the scale of your loss. Remember, we will be adding reconstruction and kl together. If the reconstruction is larger than KL, then the model will put more emphasis on reconstruction. And the other way, if KL is much larger than the reconstruction, then the model will focus on optimizing your KL and not reconstruction. So here is the rule of thumb from my testing:</p>
<ol class="arabic simple">
<li><p><span style="color:LightGreen">Average Mean Squared Pixel Loss</span>: If you are directly applying MSE to your pixel images, the reconstruction value will be very small (imagine dividing a number by all the pixels in your image, thats a lot of pixels). If you go down this route, you will have to put a much smaller weight on your KL, and will require some testing (I had good results between 0.0001 and 0.0005)</p></li>
<li><p><span style="color:LightGreen">Average Mean Squared Error Image Loss</span>: In this case, we will still compute MSE on our pixels, but we will sum up the errors for <strong>EACH IMAGE</strong> and then average the summed errors across the image. In this case, I have seen that having KL weight of 1 is fine!</p></li>
</ol>
<p>We can also do the same for our KL Divergence. So we will compute the KL divergence per image, sum them up for all the latent vectors per image, and then average across the image.</p>
<p>Obviously, both are the same. Mimimizing the first option is equivalent to the second, but there is a reason the second is better. When we compute backpropagation, if your Loss value is very small, then the gradients will also be very small, which is ok in most scenarios unless you want to do mixed precision training, in which case you may hit a gradient underflow (nan loss) error. So we will just go with the second method for this. This is important to keep in mind though, I saw different implementations required different weights to get good training, so if you aren’t getting good results, maybe take a look at this first.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">VAELoss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">log_var</span><span class="p">,</span> <span class="n">kl_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">reconstruction_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

    <span class="c1">### Compute the MSE For Every Pixel [B, C, H, W] ###</span>
    <span class="n">pixel_mse</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">x_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1">### Flatten Each Image in Batch to Vector [B, C*H*W] ###</span>
    <span class="n">pixel_mse</span> <span class="o">=</span> <span class="n">pixel_mse</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1">### Sum  Up Pixel Loss Per Image and Average Across Batch ###</span>
    <span class="n">reconstruction_loss</span> <span class="o">=</span> <span class="n">pixel_mse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1">### Compute KL Per Image and Sum Across Flattened Latent ###</span>
    <span class="n">kl</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span> <span class="n">mean</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">kl_per_image</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1">### Average KL Across the Batch ###</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl_per_image</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">reconstruction_weight</span><span class="o">*</span><span class="n">reconstruction_loss</span> <span class="o">+</span> <span class="n">kl_weight</span><span class="o">*</span><span class="n">kl_loss</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-orange-training-script-span">
<h2><span style="color:Orange">Training Script</span><a class="headerlink" href="#span-style-color-orange-training-script-span" title="Permalink to this heading">#</a></h2>
<p>This training script is basically identical to the one in the <a class="reference external" href="https://illinois-mlp.github.io/MachineLearningForPhysics/_sources/lectures/AutoEncoders.html">AutoEncoders notebook</a>, so take a look there! Just like before we will store the latent dimension for visualization purposes, but in the vanilla autoencoder it was the output of our encoder, but now it will be the random sample from a normal distribution paramaterized by our predicted means and log-variances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
          <span class="n">kl_weight</span><span class="p">,</span>
          <span class="n">train_set</span><span class="p">,</span>
          <span class="n">test_set</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="p">,</span> 
          <span class="n">training_iterations</span><span class="p">,</span> 
          <span class="n">evaluation_iterations</span><span class="p">,</span>
          <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;VAE&quot;</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">model_type</span> <span class="o">!=</span> <span class="s2">&quot;VAE&quot;</span><span class="p">:</span> <span class="n">kl_weight</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">)</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">evaluation_loss</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">encoded_data_per_eval</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">evaluation_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">training_iterations</span><span class="p">))</span>
    
    <span class="n">train</span> <span class="o">=</span> <span class="kc">True</span>
    
    <span class="n">step_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">train</span><span class="p">:</span>

        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">trainloader</span><span class="p">:</span>
            
            <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;VAE&quot;</span><span class="p">:</span>
                <span class="n">encoded</span><span class="p">,</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">VAELoss</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">kl_weight</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;AE&quot;</span><span class="p">:</span> 
                <span class="n">encoded</span><span class="p">,</span> <span class="n">decoded</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">images</span><span class="o">-</span><span class="n">decoded</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                
            <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">step_counter</span> <span class="o">%</span> <span class="n">evaluation_iterations</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                
                <span class="n">encoded_evaluations</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>

                    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;VAE&quot;</span><span class="p">:</span>
                        <span class="n">encoded</span><span class="p">,</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
                        <span class="n">loss</span> <span class="o">=</span> <span class="n">VAELoss</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">kl_weight</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;AE&quot;</span><span class="p">:</span> 
                        <span class="n">encoded</span><span class="p">,</span> <span class="n">decoded</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
                        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">images</span><span class="o">-</span><span class="n">decoded</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

                    <span class="n">evaluation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

                    <span class="n">encoded</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">encoded</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    
                    <span class="n">encoded_evaluations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">encoded</span><span class="p">,</span> <span class="n">labels</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>


                <span class="n">encoded_data_per_eval</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">encoded_evaluations</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

                <span class="n">train_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
                <span class="n">evaluation_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">evaluation_loss</span><span class="p">)</span>

                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
                <span class="n">evaluation_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evaluation_loss</span><span class="p">)</span>
                
                <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">evaluation_loss</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

            <span class="n">step_counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
           
            
            <span class="k">if</span> <span class="n">step_counter</span> <span class="o">&gt;=</span> <span class="n">training_iterations</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Completed Training!&quot;</span><span class="p">)</span>
                <span class="n">train</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>

    <span class="n">encoded_data_per_eval</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">encoded_data_per_eval</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Training Loss&quot;</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Evaluation Loss&quot;</span><span class="p">,</span> <span class="n">evaluation_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">evaluation_losses</span><span class="p">,</span> <span class="n">encoded_data_per_eval</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">encoded_datas</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">### Train our Vanilla AutoEncoder </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearVanillaAutoEncoder</span><span class="p">()</span>
<span class="n">model</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">evaluation_losses</span><span class="p">,</span> <span class="n">encoded_data_per_eval</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                                                      <span class="n">kl_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                                      <span class="n">train_set</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span>
                                                                      <span class="n">test_set</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span>
                                                                      <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                                                      <span class="n">training_iterations</span><span class="o">=</span><span class="mi">25000</span><span class="p">,</span>
                                                                      <span class="n">evaluation_iterations</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
                                                                      <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;AE&quot;</span><span class="p">)</span>

<span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">encoded_datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_data_per_eval</span><span class="p">)</span>

<span class="c1">### Train our Variational AutoEncoders with different KL Weights ###</span>
<span class="n">kl_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="k">for</span> <span class="n">kl_weight</span> <span class="ow">in</span> <span class="n">kl_weights</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KL Weight:&quot;</span><span class="p">,</span> <span class="n">kl_weight</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LinearVariationalAutoEncoder</span><span class="p">()</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">evaluation_losses</span><span class="p">,</span> <span class="n">encoded_data_per_eval</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                                                          <span class="n">kl_weight</span><span class="o">=</span><span class="n">kl_weight</span><span class="p">,</span>
                                                                          <span class="n">train_set</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span>
                                                                          <span class="n">test_set</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span>
                                                                          <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                                                          <span class="n">training_iterations</span><span class="o">=</span><span class="mi">25000</span><span class="p">,</span>
                                                                          <span class="n">evaluation_iterations</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> 
                                                                          <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;VAE&quot;</span><span class="p">)</span>

    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">encoded_datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_data_per_eval</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9cd46b4b208845ad9bee7ce56ff63c5a", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Completed Training!
Final Training Loss 0.029244251549243927
Final Evaluation Loss 0.029575829162814055
KL Weight: 1
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "f248b75a1c214ab8b16b8b14a660670f", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Completed Training!
Final Training Loss 36.65879724884033
Final Evaluation Loss 36.869628881952565
KL Weight: 100
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e13e98c23e5c46628aa856e37c650380", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Completed Training!
Final Training Loss 57.90670295715332
Final Evaluation Loss 58.16395097477421
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-orange-comparing-latents-span">
<h2><span style="color:Orange">Comparing Latents</span><a class="headerlink" href="#span-style-color-orange-comparing-latents-span" title="Permalink to this heading">#</a></h2>
<p>Lets compare the latent dimensions of our model trained with different KL Weights. Remember that the higher the KL divergence, the greater our regularization on the latent space to be closer to gaussian. Lets see what these spaces look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_latents</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">encoded_datas</span><span class="p">]</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Vanilla Autoencoder&quot;</span><span class="p">,</span> <span class="s2">&quot;VAE KL Weight: 1&quot;</span><span class="p">,</span> <span class="s2">&quot;VAE KL Weight: 100&quot;</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model_latents</span><span class="p">):</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">encoding</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">])</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;class&quot;</span><span class="p">)</span>
    <span class="n">encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">grouper</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">encoding</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">grouper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">model_names</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="c1">### Grab Models for Future Visualization ###</span>
<span class="n">vanilla_ae</span><span class="p">,</span> <span class="n">vae</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">models</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/bdab8cb45f69748a385d74617531f5bf86bc46aa168b6fc851dc62e3c77caa71.png" src="../../_images/bdab8cb45f69748a385d74617531f5bf86bc46aa168b6fc851dc62e3c77caa71.png" />
</div>
</div>
</section>
<section id="span-style-color-orange-interpolate-the-latent-space-span">
<h2><span style="color:Orange">Interpolate the Latent Space</span><a class="headerlink" href="#span-style-color-orange-interpolate-the-latent-space-span" title="Permalink to this heading">#</a></h2>
<p>Lets interpolate the latent space and see the quality of transitions between digit classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Pass in the different models we made too see reconstruction!</span>
<span class="n">interpolate_space</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/132287472f64b41b9ee7053ed9c94d67b28d15b8493977dee1234459729ddf89.png" src="../../_images/132287472f64b41b9ee7053ed9c94d67b28d15b8493977dee1234459729ddf89.png" />
</div>
</div>
</section>
<section id="span-style-color-orange-generating-new-data-span">
<h2><span style="color:Orange">Generating New Data</span><a class="headerlink" href="#span-style-color-orange-generating-new-data-span" title="Permalink to this heading">#</a></h2>
<p>If we want to generate new data from our VAE, all we have to do is pass gaussian noise to the decoder and let it create a new sample!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Generate Noise and Pass through Decoder ###</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">generated</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">forward_dec</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>

<span class="c1">### Reshape Generated Back to Image Shape and Display ###</span>
<span class="n">generated</span> <span class="o">=</span> <span class="n">generated</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">generated</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/51940312c18aad5bcae60ee402614bd2d97606c2d77644b9fe353d0f4a44f702.png" src="../../_images/51940312c18aad5bcae60ee402614bd2d97606c2d77644b9fe353d0f4a44f702.png" />
</div>
</div>
</section>
<section id="span-style-color-orange-lets-build-a-convolutional-variational-autoencoder-span">
<h2><span style="color:Orange">Lets Build A Convolutional Variational AutoEncoder</span><a class="headerlink" href="#span-style-color-orange-lets-build-a-convolutional-variational-autoencoder-span" title="Permalink to this heading">#</a></h2>
<p>Convolutional VAE will be basically the same thing, expect we aren’t going to use any linear layers. Remember from our last tutorial on basic Autoencoders, when we wrote the Convolutional AutoEncoder, our image of 1024 pixels are compressed down into a <code class="docutils literal notranslate"><span class="pre">4</span> <span class="pre">channel</span> <span class="pre">x</span> <span class="pre">4</span> <span class="pre">height</span> <span class="pre">x</span> <span class="pre">4</span> <span class="pre">width</span></code> feature cube. Like we had two new linear layers to map the output of our encoder to <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>, we will have two new convolutional layers to do the same! We can treat our latent dimension to be a random cube sampled from a gaussian distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ConvolutionalVAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels_bottleneck</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span> <span class="o">=</span> <span class="n">channels_bottleneck</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span> 
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>

            <span class="c1">### Convolutional Encoding ###</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> 

            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> 

            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>

            
        <span class="p">)</span>

        <span class="c1">#########################################################</span>
        <span class="c1">### The New Layers Added in from Original AutoEncoder ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_mu</span> <span class="o">=</span>  <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)</span>
        <span class="c1">#########################################################</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> 
            
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
    

    <span class="k">def</span><span class="w"> </span><span class="nf">forward_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">conv_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">#############################################</span>
        <span class="c1">### Compute Mu and Sigma ###</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_mu</span><span class="p">(</span><span class="n">conv_enc</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_logvar</span><span class="p">(</span><span class="n">conv_enc</span><span class="p">)</span>
        
        <span class="c1">### Sample with Reparamaterization Trick ###</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sigma</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">noise</span>
        <span class="c1">############################################</span>
        
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward_dec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">conv_dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">conv_dec</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_enc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_dec</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">dec</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

<span class="c1">### Train ConvAE ###</span>
<span class="n">conv_ae_model</span> <span class="o">=</span> <span class="n">ConvolutionalAutoEncoder</span><span class="p">()</span>
<span class="n">conv_ae_model</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">evaluation_losses</span><span class="p">,</span> <span class="n">conv_ae_encoded_data_per_eval</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">conv_ae_model</span><span class="p">,</span>
                                                                                      <span class="n">kl_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                                                      <span class="n">train_set</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span>
                                                                                      <span class="n">test_set</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span>
                                                                                      <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                                                                      <span class="n">training_iterations</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span>
                                                                                      <span class="n">evaluation_iterations</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
                                                                                      <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;AE&quot;</span><span class="p">)</span>


<span class="c1">### Train ConvVAE</span>
<span class="n">conv_vae_model</span> <span class="o">=</span> <span class="n">ConvolutionalVAE</span><span class="p">()</span>
<span class="n">conv_vae_model</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">evaluation_losses</span><span class="p">,</span> <span class="n">conv_vae_encoded_data_per_eval</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">conv_vae_model</span><span class="p">,</span>
                                                                                        <span class="n">kl_weight</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                                                                                        <span class="n">train_set</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span>
                                                                                        <span class="n">test_set</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span>
                                                                                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                                                                        <span class="n">training_iterations</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span>
                                                                                        <span class="n">evaluation_iterations</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
                                                                                        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;VAE&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b911a39a368d458b93ce1c57d7785cf0", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Completed Training!
Final Training Loss 0.004188918372616172
Final Evaluation Loss 0.004006271376600786
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "28bb6fb3e9194bc6999a37e9c4902bee", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Completed Training!
Final Training Loss 44.066411651611325
Final Evaluation Loss 43.66845537902443
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-orange-lets-plot-the-latents-with-tsne-span">
<h2><span style="color:Orange">Lets Plot the Latents with TSNE</span><a class="headerlink" href="#span-style-color-orange-lets-plot-the-latents-with-tsne-span" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### TSNE Compression of ConvAE ###</span>
<span class="n">conv_ae_encoding</span> <span class="o">=</span> <span class="n">conv_ae_encoded_data_per_eval</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">conv_ae_features</span><span class="p">,</span> <span class="n">conv_ae_labels</span> <span class="o">=</span> <span class="n">conv_ae_encoding</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_ae_encoding</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">conv_ae_compressed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">conv_ae_features</span><span class="p">)</span>

<span class="n">conv_ae_encoding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">conv_ae_compressed</span><span class="p">,</span> <span class="n">conv_ae_labels</span><span class="p">))</span>
<span class="n">conv_ae_encoding</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">conv_ae_encoding</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">])</span>
<span class="n">conv_ae_encoding</span> <span class="o">=</span> <span class="n">conv_ae_encoding</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;class&quot;</span><span class="p">)</span>
<span class="n">conv_ae_encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv_ae_encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

<span class="c1">### TSNE Compression of ConvVAE ###</span>
<span class="n">conv_vae_encoding</span> <span class="o">=</span> <span class="n">conv_vae_encoded_data_per_eval</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">conv_vae_features</span><span class="p">,</span> <span class="n">conv_vae_labels</span> <span class="o">=</span> <span class="n">conv_vae_encoding</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_vae_encoding</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">conv_vae_compressed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">conv_vae_features</span><span class="p">)</span>

<span class="n">conv_vae_encoding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">conv_vae_compressed</span><span class="p">,</span> <span class="n">conv_vae_labels</span><span class="p">))</span>
<span class="n">conv_vae_encoding</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">conv_vae_encoding</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">])</span>
<span class="n">conv_vae_encoding</span> <span class="o">=</span> <span class="n">conv_vae_encoding</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;class&quot;</span><span class="p">)</span>
<span class="n">conv_vae_encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv_vae_encoding</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

<span class="c1">### Plot Compression! ###</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">grouper</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">conv_ae_encoding</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">grouper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">grouper</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">conv_vae_encoding</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">grouper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Convolutional Autoencoder&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Convolutional Variational Autoencoder&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9cd15876e6dcacf2d9a99c5c622be5b9cdc8428489a40ac7d58eaa762655812b.png" src="../../_images/9cd15876e6dcacf2d9a99c5c622be5b9cdc8428489a40ac7d58eaa762655812b.png" />
</div>
</div>
</section>
<section id="span-style-color-orange-taking-a-closer-look-at-reconstruction-span">
<h2><span style="color:Orange">Taking a Closer Look at Reconstruction</span><a class="headerlink" href="#span-style-color-orange-taking-a-closer-look-at-reconstruction-span" title="Permalink to this heading">#</a></h2>
<p>Although VAE’s have a nice latent space, we fall into another problem, blurring.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generated_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">test_set</span><span class="p">[</span><span class="n">generated_index</span><span class="p">]</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ae_reconstructed</span> <span class="o">=</span> <span class="n">vanilla_ae</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">vae_reconstructed</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">vae</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">conv_ae_reconstructed</span> <span class="o">=</span> <span class="n">conv_ae_model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">conv_vae_reconstructed</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">conv_vae_model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>


<span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">ae_reconstructed</span> <span class="o">=</span> <span class="n">ae_reconstructed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">vae_reconstructed</span> <span class="o">=</span> <span class="n">vae_reconstructed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">conv_ae_reconstructed</span> <span class="o">=</span> <span class="n">conv_ae_reconstructed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">conv_vae_reconstructed</span> <span class="o">=</span> <span class="n">conv_vae_reconstructed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">ae_reconstructed</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;AE&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vae_reconstructed</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;VAE&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">conv_ae_reconstructed</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;ConvAE&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">conv_vae_reconstructed</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;ConvVAE&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c4f6b6b15508fe43dca4e2ba1954989d8a049c89a6b95e269f8fa325c27c18a7.png" src="../../_images/c4f6b6b15508fe43dca4e2ba1954989d8a049c89a6b95e269f8fa325c27c18a7.png" />
</div>
</div>
<p>As you can see, it looks like the reconstructions of the VAE are slightly more blurry compared to a regular autoencoder. Due to the regularized nature of our latent space, we have forced smooth transitions between different classes. Lets pretend we trained a VAE on Dogs vs Cats. Does it make sense to have representations that are a mixture of dogs and cats? Not really, our goal is to generate dogs or cats, not some weird hybrid of the two. So in this case, we would need to give an opportunity to discretize the latent space in some way.</p>
</section>
<section id="span-style-color-orange-acknowledgments-span">
<h2><span style="color:Orange">Acknowledgments</span><a class="headerlink" href="#span-style-color-orange-acknowledgments-span" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Initial version: Mark Neubauer</p>
<ul>
<li><p>Modified from the following <a class="reference external" href="https://github.com/priyammaz/PyTorch-Adventures/tree/main/PyTorch%20for%20Generation/AutoEncoders/Intro%20to%20AutoEncoders">tutorial</a></p></li>
</ul>
</li>
</ul>
<p>© Copyright 2025</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./_sources/lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="AutoEncoders.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Autoencoders</p>
      </div>
    </a>
    <a class="right-next"
       href="GenerativeAdversarialNetworks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Generative Adversarial Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-what-are-generative-models-span"><span style="color:Orange">What are Generative Models?</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-vanilla-autoencoders-as-a-generative-model-span"><span style="color:Orange">Vanilla Autoencoders as a Generative Model</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-variational-autoencoders-regularized-latent-spaces-span"><span style="color:Orange">Variational AutoEncoders: Regularized Latent Spaces</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-how-do-we-force-latents-to-be-gaussian-span"><span style="color:Orange">How do we Force Latents to be Gaussian?</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-imposing-standard-normal-distributions-on-our-latent-variables-span"><span style="color:Orange">Imposing Standard Normal Distributions on our Latent Variables</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-and-thats-it-almost-span"><span style="color:Orange">And thats it! (Almost)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-last-thing-logvariance-span"><span style="color:Orange">Last Thing - LogVariance</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-recap-span"><span style="color:LightGreen">Recap</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-helper-code-span"><span style="color:Orange">Helper Code</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-load-mnist-data-span"><span style="color:Orange">Load MNIST data</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-building-a-simple-linear-variational-autoencoder-span"><span style="color:Orange">Building a Simple Linear Variational Autoencoder</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-define-the-loss-function-span"><span style="color:Orange">Define the Loss Function</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-training-script-span"><span style="color:Orange">Training Script</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-comparing-latents-span"><span style="color:Orange">Comparing Latents</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-interpolate-the-latent-space-span"><span style="color:Orange">Interpolate the Latent Space</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-generating-new-data-span"><span style="color:Orange">Generating New Data</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-lets-build-a-convolutional-variational-autoencoder-span"><span style="color:Orange">Lets Build A Convolutional Variational AutoEncoder</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-lets-plot-the-latents-with-tsne-span"><span style="color:Orange">Lets Plot the Latents with TSNE</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-taking-a-closer-look-at-reconstruction-span"><span style="color:Orange">Taking a Closer Look at Reconstruction</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mark Neubauer
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>