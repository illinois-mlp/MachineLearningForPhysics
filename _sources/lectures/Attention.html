

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Attention &#8212; PHYS 498 MLP</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_sources/lectures/Attention';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Transformers" href="Transformers.html" />
    <link rel="prev" title="Attention Mechanism and Transformers" href="../Week_08.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="PHYS 498 MLP - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="PHYS 498 MLP - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    <span style="color:Blue">Machine Learning for Physics</span>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_01.html"><span style="color: blue;"><b>Introduction to Data Science</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1cQJycGyQ07qSOoeskr6GjjTD3byIkxDbdi228NRgFeU/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="JupyterNumpy.html">Jupyter Notebooks and Numerical Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="Pandas.html">Handling Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Visualization.html">Visualizing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Clustering.html">Finding Structure in Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Dimensionality.html">Measuring and Reducing Dimensionality</a></li>
<li class="toctree-l2"><a class="reference internal" href="Nonlinear.html">Adapting Linear Methods to Non-Linear Data and Kernel Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_01.html">Homework 01: Introduction to Data Science</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_02.html"><span style="color: blue;"><b>Probability Theory and Density Estimation</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1o9tM9ppKZWIa9B3WIHy5JDF4myR5NkO02W6WAlyiTSg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityTheory.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityDistributions.html">Important Probability Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="DensityEstimation.html">Estimating Probability Density from Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_02.html">Homework 02: Probability Theory and Density Estimation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_03.html"><span style="color: blue;"><b>Bayesian Statistics I</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1h2SMuH-Z5a_OE6UMDbFjysEiL2NmYT1tGww6VF5jzsA/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Statistics.html">Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="BayesianInference.html">Bayesian Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="MarkovChainMonteCarlo.html">Markov Chain Monte Carlo in Practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="MarkovChains.html">Stochastic Processes and Markov-Chain Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_03.html">Homework 03: Bayesian Statistics and Markov Chains</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_04.html"><span style="color: blue;"><b>Bayesian Statistics II</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/18bft9_CiBLjjBy0MHvT_vN7E95kfakvhm_7d7WKHXyY/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ModelSelection.html">Bayesian Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="VariationalInference.html">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="Optimization.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="CrossValidation.html">Cross Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_04.html">Homework 04: Metropolis-Hastings and Cross Validation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_05.html"><span style="color: blue;"><b>Introduction to Artificial Intelligence</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1by3-6jDEorKi7_WEr6PTMfEBE8f4xrS94fNdtSuATVg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="SupervisedLearning.html">Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="Learning.html">Artificial Intelligence and Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ArtificialNeuralNetworks.html">Artificial Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="DeepLearning.html">Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_05.html">Homework 05: Artificial Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_06.html"><span style="color: blue;"><b>Convolutional and Recurrent Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1cDFVtEVGLaWd4256OShSb3Roto0x4y4GwG6LkhVozg0/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ConvolutionalRecurrentNeuralNetworks.html">Convolutional and Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_06.html">Homework 06: Forecasting Projectile Motion with Recurrent Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_07.html"><span style="color: blue;"><b>Geometric Deep Learning and Graph Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1jK61M3QGH7bxFU7TMBm16G3YDb-7HOFtgNdqlj3Gs38/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="GraphNeuralNetworks.html">Geometric Deep Learning and Graph Neural Networks</a></li>





</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../Week_08.html"><span style="color: blue;"><b>Attention Mechanism and Transformers</b></span></a><input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1ZHuK7TopASFSoyUoELKeCGT8bullhtSLcEkrp4ZueGg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="Transformers.html">Transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="VisionTransformer.html">Vision Transformer</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_01.html"><span style="color: blue;"><b>Project 01</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_HiggsTauTau.html">Higgs Boson Decaying to Tau Leptons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_ExoticParticles.html">Searching for Exotic Particles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_GalaxyZoo.html">Galaxy Zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_NuclearGeometryQGP.html">Nuclear Geometry and Characterization of the Quark Gluon Plasma</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_AberratedImages.html">Aberrated Image Recovery of Ultracold Atoms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_DarkEnergySurvey.html">Dark Energy Survey</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_GravitationalWaves.html">Detection of Gravitational Waves</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_09.html"><span style="color: blue;"><b>Generative Modeling and Simulation-Based Inference</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1h13YeUjtTU_WHLxghxFBBQJO3uRr1GtsIyO4DVZviJo/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="GenerativeModeling.html">Generative Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="AutoEncoders.html">Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="VariationalAutoEncoders.html">Variational AutoEncoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="GenerativeAdversarialNetworks.html">Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="Diffusion.html">Diffusion Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="NormalizingFlows.html">Normalizing Flows</a></li>
<li class="toctree-l2"><a class="reference internal" href="SimulationBasedInference.html">Simulation Based Inference</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_10.html"><span style="color: blue;"><b>Reinforcement Learning</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1EsW71u3hdNdXyhlDfkmOX__9c4wJZUee_pjlsmlv_Vg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ReinforcementLearning.html">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_07.html">Homework 07: Reinforcement Learning: Implementing a Deep Q-Network</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_11.html"><span style="color: blue;"><b>AI Explainablility and Uncertainty Quantification</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1ydzY7IEYzALTR6ez5gvwwKDduf_7wUtZddq0SUSuvI0/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="AIExplainabilityUncertaintyQuantification.html">AI Explainability and Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_08.html">Homework 08: Detecting Distribution Shift on MNIST using Bayesian Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_12.html"><span style="color: blue;"><b>Unsupervised Learning and Anomaly Detection</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1ydzY7IEYzALTR6ez5gvwwKDduf_7wUtZddq0SUSuvI0/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="UnsupervisedLearningAnomalyDetection.html">Unsupervised Learning and Anomaly Detection</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_13.html"><span style="color: blue;"><b>Physics Informed Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1C-Z8b6WP5rE8yohZQdSxyH8O_bHIbllq97QhEJYyh0w/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="PhysicsInformedNeuralNetworks.html">Physics Informed Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="LearningTheSchrodingerEquation.html">Solving the Time Dependent Schrodinger Equation with Physics-Informed Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="SymbolicRegression.html">Introduction to Symbolic Regression</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_02.html"><span style="color: blue;"><b>Project 02</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_AnisotropyQGP.html">Anisotropy in the Quark Gluon Plasma</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_AberratedImages.html">Aberrated Image Recovery of Ultracold Atoms</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_14.html"><span style="color: blue;"><b>Learning from the Machines</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1hkfaU7JVy1f5S8jURZvTY67KRzP7I8zGRf4Zku_bpM4/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="LearningPhysicsMachines.html">Learning Physics from the Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_15.html"><span style="color: blue;"><b>Future of AI and Physics: What Lies Ahead?</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1eB1qCn5J07D5he_DCpkBiKjbVjdI6OexUzMQ351GCaI/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="LookingForward.html">Future of AI and Physics: What Lies Ahead?</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/illinois-mlp/MachineLearningForPhysics/blob/main/_sources/lectures/Attention.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/illinois-mlp/MachineLearningForPhysics" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/_sources/lectures/Attention.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Attention</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-recap-recurrent-neural-networks-span"><span style="color:Orange">Recap: Recurrent Neural Networks</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-attention-augmented-rnn-span"><span style="color:Orange">Attention Augmented RNN</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-problems-span"><span style="color:LightGreen">Problems</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-attention-is-all-you-need-span"><span style="color:Orange">Attention is All You Need!</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-step-1-compute-the-attention-matrix-span"><span style="color:LightGreen">Step 1: Compute the Attention Matrix</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-recap-the-dot-product-span"><span style="color:LightPink">Recap: The Dot Product</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-recap-matrix-multiplication-span"><span style="color:LightPink">Recap: Matrix Multiplication</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-step-2-weighting-the-values-matrix-span"><span style="color:LightGreen">Step 2: Weighting the Values Matrix</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-enforcing-causality-span"><span style="color:LightGreen">Enforcing Causality</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-lets-build-this-span"><span style="color:LightGreen">Lets Build This!</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-implement-attention-without-any-learnable-parameters-span"><span style="color:LightPink">Implement Attention Without Any Learnable Parameters</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-lets-add-learnable-parameters-span"><span style="color:LightPink">Lets Add Learnable Parameters</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-multiheaded-attention-span"><span style="color:LightGreen">MultiHeaded Attention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-single-headed-attention-recap-span"><span style="color:LightPink">Single Headed Attention Recap</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-moving-to-multiheaded-attention-span"><span style="color:LightPink">Moving to MultiHeaded Attention</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-increasing-efficiency-span"><span style="color:LightGreen">Increasing Efficiency</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-linear-layers-on-multidimensional-tensors-span"><span style="color:LightPink">Linear Layers on MultiDimensional Tensors</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-packing-linear-layers-span"><span style="color:LightPink">Packing Linear Layers</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-multidimensional-matrix-multiplication-span"><span style="color:LightPink">MultiDimensional Matrix Multiplication</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-recap-span"><span style="color:LightPink">Recap</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-the-trick-of-parallelizing-heads-span"><span style="color:LightGreen">The Trick of Parallelizing Heads</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-more-efficient-attention-implementation-span"><span style="color:LightGreen">More Efficient Attention Implementation</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-padding-span"><span style="color:Orange">Padding</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-sequence-padding-and-attention-masking-span"><span style="color:LightGreen">Sequence Padding and Attention Masking</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-computing-the-reweighted-padded-attention-mask-span"><span style="color:Orange">Computing the Reweighted Padded Attention Mask</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-add-padding-to-our-attention-computation-span"><span style="color:LightGreen">Add Padding to our Attention Computation</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-repeating-to-match-attention-matrix-shape-span"><span style="color:LightGreen">Repeating to Match Attention Matrix Shape</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-matching-shapes-for-multihead-attention-span"><span style="color:LightGreen">Matching Shapes for MultiHead Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><span style="color:LightGreen">Enforcing Causality</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-computing-the-reweighted-causal-attention-mask-span"><span style="color:LightGreen">Computing the Reweighted Causal Attention Mask</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-mask-span"><span style="color:LightGreen">Attention Mask</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-lets-incorporate-the-causal-mask-into-self-attention-span"><span style="color:LightGreen">Lets Incorporate the Causal Mask into Self-Attention</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-cross-attention-span"><span style="color:Orange">Cross Attention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-cross-attention-queries-keys-and-values-span"><span style="color:LightGreen">Cross Attention Queries, Keys and Values</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-tensor-shapes-for-cross-attention-span"><span style="color:LightGreen">Tensor Shapes for Cross Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-performing-the-attention-computation-span"><span style="color:LightGreen">Performing the Attention Computation</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-the-most-important-part-span"><span style="color:LightGreen">THE MOST IMPORTANT PART!!!</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-basically-nothing-changed-span"><span style="color:LightGreen">Basically Nothing Changed</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-padding-on-english-and-french-span"><span style="color:LightGreen">Padding on English and French</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-implementing-cross-attention-span"><span style="color:Orange">Implementing Cross Attention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><span style="color:LightGreen">Matching Shapes for MultiHead Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-flash-attention-span"><span style="color:LightGreen">Flash Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-lets-add-flash-attention-to-our-self-attention-span"><span style="color:LightGreen">Lets Add Flash Attention to our Self Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-putting-it-all-together-span"><span style="color:LightGreen">Putting it All Together</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-class-details-span"><span style="color:LightGreen">Attention Class Details</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-mask-for-self-attention-span"><span style="color:LightGreen">Attention Mask for Self-Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-mask-for-cross-attention-span"><span style="color:LightGreen">Attention Mask for Cross-Attention</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-thats-it-span"><span style="color:Orange">Thats It!</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="attention">
<h1>Attention<a class="headerlink" href="#attention" title="Permalink to this heading">#</a></h1>
<p>Since its introduction via the original transformer paper (<a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>), self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing (NLP). Since self-attention is now everywhere, it’s important to understand how it works. This lecture provides and overview of the key aspects of the attention mechanism and details (code) on how it works.</p>
<p>Although attention was mainly intended for use in sequence modeling, it has found its way into Computer Vision, Graphs and basically every domain, demonstrating the flexibility of the architecture. We will discuss attention from a sequence modeling perspective just to build intuition on how this works.</p>
<section id="span-style-color-orange-recap-recurrent-neural-networks-span">
<h2><span style="color:Orange">Recap: Recurrent Neural Networks</span><a class="headerlink" href="#span-style-color-orange-recap-recurrent-neural-networks-span" title="Permalink to this heading">#</a></h2>
<p>To start the explanation, lets reference back the original sequence modeling mechanism: <span style="color:Violet">Recurrent Neural Networks</span> (RNNs). You might review the RNN part of this <a class="reference external" href="https://illinois-mlp.github.io/MachineLearningForPhysics/_sources/lectures/ConvolutionalRecurrentNeuralNetworks.html">notebook</a> from earlier in the course.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-recurrent_neural_network_diagram.png" width=800></img>
</div><p>In recurrent neural networks, what we typically do is take our sequence and pass in a single timestep at a time and produce an output. This means when we pass in <span class="math notranslate nohighlight">\(x_1\)</span> we create a hidden state <span class="math notranslate nohighlight">\(h_1\)</span> that captures all the relevant information in the input, and this hidden state then is used to produce the output <span class="math notranslate nohighlight">\(y_1\)</span>. Now what makes it an RNN is when we pass in the second timestep <span class="math notranslate nohighlight">\(x_2\)</span> to produce the hidden state <span class="math notranslate nohighlight">\(h_2\)</span>, the hidden state already contains information about the past <span class="math notranslate nohighlight">\(h_1\)</span> Therefore our output of <span class="math notranslate nohighlight">\(y_2\)</span> is informed both by information from <span class="math notranslate nohighlight">\(x_2\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span> encoded through the hidden states. If we keep this going, when we want to make a prediction at <span class="math notranslate nohighlight">\(y_{100}\)</span>, we will be using a hidden state that has encoded information of all the inputs <span class="math notranslate nohighlight">\(x_1\)</span> to <span class="math notranslate nohighlight">\(x_{100}\)</span>.</p>
<p>Everything explained so far is a <span style="color:Violet">causal RNN</span> which makes a prediction of some timestep <span class="math notranslate nohighlight">\(t\)</span> by using all of the input timesteps <span class="math notranslate nohighlight">\(&lt;=t\)</span>. We can easily expand this to make a <span style="color:Violet">bidirectional RNN</span> to make a prediction at time <span class="math notranslate nohighlight">\(t\)</span> by looking at the entire sequence as well. In this case we will really have <span style="color:Violet">two hidden states</span> - one that looks backwards and another that looks forward.</p>
<p>Whether you use causal or bidirectional depends a lot on what you want to do. If you want to do Name Entity Recognition (i.e. determine if each word in a sentence is an entity), you can look at the entire sentence to do this. On the other hand if you want to forecast the future, like a stock price, then you have to use causal as you can only look at the past to predict the future.</p>
<p>All this sounds well and good, but there was one glaring problem: <em><strong>Memory</strong></em>. The hidden states we use to encode the history can only contain so much information, i.e. as the sequence length becomes longer the model will have to start to forget based on memory limitations. This matters a lot for things like MLP, as there may be imporant relations between parts of a book that are pages, or even chapters, apart. To solve this issue, <span style="color:Violet">Attention Augmented RNNs</span> were introduced in the paper <a class="reference external" href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation By Jointly Learning To Align and Translate</a>.</p>
</section>
<section id="span-style-color-orange-attention-augmented-rnn-span">
<h2><span style="color:Orange">Attention Augmented RNN</span><a class="headerlink" href="#span-style-color-orange-attention-augmented-rnn-span" title="Permalink to this heading">#</a></h2>
<p>If I had to use two words to define attention it would be: <span style="color:Violet">Weighted Average</span>. In Attention Augmented RNN paper, the authors call the hidden states <em><span style="color:Violet">annotations</span></em>, but they are the same thing as attention.</p>
<p>So lets go back to our RNN again. Before we do our prediction for <span class="math notranslate nohighlight">\(y_t\)</span>, we have a sequence of hidden states <span class="math notranslate nohighlight">\(h_t\)</span> that contain the information about the sequence <span class="math notranslate nohighlight">\(x_t\)</span> itself produced from the RNN mechanism. Again, the problem is that <span class="math notranslate nohighlight">\(h_t\)</span> for large values of <span class="math notranslate nohighlight">\(t\)</span> will have forgotten imporant information about early <span class="math notranslate nohighlight">\(x_t\)</span> values with small values of <span class="math notranslate nohighlight">\(t\)</span>. So what if we got everyone to know each other again? We can produce a context vector <span class="math notranslate nohighlight">\(c_i\)</span> that is a <span style="color:Violet">weighted average</span> of all the hidden states in the case of a bidirectional architecture, or just the previous hidden states in a causal architecture. This means at any time of the context vector <span class="math notranslate nohighlight">\(c_t\)</span>, it will be a weighted average of all of the timesteps so it is reminded about more distant timesteps, solving our <em>memory</em> problem!</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-rnn_with_attention.png" width=800></img>
</div><p>Now I keep saying weighted average, and this is because for one of the timesteps, the model has to learn the weights of what is the most important information to know at those times, and then weight them higher. As per the paper, the weights were learned through an <span style="color:Violet">alignment model</span>, which was just a feedforward network that scores how well hidden states as time <span class="math notranslate nohighlight">\(t\)</span> is related to those around it in the sequence. These scores were then passed through a softmax to ensure all the learned weights sum upto 1, and then the context vectors are computed based on them. This means every context vector is a customized weighted average that learned exactly what information to put empahsis on at every timestep of the context vectors.</p>
<section id="span-style-color-lightgreen-problems-span">
<h3><span style="color:LightGreen">Problems</span><a class="headerlink" href="#span-style-color-lightgreen-problems-span" title="Permalink to this heading">#</a></h3>
<p>There are some issues with this approach, however, some which were already known about RNNs:</p>
<ul class="simple">
<li><p><em><strong><span style="color:Violet">Efficient but Slow</span></strong></em>: The RNN mechanism has a for-loop through the sequence making training very slow, but inference was efficient</p></li>
<li><p><em><strong><span style="color:Violet">Lack of Positional Information</span></strong></em>: Our context vectors are just weighted averages of hidden states, there is no information about position or time. In most sequence tasks, the order in your data appears is very important</p></li>
<li><p><em><strong><span style="color:Violet">Redundancy</span></strong></em>: We are effectively learning the same information twice - the hidden states encode sequential information, but the attention mechanism also encodes sequential information</p></li>
</ul>
</section>
</section>
<section id="span-style-color-orange-attention-is-all-you-need-span">
<h2><span style="color:Orange">Attention is All You Need!</span><a class="headerlink" href="#span-style-color-orange-attention-is-all-you-need-span" title="Permalink to this heading">#</a></h2>
<p>The groundbreaking paper, <a class="reference external" href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a> solved all of the problems above, but added a new one: <span style="color:Violet">computational cost</span>. Lets first look at what the <span style="color:Violet">Attention Mechanism</span> proposed in this paper is doing.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-attention_mechanism_visual.png" width=1000></img>
</div><p>The input is a sequence of embedding vectors and the output is a sequence of context vectors. The three linear layers which you see in the above image take three things as an input — “<span style="color:Violet">QUERY</span>, <span style="color:Violet">KEY</span> &amp; <span style="color:Violet">VALUE</span>”. Let’s take a simple example to understand them.</p>
<p>If you were to search for something on Youtube or Google, The text which you type in the search box is the <span style="color:Violet">QUERY</span>. The results which appear as the video or article title are the <span style="color:Violet">KEY</span> and the content inside them is the <span style="color:Violet">VALUE</span>. So as to find the best matches the Query has to find the similarity between it and the Keys.</p>
<p>Lets quickly look at the formulation for this:</p>
<div class="math notranslate nohighlight">
\[ \Large
\text{Attention}(Q,K,V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_e}})V
\]</div>
<p>We see some new notation show up now, <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span>, so lets define them:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q\)</span>: Queries, they are the token we are interested in</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span>: Keys, they are the other tokens we want to compare our query against</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: Values, they are the values we will weight in our weighted average</p></li>
</ul>
<p>This is a little weird so lets step through it! First important note, the <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> are three projections of our original data input <span class="math notranslate nohighlight">\(X\)</span>. This basically means we have three linear layers that all take the same input <span class="math notranslate nohighlight">\(X\)</span> to produce our <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span>.</p>
<section id="span-style-color-lightgreen-step-1-compute-the-attention-matrix-span">
<h3><span style="color:LightGreen">Step 1: Compute the Attention Matrix</span><a class="headerlink" href="#span-style-color-lightgreen-step-1-compute-the-attention-matrix-span" title="Permalink to this heading">#</a></h3>
<p>The first step is computing the <span style="color:Violet">Attention Matrix</span> with <span class="math notranslate nohighlight">\(Softmax(QK^T)\)</span>, where Q and K both have the shape <code class="docutils literal notranslate"><span class="pre">Sequence</span> <span class="pre">Length</span> <span class="pre">x</span> <span class="pre">Embedding</span> <span class="pre">Dimension</span></code>. The output of this computation will be <code class="docutils literal notranslate"><span class="pre">Sequence</span> <span class="pre">Length</span> <span class="pre">x</span> <span class="pre">Sequence</span> <span class="pre">Length</span></code>. This is what it looks like:</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-computing_attention.png" width=800></img>
</div><p>In the image above, I also applied the softmax (not shown for simplicity), so each row of the attention matrix adds up to 1 (like probabilities).</p>
<section id="span-style-color-lightpink-recap-the-dot-product-span">
<h4><span style="color:LightPink">Recap: The Dot Product</span><a class="headerlink" href="#span-style-color-lightpink-recap-the-dot-product-span" title="Permalink to this heading">#</a></h4>
<p>As a quick reminder, this whole mechanism depends on the dot product, and more specifically, its geometric interpretation</p>
<div class="math notranslate nohighlight">
\[ \Large
a\cdot b = \sum_{i=1}^n a_i*b_i = |a||b|cos(\theta)
\]</div>
<p>What the dot product really signifies is the similarity between vectors. Remember the cosine of 0 is just 1, so the highest possible cosine value would be when the vectors <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> point in the exact same direction. This means vectors that are similar in direction have higher magnitude.</p>
</section>
<hr class="docutils" />
<section id="span-style-color-lightpink-recap-matrix-multiplication-span">
<h4><span style="color:LightPink">Recap: Matrix Multiplication</span><a class="headerlink" href="#span-style-color-lightpink-recap-matrix-multiplication-span" title="Permalink to this heading">#</a></h4>
<p>Also remember, matrix multiplication is basically just a bunch of dot products, repeating the multiply/add operation repeatedly. If we are multiplying matrix <span class="math notranslate nohighlight">\(A\)</span> with matrix <span class="math notranslate nohighlight">\(B\)</span>, what we are really doing is doing the dot product of every row of <span class="math notranslate nohighlight">\(A\)</span> and every column of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<hr class="docutils" />
<p>So with our quick recaps, lets go back to the image above, when we are multiplying <span class="math notranslate nohighlight">\(Q\)</span> by <span class="math notranslate nohighlight">\(K^T\)</span>, we are multiplying each vector in the sequence <span class="math notranslate nohighlight">\(Q\)</span> by each vector in the sequence <span class="math notranslate nohighlight">\(K\)</span> and computing their dot product similarity. Again, <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(K\)</span> are just projections of the original data <span class="math notranslate nohighlight">\(X\)</span>, so really we are just computing the similarity between every possible combination of timesteps in <span class="math notranslate nohighlight">\(X\)</span>. We also could have just done <span class="math notranslate nohighlight">\(XX^T\)</span>, this would technically be the same thing, but by including the projections of <span class="math notranslate nohighlight">\(X\)</span> rather than using the the raw inputs themselves, we allow the model to have more learnable parameters so it can futher accentuate similarities and differences between different timesteps!</p>
<p>The final result of this operation is the attention matrix, that computes the similarity between every possible pairs of tokens.</p>
<p><em><strong><span style="color:Violet">Note</strong></em></span>: I didn’t say anything about the <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_e}}\)</span> term in the formula. This is just a normalization constant that ensures our variance of the attention matrix isn’t too large after our matrix multiplication. This just leads to more stable training.</p>
</section>
</section>
<section id="span-style-color-lightgreen-step-2-weighting-the-values-matrix-span">
<h3><span style="color:LightGreen">Step 2: Weighting the Values Matrix</span><a class="headerlink" href="#span-style-color-lightgreen-step-2-weighting-the-values-matrix-span" title="Permalink to this heading">#</a></h3>
<p>Now that we have the similarities of how each timestep is related to all the other timesteps, we can do our weighted average. After the weighted average computation, each vector for each timestep isn’t just the data of the timestep but rather a weighted average of all the vectors in the sequence and how they are related to that timestep of interest.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-encoder_attention_vis.png" width=800></img>
</div><p>The output of this operation gives us the sequence of <span style="color:Violet">context vectors</span>.</p>
</section>
<section id="span-style-color-lightgreen-enforcing-causality-span">
<h3><span style="color:LightGreen">Enforcing Causality</span><a class="headerlink" href="#span-style-color-lightgreen-enforcing-causality-span" title="Permalink to this heading">#</a></h3>
<p>What we have seen so far is the equivalent to a Bidirectional RNN. The weighted average operation we are doing is between a timestep of interest and all timesteps before and after it. If we wanted a causal model, where a context vector only depends on the timesteps before it, then we need to apply a causal mask to our attention mechanism.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-causal_masking.png" width=400></img>
</div><p>As you can see, we apply a mask to all values of <span class="math notranslate nohighlight">\(t\)</span> where the index of the column values (our key index) is greater than the index of the row value (our value index). In practice, once we apply this mask to our attention matrix, we can then multiply by our values. You will see that the context vector at time <span class="math notranslate nohighlight">\(t\)</span> is only then dependent on previous timesteps, as we make sure future vectors of <span class="math notranslate nohighlight">\(V\)</span> are zeroed out.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-decoder_attention_vis.png" width=800></img>
</div></section>
<section id="span-style-color-lightgreen-lets-build-this-span">
<h3><span style="color:LightGreen">Lets Build This!</span><a class="headerlink" href="#span-style-color-lightgreen-lets-build-this-span" title="Permalink to this heading">#</a></h3>
<p>Now that we have everything we need, we can build it! We wont be trainig any models now, just defining and exploring the architecture here. To do so, we will define some data in the form of <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">Sequence</span> <span class="pre">Length</span> <span class="pre">x</span> <span class="pre">Embed</span> <span class="pre">Dim</span></code>. The Embedding dimension is the dimension of the vector we want to use to represent a single timestep, and the sequence length is how many timesteps there are in total.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="c1">### Lets Define some Random Data ###</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of Input is:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of Input is: torch.Size([4, 64, 128])
</pre></div>
</div>
</div>
</div>
<section id="span-style-color-lightpink-implement-attention-without-any-learnable-parameters-span">
<h4><span style="color:LightPink">Implement Attention Without Any Learnable Parameters</span><a class="headerlink" href="#span-style-color-lightpink-implement-attention-without-any-learnable-parameters-span" title="Permalink to this heading">#</a></h4>
<p>This whole attention operation is again very flexible and there is technically no reason to have any learnable parameters in its formulation (other than the obvious for wanting better predictive performance). So lets quickly just implement the formula as is using raw inputs <span class="math notranslate nohighlight">\(X\)</span> rather than doing any learned projections of the data.</p>
<div class="math notranslate nohighlight">
\[ \Large
\text{Attention} = \text{Softmax}(\frac{XX^T}{\sqrt{d_e}})X
\]</div>
<p><em><strong><span style="color:Violet">Step 1</span></strong></em></p>
<p>Compute <span class="math notranslate nohighlight">\(XX^T\)</span> which will provide the similarity score between every pair of vectors in <span class="math notranslate nohighlight">\(X\)</span>. This will be contained inside a <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">x</span> <span class="pre">sequence_length</span> <span class="pre">x</span> <span class="pre">sequence_length</span></code> matrix</p>
<p><em><strong><span style="color:Violet">Step 2</span></strong></em></p>
<p>After computing the similarity score, we can check and see that the variance of the similarity matrix is extremely high. This is the main reason for the normalization of dividing by the square root of the embedding dimension. In the end, the similarity scores are passed through a Softmax to compute a probability vector, dividing by a constant basically acts as a temperature parameter to cool the distribution and provide more stable training.</p>
<p><em><strong><span style="color:Violet">Step 3</span></strong></em></p>
<p>Each row of our <code class="docutils literal notranslate"><span class="pre">sequence_length</span> <span class="pre">x</span> <span class="pre">sequence_length</span></code> matrix is the similarity of how one timestep is related to all other timesteps. Instead of raw similarity scores, we will convert them to probabilities, so that when we do the weighted average on our values matrix, the weights add up to 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### First compute XX^T for similarity score between every pair of tokens ###</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="c1">### Normalize the Similarity Scores ###</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prenormalization Variance:&quot;</span><span class="p">,</span> <span class="n">similarity</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
<span class="n">similarity_norm</span> <span class="o">=</span> <span class="n">similarity</span> <span class="o">/</span> <span class="p">(</span><span class="n">embed_dim</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normed Similarity Variance:&quot;</span><span class="p">,</span> <span class="n">similarity_norm</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>

<span class="c1">### Check the Shape of our Similarity Tensor ###</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of Normed Similarity:&quot;</span><span class="p">,</span> <span class="n">similarity_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">### Compute similarity on every row of the attention matrix (i.e along the last dimension) ###</span>
<span class="n">attention_mat</span> <span class="o">=</span> <span class="n">similarity_norm</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">### Verify each row adds up to 1 ###</span>
<span class="n">summed_attention_mat</span> <span class="o">=</span> <span class="n">attention_mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Everything Equal to One:&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">summed_attention_mat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">summed_attention_mat</span><span class="p">)))</span>

<span class="c1">### Multiply our Attention Matrix against its Values (X in our case) for our Weighted Average ###</span>
<span class="n">context_vectors</span> <span class="o">=</span> <span class="n">attention_mat</span> <span class="o">@</span> <span class="n">x</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output Shape:&quot;</span><span class="p">,</span> <span class="n">context_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prenormalization Variance: tensor(377.7599)
Normed Similarity Variance: tensor(2.9512)
Shape of Normed Similarity: torch.Size([4, 64, 64])
Everything Equal to One: True
Output Shape: torch.Size([4, 64, 128])
</pre></div>
</div>
</div>
</div>
<p><em><strong><span style="color:Violet">Thats it!</span></strong></em> This is basically all the attention computation is doing mathematically, we will add in the learnable parameters in just a bit. Something important to bring to your attention is the input shape of <span class="math notranslate nohighlight">\(x\)</span> and our output shape of the context vectors are identical. Again, the input is the raw data, the output is weighted averaged of how every token is realted to all the other ones. But the shapes not changing is quite convenient, and allows us to stack together a bunch attention mechanisms on top of one another!</p>
</section>
<section id="span-style-color-lightpink-lets-add-learnable-parameters-span">
<h4><span style="color:LightPink">Lets Add Learnable Parameters</span><a class="headerlink" href="#span-style-color-lightpink-lets-add-learnable-parameters-span" title="Permalink to this heading">#</a></h4>
<p>This time, instead of using <span class="math notranslate nohighlight">\(X\)</span> as our input, we will create our three projections of <span class="math notranslate nohighlight">\(X\)</span> (Queries, Keys and Values) and then repeat the operation we just did. Also for convenience, we will wrap it all in a PyTorch class so we can continue adding stuff onto it as we go on to build up a general attention block.</p>
<p>Now what are these projections exactly? The are pointwise (or per timestep) projections! Remember, in our example here, each timestep is encoded by a vector of size 128. We will create three learnable weight matricies, incorporated inside the Linear modules in PyTorch, that take these 128 numbers per timestep and projects them to another 128 numbers (it is typical to keep the embedding dimension the same). This is a per timestep operation not across timesteps operation (across timesteps occurs within the attention computation). Obviously, PyTorch will accelerate this per timestep operation by doing it in parallel, but regardless, different timesteps dont get to see each other in the projection step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embedding_dimension</span> 
        
        <span class="c1">### Create Pointwise Projections ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="c1">### Create Queries, Keys and Values from X ###</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">### Do the same Computation from above, just with our QKV Matricies instead of X ###</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">similarity</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">attention</span> <span class="o">@</span> <span class="n">v</span>

        <span class="k">return</span> <span class="n">output</span>

<span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 64, 128])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="span-style-color-lightgreen-multiheaded-attention-span">
<h3><span style="color:LightGreen">MultiHeaded Attention</span><a class="headerlink" href="#span-style-color-lightgreen-multiheaded-attention-span" title="Permalink to this heading">#</a></h3>
<p>Now we have a small problem. Remember, the Attention matrix encodes the similarity between each pair of timesteps in your sequence. But in many cases, language being a prime example, there can be different types of relationships between different pairs of words, but our attention computation is restricted to learn only one of them. The solution to this is <span style="color:Violet">Multi Headed Attention</span>. Inside each attention computation, what if we have 2 attention matricies, or 8 or however many we want! The more we have the larger diversity of relationships we can learn!</p>
<section id="span-style-color-lightpink-single-headed-attention-recap-span">
<h4><span style="color:LightPink">Single Headed Attention Recap</span><a class="headerlink" href="#span-style-color-lightpink-single-headed-attention-recap-span" title="Permalink to this heading">#</a></h4>
<p>Lets summarize everything we have seen so far with a single visual, and we will call this a Single Head of attention. We will also have our embedding dimension for each word in the sequence be 9, and the sequence length is 8.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-single_headed_attention_visual.png" width=800></img>
</div><p>This is again called single headed attention because we only compute a single attention matrix following the logic described above.</p>
</section>
<section id="span-style-color-lightpink-moving-to-multiheaded-attention-span">
<h4><span style="color:LightPink">Moving to MultiHeaded Attention</span><a class="headerlink" href="#span-style-color-lightpink-moving-to-multiheaded-attention-span" title="Permalink to this heading">#</a></h4>
<p>For multiheaded attention there isn’t really a lot changing. Remember, to create our <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span> in single headed attention, we have 3 linear projection layers that take in the embedding dimension and output the same embedding dimension (in our case it takes in 9 and outputs 9). But in multiheaded attention, we can actually reduce our embedding dimension to a smaller value, do the attention computation on the tokens with this condensed embedding dimension, repeat it a bunch of times, and then concatenate together the outputs.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-multiheaded_attention_visual.png" width=1000></img>
</div><p>It is general practice to have the number of heads you pick to be a divisor of the embedding dimension. For example, in our case, our original embedding dimension is 9, so we can pick 3 heads because 9 is divisible by 3. This also means our head dimension would now be 3, because 9/3 = 3. In typical transformers, the embedding dimension is 768, and they typically have 12 heads of attention. This means each head of attention will have a dimension of 64 because 768/12 = 64.</p>
<p>The main reason we want it to evenly divide is because we have three heads, each takes in an embedding dimension of 9 and compresses to 3 before computing attention, and then outputs a tensor of embedding size 3. We can then take our 3 tensors, each having an embedding dimension of 3, concatenate them together, returning us back to the 9 that we began with! Again, this is just for convenience, so the embedding dimension of the input and output tensor dont change in any way.</p>
<p>The last problem is that since each head of attention is computed individually, the final concatenated tensor has a bunch of heads of attention packed together, but we never got to share information between the different heads of attention. This is why we have the final head projection, that will take in the embedding dimension of 9 from our concatenated tensor, and output an embedding dimension of 9, therefore meshing information across the heads of our embedding dimension.</p>
<p>Lets go ahead and build this module as shown in the figure above. Each head will have 3 projection layers for <span class="math notranslate nohighlight">\(Q\)</span>,<span class="math notranslate nohighlight">\(K\)</span>,<span class="math notranslate nohighlight">\(V\)</span>. We will perform the attention computation and then stick all the results back together at the end.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">### Make sure Embedding Dimension is Divisible by Num Heads ###</span>
        <span class="k">assert</span> <span class="n">embedding_dimension</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Make sure your embed_dim </span><span class="si">{</span><span class="n">embedding_dimension</span><span class="si">}</span><span class="s2"> is divisible by the number of heads </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="c1">### Compute Head Dimension ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>


        <span class="c1">### Create a List of Lists which has all our Q,K,V projections for each head ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multihead_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>

        <span class="c1">### For head Head create the QKV ###</span>
        <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">):</span>

            <span class="c1">### Create a dictionary of the 3 projection  layers we need ###</span>
            <span class="n">qkv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)],</span>
                    <span class="p">[</span><span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)],</span>
                    <span class="p">[</span><span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)],</span>
                <span class="p">]</span>
            <span class="p">)</span>

            <span class="c1">### Store Dictionary in List ###</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multihead_qkv</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">qkv_proj</span><span class="p">)</span>

        <span class="c1">### Create final Projection layer, it will be applied to the concatenated heads will have shape embed_dim again ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_mesh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="c1">### Create a list ot store each heads output ###</span>
        <span class="n">head_outs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1">### Loop Through Each head of Attention ###</span>
        <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">multihead_qkv</span><span class="p">:</span>

            <span class="c1">### Access layers like a dictionary (ModuleDict) ###</span>
            <span class="c1">### q,k,v will be (Batch x Seq len x head_dim)</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">head</span><span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">head</span><span class="p">[</span><span class="s2">&quot;K&quot;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">head</span><span class="p">[</span><span class="s2">&quot;V&quot;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>

            <span class="c1">### Now do the same Attention computation as before! ###</span>
            <span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
            <span class="n">attention</span>  <span class="o">=</span> <span class="n">similarity</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">attention</span> <span class="o">@</span> <span class="n">v</span>

            <span class="c1">### Store this output in the head_outs ###</span>
            <span class="n">head_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1">### head_outs has num_heads tensors, each with the compressed embedding dimension of head_dim ###</span>
        <span class="c1">### We can concatenate them all back together along the embedding dimension just like we did in the image above ###</span>
        <span class="n">head_outs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">head_outs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1">### head_outs will have the same shape now as our input x! ###</span>
        <span class="k">if</span> <span class="n">head_outs</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Something has gone wrong in the attention computation&quot;</span><span class="p">)</span>

        <span class="c1">### Now each head was computed independently, we need them to get to know each other, so pass our head_outs through final projection ### </span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_mesh</span><span class="p">(</span><span class="n">head_outs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
        

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">rand</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="span-style-color-lightgreen-increasing-efficiency-span">
<h3><span style="color:LightGreen">Increasing Efficiency</span><a class="headerlink" href="#span-style-color-lightgreen-increasing-efficiency-span" title="Permalink to this heading">#</a></h3>
<p>We now have successfully implemented a Multi Head Attention layer! This has all the same math and lodgic of attention, except for one small issue: <span style="color:Violet">efficiency</span>. Typically we want to avoid for loops as much as possible in our PyTorch code, being able to vectorize and do things in parallel will make much better use of the GPUs we train on. To make this more efficient, there is something we need to understand first: PyTorch Linear layers on multidimensional tensors.</p>
<section id="span-style-color-lightpink-linear-layers-on-multidimensional-tensors-span">
<h4><span style="color:LightPink">Linear Layers on MultiDimensional Tensors</span><a class="headerlink" href="#span-style-color-lightpink-linear-layers-on-multidimensional-tensors-span" title="Permalink to this heading">#</a></h4>
<p>We have already seen <code class="docutils literal notranslate"><span class="pre">nn.Linear(input_dim,</span> <span class="pre">output_dim)</span></code> many times already, and this module expects a tensor of shape <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">input_dim]</span></code> and it will output <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">output_dim]</span></code>. But what if our input is <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Dim1</span> <span class="pre">x</span> <span class="pre">Dim2</span> <span class="pre">x</span> <span class="pre">input_dim]</span></code>, then what happens? Basically, PyTorch will automatically flatten all the dimensions other than the last one automagically, do the Linear layer, and then return back to the expected shape, so we would get an output of <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Dim1</span> <span class="pre">x</span> <span class="pre">Dim2</span> <span class="pre">x</span> <span class="pre">output_dim]</span></code>. Another way of thinking about this is, PyTorch linear layers only are applied to the last dimension of your tensor.</p>
<p>Lets do a quick example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>

<span class="n">tensor_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">tensor_1_out</span> <span class="o">=</span> <span class="n">fc</span><span class="p">(</span><span class="n">tensor_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input Shape:&quot;</span><span class="p">,</span> <span class="n">tensor_1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Output Shape:&quot;</span><span class="p">,</span> <span class="n">tensor_1_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">tensor_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">tensor_2_out</span> <span class="o">=</span> <span class="n">fc</span><span class="p">(</span><span class="n">tensor_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input Shape:&quot;</span><span class="p">,</span> <span class="n">tensor_2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Output Shape:&quot;</span><span class="p">,</span> <span class="n">tensor_2_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input Shape: torch.Size([5, 10]) Output Shape: torch.Size([5, 30])
Input Shape: torch.Size([5, 1, 2, 3, 4, 10]) Output Shape: torch.Size([5, 1, 2, 3, 4, 30])
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-lightpink-packing-linear-layers-span">
<h4><span style="color:LightPink">Packing Linear Layers</span><a class="headerlink" href="#span-style-color-lightpink-packing-linear-layers-span" title="Permalink to this heading">#</a></h4>
<p>Another important idea is packing our Linear layers together. Lets think about our Multi Head example again: each projection for Q, K and V have a Linear layer that takes in 9 values and outputs 3 values, and we repeat this 3 times for each head. Lets just think about our Queries for now.</p>
<ul class="simple">
<li><p><span style="color:Violet">Query for Head 1</span>: Take in input <span class="math notranslate nohighlight">\(x\)</span> with embedding dim 9 and outputs tensor with embedding dimension 3</p></li>
<li><p><span style="color:Violet">Query for Head 2</span>: Take in input <span class="math notranslate nohighlight">\(x\)</span> with embedding dim 9 and outputs tensor with embedding dimension 3</p></li>
<li><p><span style="color:Violet">Query for Head 3</span>: Take in input <span class="math notranslate nohighlight">\(x\)</span> with embedding dim 9 and outputs tensor with embedding dimension 3</p></li>
</ul>
<p>Well what if we reframed this? What if we had a single Linear layer that takes input <span class="math notranslate nohighlight">\(x\)</span> with embedding dim 9 and outputs something with embedding dim 9. Afterwards, we can cut the matrix into our three heads of attention. Lets do a quick example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">)</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">)</span>

<span class="c1">### Pass tensor through layer to make Queries ###</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">fc</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of all Queries:&quot;</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">### Cut Embedding dimension into 3 heads ###</span>
<span class="n">q_head1</span><span class="p">,</span> <span class="n">q_head2</span><span class="p">,</span> <span class="n">q_head3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of each Head of Query:&quot;</span><span class="p">,</span> <span class="n">q_head1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of all Queries: torch.Size([1, 8, 9])
Shape of each Head of Query: torch.Size([1, 8, 3])
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-lightpink-multidimensional-matrix-multiplication-span">
<h4><span style="color:LightPink">MultiDimensional Matrix Multiplication</span><a class="headerlink" href="#span-style-color-lightpink-multidimensional-matrix-multiplication-span" title="Permalink to this heading">#</a></h4>
<p>So, we have composed our 9 Linear layers (3 heads have 3 projections for <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span> each) into just 3 Linear layers, where we have packed all the heads into them. But after we chunk up our <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span> tensors each into three more tensors for each head we will still need to do the looping operation to go through the cooresponding <span class="math notranslate nohighlight">\(q\)</span>, <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(v\)</span> matricies. Can we parallelize this too? Of course! We just need to better understand higher dimensional matrix multiplication.</p>
</section>
<section id="span-style-color-lightpink-recap-span">
<h4><span style="color:LightPink">Recap</span><a class="headerlink" href="#span-style-color-lightpink-recap-span" title="Permalink to this heading">#</a></h4>
<p>Matrix multiplication is typicall seen like this, multiplying an <code class="docutils literal notranslate"><span class="pre">[AxB]</span></code> matrix by a <code class="docutils literal notranslate"><span class="pre">[BxC]</span></code> which will produce a <code class="docutils literal notranslate"><span class="pre">[AxC]</span></code> matrix. But what if we have a <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">dim1</span> <span class="pre">x</span> <span class="pre">A</span> <span class="pre">x</span> <span class="pre">B]</span></code> multiplied by a <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">dim1</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">C]</span></code>. Matrix multiplication again only happens on the last two dimensions, so because our first tensor ends with an <code class="docutils literal notranslate"><span class="pre">[AxB]</span></code> and the second tensor ends with a <code class="docutils literal notranslate"><span class="pre">[BxC]</span></code>, the resulting matrix multiplication will be <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">dim1</span> <span class="pre">x</span> <span class="pre">A</span> <span class="pre">x</span> <span class="pre">C</span></code>. Lets see a quick example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Output Shape:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="nd">@b</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final Output Shape: torch.Size([1, 2, 6, 3])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="span-style-color-lightgreen-the-trick-of-parallelizing-heads-span">
<h3><span style="color:LightGreen">The Trick of Parallelizing Heads</span><a class="headerlink" href="#span-style-color-lightgreen-the-trick-of-parallelizing-heads-span" title="Permalink to this heading">#</a></h3>
<p>Now for the trick of parallelizing our heads by using everything we have just seen. All we need to do is split the embedding dimension up and move the heads out of the way so the computation can occur. Remember, we have our <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> matricies right now that each contain all the projected heads and are in the shape <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Embed_dim]</span></code> ([batch x 8 x 9] in our case).</p>
<ul class="simple">
<li><p>The first step is to split the embedding dimension into the number of heads and head dimension. We already know that our embedding dimension is divisible as thats how we set it, so we can do <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Embed_dim]</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code>. (This would be taking our [batch x 8 x 9] and converting to [batch x 8 x 3 x 3] in our case).</p></li>
<li><p>The attention computation has to happen between two matricies of shape <code class="docutils literal notranslate"><span class="pre">[Seq_len</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code> for queries and <code class="docutils literal notranslate"><span class="pre">[Embed_Dim</span> <span class="pre">x</span> <span class="pre">Seq_len]</span></code> for our transposed keys. In the case of multihead attention, the matrix multiplication happens across the Head Dimension rather than embedding. If our Queries, Keys and Values are in the shape <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code>, we can just transpose the Seq_len and Num_heads dimensions and make a tensor of shape <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code>. This way when I do Queries multiplied by the Transpose of Keys I will be doing <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code> multiplied by <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Embed</span> <span class="pre">Dim</span> <span class="pre">x</span> <span class="pre">Seq</span> <span class="pre">Len]</span></code> creating the attention matrix <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Seq</span> <span class="pre">Len]</span></code>. Therefore we have effectively created for every sample in the batch, and for every head of attention, a unique attention matrix! Thus we have parallelized the Attention Matrix computation.</p></li>
<li><p>Now that we have out all our attention matricies <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Seq</span> <span class="pre">Len]</span></code>, we can perform our scaling by our constant, and perform Softmax across every row of each attention matrix (along the last dimension).</p></li>
<li><p>The next step is to multiply out attention matrix <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Seq</span> <span class="pre">Len]</span></code> by the Values which is in the shape <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code>, which will get us to <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code>!</p></li>
<li><p>Lastly, we need to put our Num Heads and Embedding dimensions back together, so we can permute the Num Heads and Seq Len dimensions again which gives us <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code> and flatten on the last two dimension finally giving us <code class="docutils literal notranslate"><span class="pre">[Batch</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Num_Heads</span> <span class="pre">x</span> <span class="pre">Embed_Dim]</span></code>.</p></li>
<li><p>This flattening operation is equivalent to concatenation of all the heads of attention, so we can pass this through our final projection layer so all the heads of attention gets to know each other!</p></li>
</ul>
</section>
<section id="span-style-color-lightgreen-more-efficient-attention-implementation-span">
<h3><span style="color:LightGreen">More Efficient Attention Implementation</span><a class="headerlink" href="#span-style-color-lightgreen-more-efficient-attention-implementation-span" title="Permalink to this heading">#</a></h3>
<p>We will also include some extra <span style="color:Violet">dropout layers</span> typically added to attention computations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttentionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
               <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
               <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            embed_dim: Transformer Embedding Dimension</span>
<span class="sd">            num_heads: Number of heads of computation for Attention </span>
<span class="sd">            attn_p: Probability for Dropout2d on Attention cube</span>
<span class="sd">            proj_p: Probability for Dropout on final Projection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttentionEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
        
        <span class="c1">### Define all our Projections ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim] </span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        
        <span class="c1">### Perform Attention Computation ###</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>
        
        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">SelfAttentionEncoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">rand</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Output:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final Output: torch.Size([3, 8, 9])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="span-style-color-orange-padding-span">
<h2><span style="color:Orange">Padding</span><a class="headerlink" href="#span-style-color-orange-padding-span" title="Permalink to this heading">#</a></h2>
<p>One aspect we haven’t talked about yet is padding. We have to train in batches, and unfortunately, if each sequence is of different lengths, we cannot put them together simply and dynamically, In this situation, we just take all the sequences that are shorter than the longest one in the batch and then <span style="color:Violet">pad</span> them.</p>
<section id="span-style-color-lightgreen-sequence-padding-and-attention-masking-span">
<h3><span style="color:LightGreen">Sequence Padding and Attention Masking</span><a class="headerlink" href="#span-style-color-lightgreen-sequence-padding-and-attention-masking-span" title="Permalink to this heading">#</a></h3>
<p>One typical consideration for models like this is, when we pass in sentences of data, different sentences have different lengths. To create a matrix of tokens, we need to make sure all the sequences are the same, therefore we have to pad the shorter sequences to the longer ones. This is what the padding looks like:</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-sequence_padding.png" width=600></img>
</div><p>The pad tokens are just fillers and they don’t add any information at all to the data. To deal with this, we need to make sure that when we compute attention, these pad tokens are ignored. This is very similar to the Causal Mask in <a class="reference external" href="https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT">GPT</a>, and we will be doing exactly what we did there, fill in the masked locations with <span class="math notranslate nohighlight">\(-\infty\)</span> before Softmax is computed. Lets take a quick look at this for one of the sentences above.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-padding_attention_mask.png" width=800></img>
</div><p>You can see that we zero out the columns of our attention mask that has padding tokens, but not the row. The reason for this is, <span style="color:Violet">the model should learn the relation of how the padding tokens are related to the words</span> (this is most likely just positional information that padding is at the end), but it <span style="color:Violet">should not learn how other words are related to padding</span>, as the padding adds no semantic meaning. This is why, when multiplying by our values, we have zeroed out the attention weights of the 4th column, as when computing weighted averages of how one word is related to all others, the padding token is not included.</p>
</section>
</section>
<section id="span-style-color-orange-computing-the-reweighted-padded-attention-mask-span">
<h2><span style="color:Orange">Computing the Reweighted Padded Attention Mask</span><a class="headerlink" href="#span-style-color-orange-computing-the-reweighted-padded-attention-mask-span" title="Permalink to this heading">#</a></h2>
<p>Lets create some numbers so we can get a better idea of how this works. Let the tokens be <span class="math notranslate nohighlight">\(X = [10, 2, \text{&lt;pad&gt;}]\)</span>, so the third token is a padding token. Lets then also pretend, we pass this to our model, and when we go to compute our attention <span class="math notranslate nohighlight">\(QK^T\)</span>. The raw output before the Softmax is below:</p>
<div class="amsmath math notranslate nohighlight" id="equation-57561fd7-c317-4a07-a631-5943014d77a2">
<span class="eqno">(1)<a class="headerlink" href="#equation-57561fd7-c317-4a07-a631-5943014d77a2" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{bmatrix}
  7       &amp; -8   &amp; 6  \\
  -3       &amp; 2   &amp; 4   \\
  1       &amp; 6  &amp; -2   \\
\end{bmatrix}
\end{equation}\]</div>
<p>Remember, the equation for softmax is:</p>
<div class="math notranslate nohighlight">
\[ \Large
\text{Softmax}(\vec{x}) = \frac{e^{x_i}}{\sum_{j=1}^N{e^{x_j}}}
\]</div>
<p>If we ignore padding and everything right now, we can compute softmax for row of the matrix above:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c5f8e360-bae7-4fbd-82b7-25e1246f3b5a">
<span class="eqno">(2)<a class="headerlink" href="#equation-c5f8e360-bae7-4fbd-82b7-25e1246f3b5a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Softmax}
\begin{bmatrix}
  7       &amp; -8   &amp; 6  \\
  -3       &amp; 2   &amp; 4   \\
  1       &amp; 6  &amp; -2   \\
\end{bmatrix} = 
\begin{bmatrix}
  \frac{e^{7}}{e^{7}+e^{-8}+e^{6}}       &amp; \frac{e^{-8}}{e^{7}+e^{-8}+e^{6}}   &amp; \frac{e^{6}}{e^{7}+e^{-8}+e^{6}}  \\
  \frac{e^{-3}}{e^{-3}+e^{2}+e^{4}}       &amp; \frac{e^{2}}{e^{-3}+e^{2}+e^{4}}   &amp; \frac{e^{4}}{e^{-3}+e^{2}+e^{4}}  \\
  \frac{e^{1}}{e^{1}+e^{6}+e^{-2}}       &amp; \frac{e^{6}}{e^{1}+e^{6}+e^{-2}}   &amp; \frac{e^{-2}}{e^{1}+e^{6}+e^{-2}}  \\
\end{bmatrix} = 
\begin{bmatrix}
  0.73       &amp; 0.0000002   &amp; 0.27   \\
  0.0008       &amp; 0.12   &amp; 0.88 \\
  0.007       &amp; 0.99  &amp; 0.003  \\
\end{bmatrix}
\end{equation}\]</div>
<p>But what we need is to mask out all the tokens in this matrix related to padding. Just like we did in <a class="reference external" href="https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT">GPT</a>, we will fill in the indexes of the that we want to mask with <span class="math notranslate nohighlight">\(-\infty\)</span>. If only the last token was a padding token in our sequence, then the attention before the softmax should be written as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0e2cce96-934a-4701-82c8-c16d994f8f9a">
<span class="eqno">(3)<a class="headerlink" href="#equation-0e2cce96-934a-4701-82c8-c16d994f8f9a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{bmatrix}
  7       &amp; -8   &amp; -\infty  \\
  -3       &amp; 2   &amp; -\infty   \\
  1       &amp; 6  &amp; -\infty  \\
\end{bmatrix}
\end{equation}\]</div>
<p>Taking the softmax of the rows of this matrix then gives:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c6ba2d70-7f85-46d3-a719-f2e0424a310a">
<span class="eqno">(4)<a class="headerlink" href="#equation-c6ba2d70-7f85-46d3-a719-f2e0424a310a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Softmax}
\begin{bmatrix}
 7       &amp; -8   &amp; -\infty  \\
  -3       &amp; 2   &amp; -\infty   \\
  1       &amp; 6  &amp; -\infty  \\
\end{bmatrix} = 
\begin{bmatrix}
  1       &amp; 0   &amp; 0  \\
  0.0067  &amp; 0.9933 &amp; 0   \\
  0.0067       &amp; 0.9933  &amp; 0  \\
\end{bmatrix}
\end{equation}\]</div>
<p>Which is exactly what we want! Therefore, when computing attention, we need to know which tokens are padding tokens in our data, and then go ahead and perform this computation on our attention output before multiplying with values.</p>
<section id="span-style-color-lightgreen-add-padding-to-our-attention-computation-span">
<h3><span style="color:LightGreen">Add Padding to our Attention Computation</span><a class="headerlink" href="#span-style-color-lightgreen-add-padding-to-our-attention-computation-span" title="Permalink to this heading">#</a></h3>
<p>There are a few ways to indicate padding, but the most common is a simple <code class="docutils literal notranslate"><span class="pre">True</span></code>/<code class="docutils literal notranslate"><span class="pre">False</span></code> vector. Lets say we have the following 3 sequences:</p>
<ul class="simple">
<li><p>sequence_1: [a1, a2, a3]</p></li>
<li><p>sequence_2: [b1, b2, b3, b4]</p></li>
<li><p>sequence_3: [c1, c2, c3]</p></li>
</ul>
<p>In this situation we would pad sequence_1 and sequence_3 to the longest sequence, so after padding we would have something like this:</p>
<ul class="simple">
<li><p>sequence_1: [a1, a2, a3, <strong><span style="color:Blue">&lt;PAD&gt;</span></strong>]</p></li>
<li><p>sequence_2: [b1, b2, b3, b4]</p></li>
<li><p>sequence_3: [c1, c2, c3, <strong><span style="color:Blue">&lt;PAD&gt;</span></strong>]</p></li>
</ul>
<p>Now we will create boolean vectors, identifying <code class="docutils literal notranslate"><span class="pre">True/</span></code>False for tokens we want to compute attention on. Anything that is a <strong>&lt;PAD&gt;</strong> token would be omitted, so we would have the following attention masks for each sequence:</p>
<ul class="simple">
<li><p>sequence_1: [<code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>]</p></li>
<li><p>sequence_2: [<code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">True</span></code>]</p></li>
<li><p>sequence_3: [<code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>]</p></li>
</ul>
<p>Then, as indicated above, for columns for tokens identified as <code class="docutils literal notranslate"><span class="pre">False</span></code> (because they are pad tokens) we will zero out the attention scores and reweight them to properly compute attention without learning how words are related to the padding (only how the padding is related to the words)</p>
</section>
<section id="span-style-color-lightgreen-repeating-to-match-attention-matrix-shape-span">
<h3><span style="color:LightGreen">Repeating to Match Attention Matrix Shape</span><a class="headerlink" href="#span-style-color-lightgreen-repeating-to-match-attention-matrix-shape-span" title="Permalink to this heading">#</a></h3>
<p>So we now have our attention mask and know we need to mask the columns in our attention matrix that coorespond to them. Lets also pretend for now we are doing single headed attention, we can deal with the multiheaded attention in a bit. In this case, lets take a look at our attention and mask shapes:</p>
<p><code class="docutils literal notranslate"><span class="pre">attn.shape</span></code> - (Batch x seq_len x seq_len)</p>
<p><code class="docutils literal notranslate"><span class="pre">mask.shape</span></code> - (Batch x seq_len)</p>
<p>It is clear that our mask is missing a dimension, and we need to repeat it. Lets take sequence_1 for instance that has a mask of [True, True, True, False]. Because the sequence length here is 4, lets repeat this row 4 times:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f43bda6f-f80d-4ba2-84a7-05cbb98f3721">
<span class="eqno">(5)<a class="headerlink" href="#equation-f43bda6f-f80d-4ba2-84a7-05cbb98f3721" title="Permalink to this equation">#</a></span>\[\begin{bmatrix}
\textrm{True} &amp; \textrm{True} &amp; \textrm{True} &amp; \textrm{False} \\
\textrm{True} &amp; \textrm{True} &amp; \textrm{True} &amp; \textrm{False} \\
\textrm{True} &amp; \textrm{True} &amp; \textrm{True} &amp; \textrm{False} \\
\textrm{True} &amp; \textrm{True} &amp; \textrm{True} &amp; \textrm{False}
\end{bmatrix}\]</div>
<p>By repeating our mask for sequence_1, we get exactly a <code class="docutils literal notranslate"><span class="pre">seq_len</span> <span class="pre">x</span> <span class="pre">seq_len</span></code> matrix where the column we don’t want to compute attention for is indicated by the <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>Therefore our final mask will be of shape <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">seq_len</span> <span class="pre">x</span> <span class="pre">seq_len</span></code></p>
<p><em><strong><span style="color:Violet">CAVEAT</span></strong></em>: Technically we don’t need to do this, and this is why the padding can get confusing as different people/functions do it different ways. First of all, lets say we reshaped our attention mask from<code class="docutils literal notranslate"><span class="pre">(Batch</span> <span class="pre">x</span> <span class="pre">seq_len</span></code> to <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">seq_len</span></code>. That 1 is a dummy dimension, and when we use the <code class="docutils literal notranslate"><span class="pre">Tensor.masked_fill_</span></code> it will automatically broadcast our attention mask across that dummy 1 dimension. On the other hand, when we go to use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">flash_attention</a> at the end, this method wants the tensor specifically in the shape <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">seq_len</span> <span class="pre">x</span> <span class="pre">seq_len</span></code>.</p>
<p>For testing, lets just use a <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">x</span> <span class="pre">6</span> <span class="pre">x</span> <span class="pre">6</span></code> tensor as a fake <code class="docutils literal notranslate"><span class="pre">8</span> <span class="pre">x</span> <span class="pre">8</span></code> attention matrix for a single sample (not multihead attention yet). Lets also pretend the last two tokens are pad tokens, so we want the attention mask that will fill the last two columns with <span class="math notranslate nohighlight">\(-\infty\)</span>, so when we take the softmax in the future it’ll become zero!</p>
<p><code class="docutils literal notranslate"><span class="pre">Tensor.masked_fill_</span></code> will fill anywhere indicated as <code class="docutils literal notranslate"><span class="pre">True</span></code> with our fill value. In our case though, we have the tokens we don’t want to compute on (the ones we want to fill with <span class="math notranslate nohighlight">\(-\infty\)</span>) as false. So we just need to flip the boolean in our attention mask.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Create an example attention matrix (b x n x n) ###</span>
<span class="n">rand_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>

<span class="c1">### Create Attention Mask in the shape (b x n) ###</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Method 1:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------&quot;</span><span class="p">)</span>
<span class="c1">### Add Extra Dimension for the (b x n x n) ###</span>
<span class="c1">### So unsqueeze mask to be (b x 1 x n) ###</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">### Unsqueezed with dummy broadcast dimension ###</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Method 2:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------&quot;</span><span class="p">)</span>
<span class="c1">### Repeat the Dummy Dimension so attention mask is (b x n x n) ###</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># repeat dummy middle dim 6 times (for the seq_len) </span>
<span class="nb">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Method 1:
--------
tensor([[[ True,  True,  True,  True, False, False]]])
tensor([[[0.6821, 0.4571, 0.9372, 0.7789,   -inf,   -inf],
         [0.1615, 0.4683, 0.1886, 0.0264,   -inf,   -inf],
         [0.4572, 0.5982, 0.7586, 0.3741,   -inf,   -inf],
         [0.3705, 0.3646, 0.2765, 0.2522,   -inf,   -inf],
         [0.3564, 0.2086, 0.9521, 0.3250,   -inf,   -inf],
         [0.4875, 0.8989, 0.4354, 0.0878,   -inf,   -inf]]])
Method 2:
--------
tensor([[[ True,  True,  True,  True, False, False],
         [ True,  True,  True,  True, False, False],
         [ True,  True,  True,  True, False, False],
         [ True,  True,  True,  True, False, False],
         [ True,  True,  True,  True, False, False],
         [ True,  True,  True,  True, False, False]]])
tensor([[[0.6821, 0.4571, 0.9372, 0.7789,   -inf,   -inf],
         [0.1615, 0.4683, 0.1886, 0.0264,   -inf,   -inf],
         [0.4572, 0.5982, 0.7586, 0.3741,   -inf,   -inf],
         [0.3705, 0.3646, 0.2765, 0.2522,   -inf,   -inf],
         [0.3564, 0.2086, 0.9521, 0.3250,   -inf,   -inf],
         [0.4875, 0.8989, 0.4354, 0.0878,   -inf,   -inf]]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-lightgreen-matching-shapes-for-multihead-attention-span">
<h3><span style="color:LightGreen">Matching Shapes for MultiHead Attention</span><a class="headerlink" href="#span-style-color-lightgreen-matching-shapes-for-multihead-attention-span" title="Permalink to this heading">#</a></h3>
<p>So we now have our attention mask and know how to mask the columns cooresponding to them. But we have one other problem: Multi Head Attention. This means we don’t have just a single attention matrix, but rather <code class="docutils literal notranslate"><span class="pre">num_heads</span></code> of them. This means we need to mask the corresponding columns of <span style="color:Violet">ALL</span> attention matricies across the heads. Lets take a quick look at our tensors in question again:</p>
<p><code class="docutils literal notranslate"><span class="pre">attn.shape</span></code> - (Batch x num_heads x seq_len x seq_len)</p>
<p><code class="docutils literal notranslate"><span class="pre">mask.shape</span></code> - (Batch x seq_len)</p>
<p>This means we can do what we did earlier, just twice! We can take our mask and unsqueeze twice, and go from <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">seq_len</span></code> to <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">seq_len</span></code>, and this would be totally fine. Except again, when we do our <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">flash_attention</a> later, it again expects a <code class="docutils literal notranslate"><span class="pre">seq_len</span> <span class="pre">x</span> <span class="pre">seq_len</span></code>. Luckily though, flash attention will broadcast over the head dimension as long as that dimension is present in the mask, so we just have to do what we did earlier and repeat <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">seq_len</span></code> to <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">seq_len</span> <span class="pre">x</span> <span class="pre">seq_len</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Create an example attention matrix (b x h x n x n) ###</span>
<span class="n">rand_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span> <span class="c1"># I have 2 heads here!</span>

<span class="c1">### Create Attention Mask in the shape (b x n) ###</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Method 1:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------&quot;</span><span class="p">)</span>
<span class="c1">### Add Two Extra Dimension for the (b x h x n x n) ###</span>
<span class="c1">### So unsqueeze mask to be (b x 1 x 1 x n) ###</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">### Unsqueezed with dummy broadcast dimension ###</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Method 2:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------&quot;</span><span class="p">)</span>
<span class="c1">### Repeat the Dummy Dimension for seq_len so attention mask is (b 1 x n x n) ###</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># repeat dummy middle dim 6 times (for the seq_len) </span>
<span class="nb">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Method 1:
--------
tensor([[[[ True,  True,  True,  True, False, False]]]])
tensor([[[[0.4502, 0.4681, 0.3192, 0.6109,   -inf,   -inf],
          [0.8668, 0.0899, 0.7930, 0.7684,   -inf,   -inf],
          [0.6498, 0.2491, 0.8711, 0.9962,   -inf,   -inf],
          [0.9936, 0.9426, 0.4658, 0.4616,   -inf,   -inf],
          [0.3343, 0.3867, 0.0966, 0.0596,   -inf,   -inf],
          [0.5665, 0.5316, 0.8865, 0.6268,   -inf,   -inf]],

         [[0.8691, 0.4049, 0.5839, 0.4622,   -inf,   -inf],
          [0.3121, 0.0574, 0.0720, 0.6727,   -inf,   -inf],
          [0.8972, 0.7221, 0.7368, 0.8762,   -inf,   -inf],
          [0.5947, 0.8555, 0.8953, 0.3878,   -inf,   -inf],
          [0.1727, 0.1827, 0.4982, 0.2055,   -inf,   -inf],
          [0.9288, 0.4976, 0.4614, 0.9533,   -inf,   -inf]]]])
Method 2:
--------
tensor([[[[ True,  True,  True,  True, False, False],
          [ True,  True,  True,  True, False, False],
          [ True,  True,  True,  True, False, False],
          [ True,  True,  True,  True, False, False],
          [ True,  True,  True,  True, False, False],
          [ True,  True,  True,  True, False, False]]]])
tensor([[[[0.4502, 0.4681, 0.3192, 0.6109,   -inf,   -inf],
          [0.8668, 0.0899, 0.7930, 0.7684,   -inf,   -inf],
          [0.6498, 0.2491, 0.8711, 0.9962,   -inf,   -inf],
          [0.9936, 0.9426, 0.4658, 0.4616,   -inf,   -inf],
          [0.3343, 0.3867, 0.0966, 0.0596,   -inf,   -inf],
          [0.5665, 0.5316, 0.8865, 0.6268,   -inf,   -inf]],

         [[0.8691, 0.4049, 0.5839, 0.4622,   -inf,   -inf],
          [0.3121, 0.0574, 0.0720, 0.6727,   -inf,   -inf],
          [0.8972, 0.7221, 0.7368, 0.8762,   -inf,   -inf],
          [0.5947, 0.8555, 0.8953, 0.3878,   -inf,   -inf],
          [0.1727, 0.1827, 0.4982, 0.2055,   -inf,   -inf],
          [0.9288, 0.4976, 0.4614, 0.9533,   -inf,   -inf]]]])
</pre></div>
</div>
</div>
</div>
<p>You can obviously pick whichever method you want, but because we want to use flash attention later, lets go with their requirements!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
                 <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            embed_dim: Transformer Embedding Dimension</span>
<span class="sd">            num_heads: Number of heads of computation for Attention </span>
<span class="sd">            attn_p: Probability for Dropout2d on Attention cube</span>
<span class="sd">            proj_p: Probability for Dropout on final Projection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
        
        <span class="c1">### Define all our Projections ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim] </span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        
        <span class="c1">### Perform Attention Computation ###</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="c1">####################################################################################</span>
        <span class="c1">### FILL ATTENTION MASK WITH -Infinity ###</span>

        <span class="c1">### NOTE: </span>
        <span class="c1">### attn.shape - (Batch x num_heads x seq_len x seq_len)</span>
        <span class="c1">### mask.shape - (Batch x seq_len)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
            
        <span class="c1">####################################################################################</span>
 
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>
        
        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>


<span class="c1">### We will now have sequences of different lengths, identify the number of tokens in each sequence ###</span>
<span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###</span>
<span class="c1">### This will be a tensor upto the max(seq_lens) ###</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###</span>
<span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention Mask:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">rand</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Output:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention Mask:
tensor([[ True,  True,  True, False, False],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True, False]])
Final Output: torch.Size([3, 5, 9])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3><span style="color:LightGreen">Enforcing Causality</span><a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>Everything we have done so far is an <span style="color:Violet">encoding transformer</span>, which is eqivalent to a bidirectional RNN where a single timestep can look both forwards and backwards. To enforce causality, we can only look backwards, so we have to add in a causal mask in exactly the same way we described before.</p>
</section>
<section id="span-style-color-lightgreen-computing-the-reweighted-causal-attention-mask-span">
<h3><span style="color:LightGreen">Computing the Reweighted Causal Attention Mask</span><a class="headerlink" href="#span-style-color-lightgreen-computing-the-reweighted-causal-attention-mask-span" title="Permalink to this heading">#</a></h3>
<p>Lets pretend the raw outputs of <span class="math notranslate nohighlight">\(QK^T\)</span>, before the softmax, is below:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2c676f3d-d734-4235-8879-0beed9e279c3">
<span class="eqno">(6)<a class="headerlink" href="#equation-2c676f3d-d734-4235-8879-0beed9e279c3" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{bmatrix}
  7       &amp; -8   &amp; 6  \\
  -3       &amp; 2   &amp; 4   \\
  1       &amp; 6  &amp; -2   \\
\end{bmatrix}
\end{equation}\]</div>
<p>Remember, the equation for softmax is:</p>
<div class="math notranslate nohighlight">
\[\text{Softmax}(\vec{x}) = \frac{e^{x_i}}{\sum_{j=1}^N{e^{x_j}}}\]</div>
<p>Then, we can compute softmax for row of the matrix above:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4fe81405-25da-4df3-929d-b57245a40a23">
<span class="eqno">(7)<a class="headerlink" href="#equation-4fe81405-25da-4df3-929d-b57245a40a23" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Softmax}
\begin{bmatrix}
  7       &amp; -8   &amp; 6  \\
  -3       &amp; 2   &amp; 4   \\
  1       &amp; 6  &amp; -2   \\
\end{bmatrix} = 
\begin{bmatrix}
  \frac{e^{7}}{e^{7}+e^{-8}+e^{6}}       &amp; \frac{e^{-8}}{e^{7}+e^{-8}+e^{6}}   &amp; \frac{e^{6}}{e^{7}+e^{-8}+e^{6}}  \\
  \frac{e^{-3}}{e^{-3}+e^{2}+e^{4}}       &amp; \frac{e^{2}}{e^{-3}+e^{2}+e^{4}}   &amp; \frac{e^{4}}{e^{-3}+e^{2}+e^{4}}  \\
  \frac{e^{1}}{e^{1}+e^{6}+e^{-2}}       &amp; \frac{e^{6}}{e^{1}+e^{6}+e^{-2}}   &amp; \frac{e^{-2}}{e^{1}+e^{6}+e^{-2}}  \\
\end{bmatrix} = 
\begin{bmatrix}
  0.73       &amp; 0.0000002   &amp; 0.27   \\
  0.0008       &amp; 0.12   &amp; 0.88 \\
  0.007       &amp; 0.99  &amp; 0.003  \\
\end{bmatrix}
\end{equation}\]</div>
<p>But, what we want, is the top triangle to have weights of 0, and the rest adding up to 1. So lets take the second vector in the matrix above to see how we can do that.</p>
<div class="math notranslate nohighlight">
\[ \Large
x_2 = [-3, 2, 4]
\]</div>
<p>Because this is the second vector, we need to zero out the softmax output for everything after the second index (so in our case just the last value). Lets replace the value 4 by <span class="math notranslate nohighlight">\(-\infty\)</span>. Then we can write it as:</p>
<div class="math notranslate nohighlight">
\[ \Large
x_2 = [-3, 2, -\infty]
\]</div>
<p>Lets now take softmax of this vector!</p>
<div class="math notranslate nohighlight">
\[ \Large
\text{Softmax}(x_2) = [\frac{e^{-3}}{e^{-3}+e^{2}+e^{-\infty}}, \frac{e^{2}}{e^{-3}+e^{2}+e^{-\infty}}, \frac{e^{-\infty}}{e^{-3}+e^{2}+e^{-\infty}}]
\]</div>
<p>Remember, <span class="math notranslate nohighlight">\(e^{-\infty}\)</span> is equal to 0, so we can solve solve this!</p>
<div class="math notranslate nohighlight">
\[ \large
\text{Softmax}(x_2) = [\frac{e^{-3}}{e^{-3}+e^{2}+0}, \frac{e^{2}}{e^{-3}+e^{2}+0}, \frac{0}{e^{-3}+e^{2}+0}] = [\frac{e^{-3}}{e^{-3}+e^{2}+0}, \frac{e^{2}}{e^{-3}+e^{2}+0}, \frac{0}{e^{-3}+e^{2}+0}] = [0.0067, 0.9933, 0.0000]
\]</div>
<p>So we have exactly what we want! The attention weight of the last value is set to 0, so when we are on the second vector <span class="math notranslate nohighlight">\(x_2\)</span>, we cannot look forward to the future value vectors <span class="math notranslate nohighlight">\(v_3\)</span>, and the remaining parts add up to 1 so its still a probability vector! To do this correctly for the entire matrix, we can just substitute in the top triangle of <span class="math notranslate nohighlight">\(QK^T\)</span> with <span class="math notranslate nohighlight">\(-\infty\)</span>. This would look like:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f3e7d9c2-b371-459b-b150-461c7bf5c3db">
<span class="eqno">(8)<a class="headerlink" href="#equation-f3e7d9c2-b371-459b-b150-461c7bf5c3db" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{bmatrix}
  7       &amp; -\infty   &amp; -\infty  \\
  -3       &amp; 2   &amp; -\infty   \\
  1       &amp; 6  &amp; -2   \\
\end{bmatrix}
\end{equation}\]</div>
<p>Taking the softmax of the rows of this matrix then gives:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6dd91645-bacf-4a60-973d-9c57f513fdf3">
<span class="eqno">(9)<a class="headerlink" href="#equation-6dd91645-bacf-4a60-973d-9c57f513fdf3" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Softmax}
\begin{bmatrix}
  7       &amp; -\infty   &amp; -\infty  \\
  -3       &amp; 2   &amp; -\infty   \\
  1       &amp; 6  &amp; -2   \\
\end{bmatrix} = 
\begin{bmatrix}
  1       &amp; 0   &amp; 0  \\
  0.0067  &amp; 0.9933 &amp; 0   \\
  0.007       &amp; 0.99  &amp; 0.003   \\
\end{bmatrix}
\end{equation}\]</div>
<p>Therefore, the best way to apply out attention mask is by filling the top right triangle with <span class="math notranslate nohighlight">\(-\inf\)</span> and then take the softmax! So lets go ahead and add an option for causality for our attention function we wrote above!</p>
</section>
<section id="span-style-color-lightgreen-attention-mask-span">
<h3><span style="color:LightGreen">Attention Mask</span><a class="headerlink" href="#span-style-color-lightgreen-attention-mask-span" title="Permalink to this heading">#</a></h3>
<p>Again, we may have a causal mask, but that doesn’t mean we dont also have an attention mask! Regardless of causality, we want to ensure that tokens never attend to pad tokens. Lets say we have a sequence of 8, but the last 4 tokens are pad tokens, then this would look like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1">### Create Causal Mask ###</span>
<span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

<span class="c1">### Create Padding Mask ###</span>
<span class="n">padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">padding_mask</span> <span class="o">=</span> <span class="n">padding_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">### Combine Masks (set positions we dont want in our causal mask to False based on the padding mask) ###</span>
<span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">causal_mask</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ True, False, False, False, False, False, False, False],
        [ True,  True, False, False, False, False, False, False],
        [ True,  True,  True, False, False, False, False, False],
        [ True,  True,  True,  True, False, False, False, False],
        [ True,  True,  True,  True, False, False, False, False],
        [ True,  True,  True,  True, False, False, False, False],
        [ True,  True,  True,  True, False, False, False, False],
        [ True,  True,  True,  True, False, False, False, False]])
</pre></div>
</div>
</div>
</div>
<p>As you can see here, we have our causal attention mask as described earlier where the top right triangle is masked out, but for tokens that were pad tokens, the full column is masked out!</p>
</section>
<section id="span-style-color-lightgreen-lets-incorporate-the-causal-mask-into-self-attention-span">
<h3><span style="color:LightGreen">Lets Incorporate the Causal Mask into Self-Attention</span><a class="headerlink" href="#span-style-color-lightgreen-lets-incorporate-the-causal-mask-into-self-attention-span" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
               <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
               <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            embed_dim: Transformer Embedding Dimension</span>
<span class="sd">            num_heads: Number of heads of computation for Attention </span>
<span class="sd">            attn_p: Probability for Dropout2d on Attention cube</span>
<span class="sd">            proj_p: Probability for Dropout on final Projection</span>
<span class="sd">            causal: Do you want to apply a causal mask?</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
        
        <span class="c1">### Define all our Projections ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim] </span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        
        <span class="c1">### Perform Attention Computation ###</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span><span class="p">:</span>
            <span class="c1">####################################################################################</span>
            <span class="c1">### Create the Causal Mask (On the correct device) ###</span>

            <span class="c1">### Create a Seq_Len x Seq_Len tensor full of Ones</span>
            <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">attn</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            
            <span class="c1">### Fill Top right triangle with Zeros (as we dont want to attend to them) ###</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span> 

            <span class="c1">### Add extra dimensions for Batch size and Number of Heads ###</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

            <span class="c1">### If we have padding mask, then update our causal mask ###</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

                <span class="c1">### Each sample could have a different number of pad tokens, so repeat causal mask for batch size ###</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1">### Expand and repeat the Padding Mask (b x s) -&gt; (b x 1 x s x s)###</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

                <span class="c1">### Fill causal mask where attention mask is False with False (to ensure all padding tokens are masked out) ###</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

            <span class="c1">### Fill attn with -inf wherever causal mask is False ###</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1">####################################################################################</span>
    
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>
        
        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">### We will now have sequences of different lengths, identify the number of tokens in each sequence ###</span>
<span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###</span>
<span class="c1">### This will be a tensor upto the max(seq_lens) ###</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###</span>
<span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention Mask:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">rand</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Output:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention Mask:
tensor([[ True,  True,  True, False, False],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True, False]])
Final Output: torch.Size([3, 5, 9])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="span-style-color-orange-cross-attention-span">
<h2><span style="color:Orange">Cross Attention</span><a class="headerlink" href="#span-style-color-orange-cross-attention-span" title="Permalink to this heading">#</a></h2>
<p>We have talked about Self-Attention so far, where a model looks at a sequence and tries to learn how every word in that sequence is related to itself. This is totally fine if we are just dealing with that sequence (BERT/GPT), but what if we are translating between two different sequences? For example, the original <a class="reference external" href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a> paper was about Neural Machine Translation. So if we are doing English to French generation, we don’t only care about how english tokens are related english, and how french tokens are related to french, we also need to learn how english tokens are related to french.</p>
<p>Luckily this isn’t too bad or different from what we have done so far but we have some new complexities:</p>
<ol class="arabic simple">
<li><p>English will have a different length than French (this means our attention matricies of how each english token is related to french is no longer a square!)</p></li>
<li><p>We still train in batches, so we will have a batch of english and a batch of french. This also means that the enlish tokens will have padding, but the french tokens will have its own padding, and they can be totally different from each other!</p></li>
</ol>
<section id="span-style-color-lightgreen-cross-attention-queries-keys-and-values-span">
<h3><span style="color:LightGreen">Cross Attention Queries, Keys and Values</span><a class="headerlink" href="#span-style-color-lightgreen-cross-attention-queries-keys-and-values-span" title="Permalink to this heading">#</a></h3>
<p>So the first question, before we had some data that we projected to our Queries, Keys and Values. But now we have two sources of data, English and French. Lets pretend we are translating from English to French. This means to understand how the French is related to the English to inform generating the French we will set the Queries as the projection of our French Embeddings and the Keys/Values as the projection of our English Embeddings. Typically, we set the queries to the thing we care about (what we want to produce), and they keys/values to what we want to relate it to.</p>
<p>We can think about this in another way as well: Lets say we are doing Text to Image Generation. Again, because we have two sources we are translating between, we can use <span style="color:Violet">Cross Attention</span>. Attention will tell us how pieces of our image are related to the text embeddings. In the end we want the image, so we can set them to the queries, and then the text will be the keys/values, which will be how we want to weight the relation between the image pieces and our text tokens.</p>
</section>
<section id="span-style-color-lightgreen-tensor-shapes-for-cross-attention-span">
<h3><span style="color:LightGreen">Tensor Shapes for Cross Attention</span><a class="headerlink" href="#span-style-color-lightgreen-tensor-shapes-for-cross-attention-span" title="Permalink to this heading">#</a></h3>
<p>We spent a lot of time understanding the tensor shapes for self-attention, lets do the same thing now for cross attention. We will be using our English to French translation example for this, so lets start with producing our Queries/Keys/Values:</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-cross_atttenion_qkv.png" width=600></img>
</div><p>Lets pretend we have an input of French embeddings with <span class="math notranslate nohighlight">\(N\)</span> tokens (with an embedding dimension of <span class="math notranslate nohighlight">\(E\)</span>) and then also English Embeddings with <span class="math notranslate nohighlight">\(L\)</span> tokens (again with an embedding dimension of <span class="math notranslate nohighlight">\(E\)</span>). Again, because the French is what we want, we will project our French embeddings to be our Queries, and then our English Embeddings to our Keys/Values.</p>
<ul class="simple">
<li><p>French Query Embeddings: <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span> <span class="pre">x</span> <span class="pre">E</span></code></p></li>
<li><p>English Key Embeddings: <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">L</span> <span class="pre">x</span> <span class="pre">E</span></code></p></li>
<li><p>English Value Embeddings: <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">L</span> <span class="pre">x</span> <span class="pre">E</span></code></p></li>
</ul>
</section>
<section id="span-style-color-lightgreen-performing-the-attention-computation-span">
<h3><span style="color:LightGreen">Performing the Attention Computation</span><a class="headerlink" href="#span-style-color-lightgreen-performing-the-attention-computation-span" title="Permalink to this heading">#</a></h3>
<p>Nothing changes with our attention formula as described. This attention computation is identical to earlier, its just because our Queries is of length <span class="math notranslate nohighlight">\(N\)</span> and the Keys are of length <span class="math notranslate nohighlight">\(L\)</span>, our final attention matrix will be an (B x N x L) matrix.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-cross_attention_matmul.png" width=600></img>
</div><p>We can take a closer look at our attention matrix now as well, and we see that it isn’t a square matrix anymore, but it still does the same thing: How are each French token related to each English token?</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-cross_attention_mat_vals.png" width=600></img>
</div></section>
<section id="span-style-color-lightgreen-the-most-important-part-span">
<h3><span style="color:LightGreen">THE MOST IMPORTANT PART!!!</span><a class="headerlink" href="#span-style-color-lightgreen-the-most-important-part-span" title="Permalink to this heading">#</a></h3>
<p>Once our attention matrix <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span> <span class="pre">x</span> <span class="pre">L</span></code> is computed, we can again multiply it by our Values matrix which is also from our English embeddings, so we are multiplying a <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span> <span class="pre">x</span> <span class="pre">L</span></code> by a <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">L</span> <span class="pre">x</span> <span class="pre">E</span></code>, producing the final output of <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span> <span class="pre">x</span> <span class="pre">E</span></code>! Why does this matter? Because our original French was also <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span> <span class="pre">x</span> <span class="pre">E</span></code>! Remember, our attention computation told us how every French token is related to every English token. Then, the output of our full attention computation does the weighted average, so our new <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span> <span class="pre">x</span> <span class="pre">E</span></code> is the weighted average of those english tokens. Lets see what that looks like:</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-cross_attention_weight_by_val.png" width=600></img>
</div></section>
<section id="span-style-color-lightgreen-basically-nothing-changed-span">
<h3><span style="color:LightGreen">Basically Nothing Changed</span><a class="headerlink" href="#span-style-color-lightgreen-basically-nothing-changed-span" title="Permalink to this heading">#</a></h3>
<p>Nothing really changed in the end! Cross Attention is just attention between two different sequences rather than than one sequence to itself. This means we also have multihead attention (although I didn’t draw it), so nothing changes at all. Cross Attention is <span style="color:Violet">ALWAYS</span> an encoder, not autoregressive. It doesnt make sense to have a causal mask here because one sequence can look at the entirety of the other (i.e. our input english can look at all of the generated french). But there is one more caveat:</p>
</section>
<section id="span-style-color-lightgreen-padding-on-english-and-french-span">
<h3><span style="color:LightGreen">Padding on English and French</span><a class="headerlink" href="#span-style-color-lightgreen-padding-on-english-and-french-span" title="Permalink to this heading">#</a></h3>
<p>We are now training a language translation model, this means that when we batch data, both english and french can be padded, and they can be differently padded. For example lets say we have the following data (pairs of english and french tokens)</p>
<ul class="simple">
<li><p>English: [a1, a2, a3, a4], French: [b1, b2, b3, b4, b5]</p></li>
<li><p>English: [c1, c2, c3], French: [d1, d2]</p></li>
</ul>
<p>This means, when we batch the english we have to pad the sequence of length 3 to the sequence of length 4.</p>
<ul class="simple">
<li><p>[a1, a2, a3, a4]</p></li>
<li><p>[c1, c2, c3, <strong><span style="color:Blue">&lt;PAD&gt;</span></strong>]</p></li>
</ul>
<p>And similarly, when we batch the french, we have to pad the sequence of length 2 to the sequence of length 5.</p>
<ul class="simple">
<li><p>[b1, b2, b3, b4, b5]</p></li>
<li><p>[d1, d2, <strong><span style="color:Blue">&lt;PAD&gt;</span></strong>, <strong><span style="color:Blue">&lt;PAD&gt;</span></strong>, <strong><span style="color:Blue">&lt;PAD&gt;</span></strong>]</p></li>
</ul>
<p>So we have padding on both dimensions, but again nothing changes. Remember before (in self attention), when we have pad tokens we just zeroed out the columns of the cooresponding pad tokens. But really, we are zeroing out the <span style="color:Violet">Keys</span> columns. We want to make sure that when we learn how the French is related to the English, that the French is <span style="color:Violet">not looking at English Pad Tokens</span>. It is ok though for French pad tokens to look at English. (i.e. the Query pad tokens can look at the keys, but the Query non-pad tokens should not look at the Key pad tokens). This is what this would again look like:</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/Attention-cross_attention_padding.png" width=800></img>
</div><p>As we can see here, all French tokens (french padding included) can look at all non-padding english tokens. This is identical to what we did before, its just now our attention matrix just isn’t square.</p>
</section>
</section>
<section id="span-style-color-orange-implementing-cross-attention-span">
<h2><span style="color:Orange">Implementing Cross Attention</span><a class="headerlink" href="#span-style-color-orange-implementing-cross-attention-span" title="Permalink to this heading">#</a></h2>
<p>Implementing Cross Attention shouldn’t be all that different from before. There is no causality here, so we only have to keep an eye on the attention mask. We will have two attention masks now, one for the english and another for the french. But when doing English to French Translation, we really just care about the English padding mask, so we can remove those columns from our attention computation. We do need the French padding mask though when we are doing french self-attention, but lets focus on the Cross Attention now</p>
<p>In this implementation, we will be passing in both a <code class="docutils literal notranslate"><span class="pre">src</span></code> (our English) and <code class="docutils literal notranslate"><span class="pre">tgt</span></code> (our French).</p>
<section id="id2">
<h3><span style="color:LightGreen">Matching Shapes for MultiHead Attention</span><a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>We again have our attention mask and need to do the reshapes and repeats to get it to the shape necessary to mask our attention computation! Lets take a look at our shapes for Cross Attention again.</p>
<p><code class="docutils literal notranslate"><span class="pre">attn.shape</span></code> - (Batch x num_heads x french_seq_len x english_seq_len)</p>
<p><code class="docutils literal notranslate"><span class="pre">mask.shape</span></code> - (Batch x english_seq_len)</p>
<p>So just like before, we can take our mask, and add dimensions from <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">english_seq_len</span></code> to <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">english_seq_len</span></code> which would be enough, except our Flash Attention implementation will expect a <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">french_seq_len</span> <span class="pre">x</span> <span class="pre">english_seq_len</span></code>. So then we can take our <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">english_seq_len</span></code> and repeat the dummy 1 dimension which is a placeholder for the french_seq_len and repeat it french_seq_len times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cross Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
               <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
               <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            embed_dim: Transformer Embedding Dimension</span>
<span class="sd">            num_heads: Number of heads of computation for Attention </span>
<span class="sd">            attn_p: Probability for Dropout2d on Attention cube</span>
<span class="sd">            proj_p: Probability for Dropout on final Projection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="nb">super</span><span class="p">(</span><span class="n">CrossAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
        
        <span class="c1">### Define all our Projections ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="n">batch</span><span class="p">,</span> <span class="n">src_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">tgt_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1">### Compute Q (on our tgt French) ###</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">tgt_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="c1">### Compute K, V (on src English) Projections ###</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        
        <span class="c1">### Perform Attention Computation ###</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="c1">####################################################################################</span>
        <span class="c1">### FILL ATTENTION MASK WITH -Infinity ###</span>

        <span class="c1">### NOTE: </span>
        <span class="c1">### attn.shape - (Batch x num_heads x french_seq_len x english_seq_len)</span>
        <span class="c1">### mask.shape - (Batch x english_seq_len)</span>

        <span class="c1">### Need to expand mask (Batch x english_seq_len) -&gt; (Batch x 1 x 1 x english_seq_len) -&gt; (Batch x 1 x french_seq_len x english_seq_len)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">tgt_seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1">####################################################################################</span>
         
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>
        
        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">tgt_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>


<span class="c1">### We will now have sequences of different lengths, identify the number of tokens in each sequence ###</span>
<span class="n">english_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">french_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">18</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">CrossAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create random tensor in the shape (Batch x Seq Len x Embed Dim) for French and English ###</span>
<span class="c1">### This will be a tensor upto the max(seq_lens) ###</span>
<span class="n">rand_english</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>
<span class="n">rand_french</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>


<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###</span>
<span class="n">english_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">english_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">french_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">french_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;English Attention Mask:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;French Attention Mask:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">french_masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">rand_english</span><span class="p">,</span> <span class="n">tgt</span><span class="o">=</span><span class="n">rand_french</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Output:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>English Attention Mask:
tensor([[ True,  True,  True, False, False],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True, False]])
French Attention Mask:
tensor([[ True,  True,  True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True,  True, False],
        [ True,  True, False, False, False, False, False]])
Final Output: torch.Size([3, 7, 18])
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-lightgreen-flash-attention-span">
<h3><span style="color:LightGreen">Flash Attention</span><a class="headerlink" href="#span-style-color-lightgreen-flash-attention-span" title="Permalink to this heading">#</a></h3>
<p>Although our attention computations that we have done are completely correct, they are not the most efficient! <a class="reference external" href="https://github.com/Dao-AILab/flash-attention">Flash Attention</a> is a highly optimized hardware aware attention implementation. Due to the structure of the attention computation you can actually opt for a faster tiled matrix multiplication that fuses the matrix multiplication, softmax and scaling into a single CUDA kernel. We were obviously doing this as separate steps, beacause we don’t have that level of control in PyTorch over CUDA. The most expensive part of GPU operations is copying between global memory and GPU shared memory, and so by merging a bunch of operations together, we just get faster attention computations especially for long sequences.</p>
<p>There are a few places we can access this, but the easiest for us is to use <code class="docutils literal notranslate"><span class="pre">torch.scaled_dot_product_attention</span></code> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">documentation</a></p>
<p>This method allows us to pass in:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Queries</span></code>: (B x H x L x E)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Keys</span></code>: (B x H x S x E)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Values</span></code>: (B x H x S x E)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attn_mask</span></code>: (B x 1 x L x S)</p></li>
</ul>
<p>So as you can see, Flash Attention supports our <code class="docutils literal notranslate"><span class="pre">Queries</span></code> being different from <code class="docutils literal notranslate"><span class="pre">Keys</span></code>/<code class="docutils literal notranslate"><span class="pre">Values</span></code> as we saw in our Cross Attention implementation. In the case of self-attention <span class="math notranslate nohighlight">\(L = S\)</span> which also isn’t a problem. The only extra step on our end is to make sure our <code class="docutils literal notranslate"><span class="pre">attn_mask</span></code> is of shape (B x 1 x L x S), which means we have to do the extra repeat, as it wont automatically broadcast along the <span class="math notranslate nohighlight">\(L\)</span> dimension if we left it as (B x 1 x 1 x S). Flash Attention also expects tokens we don’t want to compute attention on to be <code class="docutils literal notranslate"><span class="pre">False</span></code> in the attention mask!</p>
<p>The other option that is important it:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">is_causal</span></code>: True/False Boolean indicating if we want a causal mask. The method will apply the causal mask itself so we don’t need to do it!</p></li>
</ul>
</section>
<section id="span-style-color-lightgreen-lets-add-flash-attention-to-our-self-attention-span">
<h3><span style="color:LightGreen">Lets Add Flash Attention to our Self Attention</span><a class="headerlink" href="#span-style-color-lightgreen-lets-add-flash-attention-to-our-self-attention-span" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
                 <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">fused_attn</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            embed_dim: Transformer Embedding Dimension</span>
<span class="sd">            num_heads: Number of heads of computation for Attention </span>
<span class="sd">            attn_p: Probability for Dropout2d on Attention cube</span>
<span class="sd">            proj_p: Probability for Dropout on final Projection</span>
<span class="sd">            causal: Do you want to apply a causal mask?</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fused</span> <span class="o">=</span> <span class="n">fused_attn</span>
        
        <span class="c1">### Define all our Projections ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim] </span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="c1">### Use Flash Attention ###</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused</span><span class="p">:</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">,</span>
                                               <span class="n">is_causal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">causal</span><span class="p">,</span>
                                               <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
        
            <span class="c1">### Perform Attention Computation ###</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
    
            
                
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span><span class="p">:</span>
                <span class="c1">####################################################################################</span>
                <span class="c1">### Create the Causal Mask (On the correct device) ###</span>
    
                <span class="c1">### Create a Seq_Len x Seq_Len tensor full of Ones</span>
                <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">attn</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                
                <span class="c1">### Fill Top right triangle with Zeros (as we dont want to attend to them) ###</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span> 
    
                <span class="c1">### Add extra dimensions for Batch size and Number of Heads ###</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
    
                <span class="c1">### If we have padding mask, then update our causal mask ###</span>
                <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    
                    <span class="c1">### Each sample could have a different number of pad tokens, so repeat causal mask for batch size ###</span>
                    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
                    <span class="c1">### Expand and repeat the Padding Mask (b x s) -&gt; (b x 1 x s x s)###</span>
                    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    
                    <span class="c1">### Fill causal mask where attention mask is False with False (to ensure all padding tokens are masked out) ###</span>
                    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    
                <span class="c1">### Fill attn with -inf wherever causal mask is False ###</span>
                <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
    
            <span class="c1">####################################################################################</span>
        
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>
        
        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="span-style-color-lightgreen-putting-it-all-together-span">
<h3><span style="color:LightGreen">Putting it All Together</span><a class="headerlink" href="#span-style-color-lightgreen-putting-it-all-together-span" title="Permalink to this heading">#</a></h3>
<p>Obviously, we would like to use Flash Attention for Everything, but we have a few moving pieces right now:</p>
<ol class="arabic simple">
<li><p>We can have self-attention on a source with padding mask</p></li>
<li><p>We can have self-attention on a source with padding mask and causal mask</p></li>
<li><p>We can have cross attention between a source and target</p></li>
</ol>
<p>Lets write our final Attention computation, putting all of these things together. Again, lets set the stage in terms of and English (src) to French (tgt) translation:</p>
</section>
<section id="span-style-color-lightgreen-attention-class-details-span">
<h3><span style="color:LightGreen">Attention Class Details</span><a class="headerlink" href="#span-style-color-lightgreen-attention-class-details-span" title="Permalink to this heading">#</a></h3>
<p>This class will handle all the cases we need. Lets pretend we are doing English to French</p>
<ul class="simple">
<li><p>We can provide English as the src along with its padding mask for Encoder self-attention</p></li>
<li><p>We can provide French as the src along with its padding mask and causal as True for decoder self-attention</p></li>
<li><p>We can provide English as src and French as tgt along with the src padding_mask for cross attention</p></li>
</ul>
<p>All of this should be very familiar now as we have implemented all the pieces of this already! Its just time to put it together!</p>
</section>
<section id="span-style-color-lightgreen-attention-mask-for-self-attention-span">
<h3><span style="color:LightGreen">Attention Mask for Self-Attention</span><a class="headerlink" href="#span-style-color-lightgreen-attention-mask-for-self-attention-span" title="Permalink to this heading">#</a></h3>
<p>Attention Mask is in <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">Sequence</span> <span class="pre">Length</span></code> where we have <code class="docutils literal notranslate"><span class="pre">False</span></code> for tokens we don’t want to attend to. <code class="docutils literal notranslate"><span class="pre">F.scaled_dot_product_attention</span></code> expects a mask of the shape <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">...,</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Seq_len</span></code>, where the “…” in this case is any extra dimensions (such as heads of attention). Lets expand our mask to <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Seq_len</span></code>. The 1 in this case refers to the number of heads of attention we want, so it is a dummy index to broadcast over. In each <code class="docutils literal notranslate"><span class="pre">Seq_len</span> <span class="pre">x</span> <span class="pre">Seq_len</span></code> matrix for every batch, we want False for all columns corresponding to padding tokens.</p>
</section>
<section id="span-style-color-lightgreen-attention-mask-for-cross-attention-span">
<h3><span style="color:LightGreen">Attention Mask for Cross-Attention</span><a class="headerlink" href="#span-style-color-lightgreen-attention-mask-for-cross-attention-span" title="Permalink to this heading">#</a></h3>
<p>When doing cross attention, our French will be <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">french_len</span> <span class="pre">x</span> <span class="pre">embed_dim</span></code> and our English will be <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">english_len</span> <span class="pre">x</span> <span class="pre">embed_dim</span></code>. In typical cross attention fashion, the queries will be the thing we want and Keys/Values will be the thing we are crossing with. In our Decoder Cross Attention, we want to learn how our generated French is related to the encoded english from the Encoder. So our Queries will be French and Keys/Values will be the encoded English.</p>
<p><span class="math notranslate nohighlight">\(Q\)</span> &#64; <span class="math notranslate nohighlight">\(K^T\)</span> will then give a shape <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">...</span> <span class="pre">x</span> <span class="pre">french_len</span> <span class="pre">x</span> <span class="pre">english_len</span></code>. This means our attention mask also has to have this shape. Just like before, we want to mask out the columns of the attention mask, so our french tokens dont attend to any english padding tokens. We can then take our english padding mask which is (Batch x english_len), add extra dimensions for head and src_len dimension which will give a <code class="docutils literal notranslate"><span class="pre">Batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">english_len</span></code> and then repeat the mask for the source length <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">french_len</span> <span class="pre">x</span> <span class="pre">english_len</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Regular Self-Attention but in this case we utilize flash_attention</span>
<span class="sd">    incorporated in the F.scaled_dot_product_attention to speed up our training. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dimension</span> <span class="o">=</span> <span class="n">embedding_dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        
        <span class="c1">### Sanity Checks ###</span>
        <span class="k">assert</span> <span class="n">embedding_dimension</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Double check embedding dim divisible by number of heads&quot;</span>

        <span class="c1">### Attention Head Dim ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embedding_dimension</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="c1">### Attention Projections ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>

        <span class="c1">### Post Attention Projection ###</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                <span class="n">src</span><span class="p">,</span> 
                <span class="n">tgt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        By default, self-attention will be computed on src (with optional causal and/or attention mask). If tgt is provided, then</span>
<span class="sd">        we are doing cross attention. In cross attention, an attention_mask can be used, but no causal mask can be applied.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1">### Grab Shapes ###</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1">### If target is not provided, we are doing self attention (with potential causal mask) ###    </span>
        <span class="k">if</span> <span class="n">tgt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">src_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">attention_out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">,</span> 
                                                           <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> 
                                                           <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span> 
                                                           <span class="n">is_causal</span><span class="o">=</span><span class="n">causal</span><span class="p">)</span>
        
        <span class="c1">### If target is provided then we are doing cross attention ###</span>
        <span class="c1">### Our query will be the target and we will be crossing it with the encoder source (keys and values) ###</span>
        <span class="c1">### The src_attention_mask will still be the mask here, just repeated to the target size ###</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_len</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">tgt_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">attention_out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">,</span>
                                                           <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> 
                                                           <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span> 
                                                           <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1">### Reshape and Project ###</span>
        <span class="n">attention_out</span> <span class="o">=</span> <span class="n">attention_out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">attention_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attention_out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attention_out</span>

<span class="c1">### Test Out Self-Attention!! ###</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TESTING SELF-ATTENTION!!!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------------------&quot;</span><span class="p">)</span>
<span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###</span>
<span class="c1">### This will be a tensor upto the max(seq_lens) ###</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###</span>
<span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention Mask:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">rand</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Output:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TESTING CROSS-ATTENTION!!!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------------------&quot;</span><span class="p">)</span>
<span class="c1">### Test out Cross Attention </span>
<span class="c1">### We will now have sequences of different lengths, identify the number of tokens in each sequence ###</span>
<span class="n">english_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">french_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create random tensor in the shape (Batch x Seq Len x Embed Dim) for French and English ###</span>
<span class="c1">### This will be a tensor upto the max(seq_lens) ###</span>
<span class="n">rand_english</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>
<span class="n">rand_french</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>


<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###</span>
<span class="n">english_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">english_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">french_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">french_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;English Attention Mask:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;French Attention Mask:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">french_masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">rand_english</span><span class="p">,</span> <span class="n">tgt</span><span class="o">=</span><span class="n">rand_french</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Output:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TESTING SELF-ATTENTION!!!
-------------------------
Attention Mask:
tensor([[ True,  True,  True, False, False],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True, False]])
Final Output: torch.Size([3, 5, 9]) 

TESTING CROSS-ATTENTION!!!
-------------------------
English Attention Mask:
tensor([[ True,  True,  True, False, False],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True, False]])
French Attention Mask:
tensor([[ True,  True,  True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True,  True, False],
        [ True,  True, False, False, False, False, False]])
Final Output: torch.Size([3, 7, 9])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="span-style-color-orange-thats-it-span">
<h2><span style="color:Orange">Thats It!</span><a class="headerlink" href="#span-style-color-orange-thats-it-span" title="Permalink to this heading">#</a></h2>
<p>That is basically everything you need to know about attention! Now if you are actually going to be using attention in your model, it is better to use optimized cuda implementations rather than this only for speed and efficiency reasons, but they are doing exactly the same thing underneath the hood, just faster! This sets us up though for the next step! All we have done so far is implement the Attention mechanism, this is only half the picture for the Transformer. There are bunch of directions one can now go:</p>
<ul class="simple">
<li><p>Seq2Seq Models (Language Translation, Image Captioning)</p></li>
<li><p>Encoder Models (RoBERTa, Vision Transformer)</p></li>
<li><p>Decoder Models (GPT, LLAMA)</p></li>
</ul>
</section>
<section id="span-style-color-orange-acknowledgments-span">
<h2><span style="color:Orange">Acknowledgments</span><a class="headerlink" href="#span-style-color-orange-acknowledgments-span" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Initial version: Mark Neubauer</p>
<ul>
<li><p>Modified from the following <a class="reference external" href="https://github.com/priyammaz/PyTorch-Adventures/tree/main/PyTorch%20for%20Transformers/Attention%20Mechanisms/Attention">Tutorial</a></p></li>
</ul>
</li>
</ul>
<p>© Copyright 2025</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./_sources/lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../Week_08.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span style="color: blue;"><b>Attention Mechanism and Transformers</b></span></p>
      </div>
    </a>
    <a class="right-next"
       href="Transformers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Transformers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-recap-recurrent-neural-networks-span"><span style="color:Orange">Recap: Recurrent Neural Networks</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-attention-augmented-rnn-span"><span style="color:Orange">Attention Augmented RNN</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-problems-span"><span style="color:LightGreen">Problems</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-attention-is-all-you-need-span"><span style="color:Orange">Attention is All You Need!</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-step-1-compute-the-attention-matrix-span"><span style="color:LightGreen">Step 1: Compute the Attention Matrix</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-recap-the-dot-product-span"><span style="color:LightPink">Recap: The Dot Product</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-recap-matrix-multiplication-span"><span style="color:LightPink">Recap: Matrix Multiplication</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-step-2-weighting-the-values-matrix-span"><span style="color:LightGreen">Step 2: Weighting the Values Matrix</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-enforcing-causality-span"><span style="color:LightGreen">Enforcing Causality</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-lets-build-this-span"><span style="color:LightGreen">Lets Build This!</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-implement-attention-without-any-learnable-parameters-span"><span style="color:LightPink">Implement Attention Without Any Learnable Parameters</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-lets-add-learnable-parameters-span"><span style="color:LightPink">Lets Add Learnable Parameters</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-multiheaded-attention-span"><span style="color:LightGreen">MultiHeaded Attention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-single-headed-attention-recap-span"><span style="color:LightPink">Single Headed Attention Recap</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-moving-to-multiheaded-attention-span"><span style="color:LightPink">Moving to MultiHeaded Attention</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-increasing-efficiency-span"><span style="color:LightGreen">Increasing Efficiency</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-linear-layers-on-multidimensional-tensors-span"><span style="color:LightPink">Linear Layers on MultiDimensional Tensors</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-packing-linear-layers-span"><span style="color:LightPink">Packing Linear Layers</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-multidimensional-matrix-multiplication-span"><span style="color:LightPink">MultiDimensional Matrix Multiplication</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightpink-recap-span"><span style="color:LightPink">Recap</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-the-trick-of-parallelizing-heads-span"><span style="color:LightGreen">The Trick of Parallelizing Heads</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-more-efficient-attention-implementation-span"><span style="color:LightGreen">More Efficient Attention Implementation</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-padding-span"><span style="color:Orange">Padding</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-sequence-padding-and-attention-masking-span"><span style="color:LightGreen">Sequence Padding and Attention Masking</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-computing-the-reweighted-padded-attention-mask-span"><span style="color:Orange">Computing the Reweighted Padded Attention Mask</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-add-padding-to-our-attention-computation-span"><span style="color:LightGreen">Add Padding to our Attention Computation</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-repeating-to-match-attention-matrix-shape-span"><span style="color:LightGreen">Repeating to Match Attention Matrix Shape</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-matching-shapes-for-multihead-attention-span"><span style="color:LightGreen">Matching Shapes for MultiHead Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><span style="color:LightGreen">Enforcing Causality</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-computing-the-reweighted-causal-attention-mask-span"><span style="color:LightGreen">Computing the Reweighted Causal Attention Mask</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-mask-span"><span style="color:LightGreen">Attention Mask</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-lets-incorporate-the-causal-mask-into-self-attention-span"><span style="color:LightGreen">Lets Incorporate the Causal Mask into Self-Attention</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-cross-attention-span"><span style="color:Orange">Cross Attention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-cross-attention-queries-keys-and-values-span"><span style="color:LightGreen">Cross Attention Queries, Keys and Values</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-tensor-shapes-for-cross-attention-span"><span style="color:LightGreen">Tensor Shapes for Cross Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-performing-the-attention-computation-span"><span style="color:LightGreen">Performing the Attention Computation</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-the-most-important-part-span"><span style="color:LightGreen">THE MOST IMPORTANT PART!!!</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-basically-nothing-changed-span"><span style="color:LightGreen">Basically Nothing Changed</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-padding-on-english-and-french-span"><span style="color:LightGreen">Padding on English and French</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-implementing-cross-attention-span"><span style="color:Orange">Implementing Cross Attention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><span style="color:LightGreen">Matching Shapes for MultiHead Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-flash-attention-span"><span style="color:LightGreen">Flash Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-lets-add-flash-attention-to-our-self-attention-span"><span style="color:LightGreen">Lets Add Flash Attention to our Self Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-putting-it-all-together-span"><span style="color:LightGreen">Putting it All Together</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-class-details-span"><span style="color:LightGreen">Attention Class Details</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-mask-for-self-attention-span"><span style="color:LightGreen">Attention Mask for Self-Attention</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-attention-mask-for-cross-attention-span"><span style="color:LightGreen">Attention Mask for Cross-Attention</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-thats-it-span"><span style="color:Orange">Thats It!</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mark Neubauer
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>