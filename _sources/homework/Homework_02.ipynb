{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 02: Probability Theory and Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for Getting, Loading and Locating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_data(url: str):\n",
    "    local_path = './tmp_data'\n",
    "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
    "    rc = None\n",
    "    while rc is None:\n",
    "      line = p.stderr.readline().strip('\\n')\n",
    "      if len(line) > 0:\n",
    "        print(line)\n",
    "      rc = p.poll()\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below to calculate the event probabilities $P(A)$, $P(B)$, $P(A \\cap B)$ and the conditional probabilities $P(A\\mid B)$, $P(B\\mid A)$ for an arbitrary (finite) probability space specified by each outcome's probability. *Hint: the probability of an event containing a set of outcomes is just the sum of the individual outcome probabilities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(p, A, B):\n",
    "    \"\"\"Calculate probabilities for an arbitrary probability space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : float array of shape (N,)\n",
    "        Probabilities for each of the N possible outcomes in the probability space.\n",
    "    A : boolean array of shape (N,)\n",
    "        Identifies members of event set A in the probability space.\n",
    "    B : boolean array of shape (N,)\n",
    "        Identifies members of event set B in the probability space.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple of five probabilities values:\n",
    "        P(A), P(B), P(A instersect B), P(A | B), P(B | A).\n",
    "    \"\"\"\n",
    "    assert np.all((p >= 0) & (p <= 1))\n",
    "    assert np.sum(p) == 1\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correction solution should pass the tests below.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "N = 100\n",
    "p = gen.uniform(size=(4, N))\n",
    "p = (p / p.sum(axis=1).reshape(-1, 1)).reshape(-1) / 4.\n",
    "\n",
    "# Test when A and B are \"independent\" events, i.e., P(A interset B) = P(A) P(B).\n",
    "A = np.arange(4 * N) < 2 * N\n",
    "B = (np.arange(4 * N) >= N) & (np.arange(4 * N) < 3 * N)\n",
    "assert np.allclose(\n",
    "    np.round(calculate_probabilities(p, A, B), 3),\n",
    "    [0.5, 0.5, 0.25, 0.5, 0.5])\n",
    "\n",
    "# Test with randomly generated events A, B.\n",
    "A = gen.uniform(size=4*N) < 0.3\n",
    "B = gen.uniform(size=4*N) > 0.6\n",
    "#print(np.round(event_probabilities(p, A, B), 3))\n",
    "assert np.allclose(\n",
    "    np.round(calculate_probabilities(p, A, B), 3),\n",
    "    [0.278, 0.33, 0.076, 0.23, 0.273])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative distribution function (CDF) is the fundamental representation of a random variable, rather than the probability density function (PDF) which might not be defined, is not a probability and generally has dimensions. In this problem, you will explore a practical application of the CDF for generating random numbers.\n",
    "\n",
    "Since the CDF $y = F_X(x)$ maps from random variable values to the range $[0,1]$, its inverse $x = F_X^{-1}(y)$ maps from $[0,1]$ back to the random variable. What distribution of $y$ values would generate values according to the PDF $f_X(x)$ when transformed by the inverse $F_X^{-1}(y)$? The answer is a uniform distribution, as we can demonstrate numerically for an arbitrary random variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_hist(X, n=10000, seed=123):\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # Generate n random value from the scipy.stats distribution X.\n",
    "    x = X.rvs(n, random_state=gen)\n",
    "    plt.hist(x, bins=50, label='$f_X(x)$', histtype='step', lw=2, density=True)\n",
    "    # Histogram the corresponding CDF values.\n",
    "    y = X.cdf(x)\n",
    "    plt.hist(y, bins=20, label='$F_X(x)$', alpha=0.5, density=True)\n",
    "    plt.xlabel('x')\n",
    "    plt.legend(loc='upper center', ncol=2)\n",
    "    \n",
    "cdf_hist(scipy.stats.beta(0.9, 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the function $F_X(x)$ can be inverted analytically, you can use it to transform uniformly generated random values into a random sampling of the PDF $f_X(x)$.\n",
    "\n",
    "For example, consider random outcomes consisting of $(x,y)$ points uniformly distributed on the disk,\n",
    "$$\n",
    "0 \\le r_1 \\le \\sqrt{x^2 + y^2} \\le r_2 \\; .\n",
    "$$\n",
    "The CDF of the random variable $r \\equiv \\sqrt{x^2 + y^2}$ is then\n",
    "$$\n",
    "F_R(r) = \\begin{cases}\n",
    "1 & r > r_2 \\\\\n",
    "\\frac{r^2 - r_1^2}{r_2^2 - r_1^2} & r_1 \\le r \\le r_2 \\\\\n",
    "0 & r < r_1\n",
    "\\end{cases}\\; .\n",
    "$$\n",
    "Implement the function below to apply $F_R^{-1}(y)$ to uniformly distributed random values in order to sample $f_R(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_disk(r1, r2, n, gen):\n",
    "    \"\"\"Sample random radii for points uniformly distributed on a disk.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r1 : float\n",
    "        Inner radius of disk.\n",
    "    r2 : float\n",
    "        Outer radius of disk.\n",
    "    n : int\n",
    "        Number of random samples to generate.\n",
    "    gen : np.random.RandomState\n",
    "        Random state for reproducible random numbers.\n",
    "        Uses gen.uniform() internally, not gen.rand().\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Array of n randomly generated r values.\n",
    "    \"\"\"\n",
    "    assert (r1 >= 0) and (r1 < r2)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "r1, r2, n = 1., 2., 1000\n",
    "gen = np.random.RandomState(seed=123)\n",
    "r = sample_disk(r1, r2, n, gen)\n",
    "assert np.all((r >= r1) & (r <= r2))\n",
    "assert np.allclose(np.round(np.mean(r), 3), 1.556)\n",
    "assert np.allclose(np.round(np.std(r), 3), 0.279)\n",
    "\n",
    "r1, r2, n = 0., 2., 1000\n",
    "r = sample_disk(r1, r2, n, gen)\n",
    "assert np.all((r >= r1) & (r <= r2))\n",
    "assert np.allclose(np.round(np.mean(r), 3), 1.325)\n",
    "assert np.allclose(np.round(np.std(r), 3), 0.494)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by plotting some $(x,y)$ points with uniformly random $0 \\le \\theta < 2\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.RandomState(seed=123)\n",
    "r = sample_disk(0.8, 1.2, 1000, gen)\n",
    "theta = gen.uniform(0, 2 * np.pi, size=len(r))\n",
    "plt.scatter(r * np.cos(theta), r * np.sin(theta), s=5)\n",
    "plt.gca().set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes $F_X(x)$ cannot be inverted explicitly, either because the inverse has no closed form or because the underlying distribution is arbitrary.  In these cases, we can still apply the same method numerically.\n",
    "\n",
    "Implement the function below to tabulate an empirical estimate of the CDF for an arbitrary random variable, as:\n",
    "$$\n",
    "x_{CDF} = x_{\\text{lo}}, x_0, x_1, \\ldots, x_{N-1}, x_{\\text{hi}} \\; ,\n",
    "$$\n",
    "where the $x_i$ are [sorted](https://docs.scipy.org/doc/numpy/reference/generated/numpy.interp.html), $x_0 \\le x_1 \\le \\ldots \\le x_{N-1}$, and corresponding CDF values:\n",
    "$$\n",
    "y_{CDF} = 0, \\frac{1}{N+1}, \\frac{2}{N+1}, \\ldots, \\frac{N}{N+1}, 1 \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_cdf(x, xlo, xhi):\n",
    "    \"\"\"Tabulate the empirical CDF from samples of an arbitrary random variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array of shape (N,)\n",
    "        Array of input random variable values to use.\n",
    "    xlo : float\n",
    "        Low limit for the random variable x.\n",
    "    xhi : float\n",
    "        High limit for the random variable x.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple (x_cdf, y_cdf) of arrays both of shape (N+2,), padded at each end\n",
    "        as described above.\n",
    "    \"\"\"\n",
    "    assert xlo < xhi\n",
    "    x = np.asarray(x)\n",
    "    assert np.all((x >= xlo) & (x <= xhi))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "x_cdf, y_cdf = empirical_cdf([1, 2, 3, 4], 0, 5)\n",
    "assert np.array_equal(x_cdf, [0, 1, 2, 3, 4, 5])\n",
    "assert np.allclose(y_cdf, [0., .2, .4, .6, .8, 1.])\n",
    "\n",
    "x_cdf, y_cdf = empirical_cdf([4, 2, 1, 3], 0, 5)\n",
    "assert np.array_equal(x_cdf, [0, 1, 2, 3, 4, 5])\n",
    "assert np.allclose(y_cdf, [0., .2, .4, .6, .8, 1.])\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "x = scipy.stats.beta(0.9, 1.5).rvs(size=4, random_state=gen)\n",
    "x_cdf, y_cdf = empirical_cdf(x, 0., 1.)\n",
    "assert np.allclose(\n",
    "    np.round(x_cdf, 3),\n",
    "    [ 0.   ,  0.087,  0.152,  0.42 ,  0.721,  1.   ])\n",
    "assert np.allclose(y_cdf, [0., .2, .4, .6, .8, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by generating CDF samples matched to an unknown distribution.  Note that we use [linear interpolation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.interp.html) to numerically invert the empirical CDF in this approach, which is a useful trick to remember:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = scipy.stats.beta(0.9, 1.5)\n",
    "# Generate samples using scipy.stats\n",
    "x_in = X.rvs(n, random_state=gen)\n",
    "plt.hist(x_in, bins=50, label='Input data', alpha=0.5, density=True)\n",
    "# Generate samples using the empirical CDF of x_in\n",
    "x_cdf, y_cdf = empirical_cdf(x_in, 0., 1.)\n",
    "y = gen.uniform(size=n)\n",
    "x = np.interp(y, y_cdf, x_cdf)\n",
    "plt.hist(x, bins=50, label='CDF samples', histtype='step', lw=2, density=True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sequence of $n$ Bernoulli trials with success probabilty $p$ per trial. A string of consecutive successes is known as a success run. Write a function that returns the counts for runs of length $k$ for each $k$ observed in a python dictionary.\n",
    "\n",
    "For example: if the trials were `[0, 1, 0, 1, 1, 0, 0, 0, 0, 1]`, the function should return `{1: 2, 2: 1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_runs(xs):\n",
    "    \"\"\"Count number of success runs of length k.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xs : array of shape (1, nx)\n",
    "        Vector of Bernoulli trials\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Returns a dictionary the counts (val) for runs of length k (key) for each k observed\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = count_runs([0, 1, 0, 1, 1, 0, 0, 0, 0, 1],)\n",
    "assert [cnt[1],cnt[2]] == [2,1]\n",
    "\n",
    "cnt = count_runs([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1],)\n",
    "assert [cnt[1],cnt[2],cnt[3]] == [1,1,1]\n",
    "\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "cnt = count_runs(rng.integers(2,size=1000))\n",
    "assert [cnt[1],cnt[2],cnt[3],cnt[4],cnt[5],cnt[6],cnt[7],cnt[8],cnt[9]] == [127,60,26,24,6,4,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `count_runs` above, write a method `run_prob` that returns the probability of observing at least one run of length `k` or more from `n` trials with success probabilty `p` per trial. This probability is estimated from `expts` simulated experinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prob(expts, n, k, p, seed=1234):\n",
    "    \"\"\"Calculate the probability of observing at least one run of length `k` or more from `n` trials with success probabilty `p` per trial. This probability is estimated from `expts` simulated experinents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    expts : int\n",
    "        Number of simulated experiments\n",
    "    k: int\n",
    "        Number of consecutive successes defining a success run\n",
    "    n: int\n",
    "        Number of trials per experiment\n",
    "    p: float\n",
    "        Success probability per trial\n",
    "    seed : int\n",
    "        Random number seed to use.    \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Returns the probability of observing at least one run of length `k` or more\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize random generator. Use this generator in your code below\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the probability of observing at least one run of length $k$=5 or more when $n$=100 and $p$=0.5. Estimate this probability from 100,000 simulated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = run_prob(expts=100000, n=100, k=5, p=0.5)\n",
    "print (np.round(prob*100,1),'%')\n",
    "assert np.allclose(prob, 0.81044, rtol=0.001, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the probability of observing at least one run of length $k$=7 or more when $n$=100 and $p$=0.7. Estimate this probability from 100,000 simulated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = run_prob(expts=100000, n=100, k=7, p=0.7)\n",
    "print (np.round(prob*100,1),'%')\n",
    "assert np.allclose(prob, 0.9489, rtol=0.001, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement the core of the E- and M-steps for the [Gaussian mixture model (GMM)](http://scikit-learn.org/stable/modules/mixture.html) method. Note the similarities with the E- and M-steps of the K-means method.\n",
    "\n",
    "First, implement the function below to evaluate the [multidimensional Gaussian probability density](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) for arbitrary mean $\\vec{\\mu}$ and covariance matrix $C$ (refer to the lecture for more details on the notation used here):\n",
    "$$\n",
    "G(\\vec{x} ; \\vec{\\mu}, C) = \\left(2\\pi\\right)^{-D/2}\\,\\left| C\\right|^{-1/2}\\,\n",
    "\\exp\\left[  -\\frac{1}{2} \\left(\\vec{x} - \\vec{\\mu}\\right)^T C^{-1} \\left(\\vec{x} - \\vec{\\mu}\\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mu, C):\n",
    "    \"\"\"Evaluate the Gaussian probability density.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        1D array of D feature values for a single sample\n",
    "    mu : array\n",
    "        1D array of D mean feature values for this component.\n",
    "    C : array\n",
    "        2D array with shape (D, D) of covariance matrix elements for this component.\n",
    "        Must be positive definite (and therefore symmetric).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Probability density.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    mu = np.asarray(mu)\n",
    "    C = np.asarray(C)\n",
    "    D = len(x)\n",
    "    assert x.shape == (D,) and mu.shape == (D,)\n",
    "    assert C.shape == (D, D)\n",
    "    assert np.allclose(C.T, C)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [1], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[2]]), 1 / np.sqrt(4 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [0], [[1]]), np.exp(-0.5) / np.sqrt(2 * np.pi))\n",
    "\n",
    "assert np.allclose(Gaussian_pdf([0, 0], [0, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, -1], [1, -1], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[4, 0], [0, 1]]), 1 / (4 * np.pi))\n",
    "\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, +1], [+1, 1]]), 5) == 0.07778\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, -1], [-1, 1]]), 5) == 0.07778\n",
    "assert np.round(np.log(Gaussian_pdf([1, 0, -1], [1, 2, 3], [[4, -1, 0], [-1, 1, 0], [0, 0, 2]])), 5) == -10.31936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the E-step in the function below. This consists of calculating the relative probability that each sample $\\vec{x}_n$ ($n$-th row of $X$) belongs to each component $k$:\n",
    "$$\n",
    "p_{nk} = \\frac{\\omega_k G(\\vec{x}_n; \\vec{\\mu}_k, C_k)}\n",
    "{\\sum_{j=1}^K\\, \\omega_j G(\\vec{x}_n; \\vec{\\mu}_j, C_j)}\n",
    "$$\n",
    "Note that these relative probabilities (also called *responsibilities*) sum to one over components $k$ for each sample $n$.  Also note that we consider the parameters ($\\omega_k$, $\\vec{\\mu}_k$, $C_k$) of each component fixed during this step. *Hint: use your `Gaussian_pdf` function here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, w, mu, C):\n",
    "    \"\"\"Perform a GMM E-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    w : array with shape (K,)\n",
    "        Per-component weights.\n",
    "    mu : array with shape (K, D)\n",
    "        Array of mean vectors for each component.\n",
    "    C : array with shape (K, D, D).\n",
    "        Array of covariance matrices for each component.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(w)\n",
    "    assert w.shape == (K,)\n",
    "    assert mu.shape == (K, D)\n",
    "    assert C.shape == (K, D, D)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "w = np.full(4, 0.25)\n",
    "mu = np.array([[-2], [-1], [0], [1]])\n",
    "C = np.ones((4, 1, 1))\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.258,  0.134,  0.058,  0.021,  0.006],\n",
    "     [ 0.426,  0.366,  0.258,  0.152,  0.077],\n",
    "     [ 0.258,  0.366,  0.426,  0.414,  0.346],\n",
    "     [ 0.058,  0.134,  0.258,  0.414,  0.57 ]])\n",
    "\n",
    "X = np.zeros((1, 3))\n",
    "w = np.ones((2,))\n",
    "mu = np.zeros((2, 3))\n",
    "C = np.zeros((2, 3, 3))\n",
    "diag = range(3)\n",
    "C[:, diag, diag] = 1\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.5], [ 0.5]])\n",
    "\n",
    "X = np.array([[0,0,0], [1,0,0]])\n",
    "mu = np.array([[0,0,0], [1,0,0]])\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.622,  0.378], [ 0.378,  0.622]])\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "subsample = X.reshape(K, (N//K), D)\n",
    "mu = subsample.mean(axis=1)\n",
    "C = np.empty((K, D, D))\n",
    "w = gen.uniform(size=K)\n",
    "w /= w.sum()\n",
    "for k in range(K):\n",
    "    C[k] = np.cov(subsample[k], rowvar=False)\n",
    "#print(repr(np.round(E_step(X, w, mu, C)[:, :5], 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C)[:, :5], 3) ==\n",
    "    [[ 0.422,  0.587,  0.344,  0.279,  0.19 ],\n",
    "     [ 0.234,  0.11 ,  0.269,  0.187,  0.415],\n",
    "     [ 0.291,  0.194,  0.309,  0.414,  0.279],\n",
    "     [ 0.053,  0.109,  0.077,  0.12 ,  0.116]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the M-step in the function below.  During this step, we consider the relative weights $p_{nk}$ from the previous step fixed and instead update the parameters of each component (which were fixed in the previous step), using:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\omega_k &= \\frac{1}{N}\\, \\sum_{n=1}^N\\, p_{nk} \\\\\n",
    "\\vec{\\mu}_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\vec{x}_n}{\\sum_{n=1}^N\\, p_{nk}} \\\\\n",
    "C_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\left( \\vec{x}_n - \\vec{\\mu}_k\\right) \\left( \\vec{x}_n - \\vec{\\mu}_k\\right)^T}\n",
    "{\\sum_{n=1}^N\\, p_{nk}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Make sure you understand why the last expression yields a matrix rather than a scalar dot product before jumping into the code. (If you would like a numpy challenge, try implementing this function without any loops, e.g., with `np.einsum`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(X, p):\n",
    "    \"\"\"Perform a GMM M-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    p : array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple w, mu, C of arrays with shapes (K,), (K, D) and (K, D, D) giving\n",
    "        the updated component parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(p)\n",
    "    assert p.shape == (K, N)\n",
    "    assert np.allclose(p.sum(axis=0), 1)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "p = np.full(20, 0.25).reshape(4, 5)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C, 5)))\n",
    "assert np.all(np.round(w, 5) == 0.25)\n",
    "assert np.all(np.round(mu, 5) == 0.0)\n",
    "assert np.all(np.round(C, 5) == 0.5)\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "p = gen.uniform(size=(K, N))\n",
    "p /= p.sum(axis=0)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C[0], 5)))\n",
    "assert np.all(\n",
    "    np.round(w, 5) == [ 0.25216,  0.24961,  0.24595,  0.25229])\n",
    "assert np.all(\n",
    "    np.round(mu, 5) ==\n",
    "    [[ 0.06606,  0.06   , -0.00413,  0.01562,  0.00258],\n",
    "     [ 0.02838,  0.01299,  0.01286,  0.03068, -0.01714],\n",
    "     [ 0.03157,  0.04558, -0.01206,  0.03493, -0.0326 ],\n",
    "     [ 0.05467,  0.06293, -0.01779,  0.04454,  0.00065]])\n",
    "assert np.all(\n",
    "    np.round(C[0], 5) ==\n",
    "    [[ 0.98578,  0.01419, -0.03717,  0.01403,  0.0085 ],\n",
    "     [ 0.01419,  0.95534, -0.02724,  0.03201, -0.00648],\n",
    "     [-0.03717, -0.02724,  0.90722,  0.00313,  0.0299 ],\n",
    "     [ 0.01403,  0.03201,  0.00313,  1.02891,  0.0813 ],\n",
    "     [ 0.0085 , -0.00648,  0.0299 ,  0.0813 ,  0.922  ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented the core of the GMM algorithm. Next you will use KMeans as a means of seeding the GMM model fit. First we include two helpful functions `draw_ellipses` and `GMM_parplot` to help display results. The details of these methods are not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "from matplotlib.collections import EllipseCollection\n",
    "\n",
    "def draw_ellipses(w, mu, C, nsigmas=2, color='red', outline=None, filled=True, axis=None):\n",
    "    \"\"\"Draw a collection of ellipses.\n",
    "\n",
    "    Uses the low-level EllipseCollection to efficiently draw a large number\n",
    "    of ellipses. Useful to visualize the results of a GMM fit via\n",
    "    GMM_pairplot() defined below.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array\n",
    "        1D array of K relative weights for each ellipse. Must sum to one.\n",
    "        Ellipses with smaller weights are rendered with greater transparency\n",
    "        when filled is True.\n",
    "    mu : array\n",
    "        Array of shape (K, 2) giving the 2-dimensional centroids of\n",
    "        each ellipse.\n",
    "    C : array\n",
    "        Array of shape (K, 2, 2) giving the 2 x 2 covariance matrix for\n",
    "        each ellipse.\n",
    "    nsigmas : float\n",
    "        Number of sigmas to use for scaling ellipse area to a confidence level.\n",
    "    color : matplotlib color spec\n",
    "        Color to use for the ellipse edge (and fill when filled is True).\n",
    "    outline : None or matplotlib color spec\n",
    "        Color to use to outline the ellipse edge, or no outline when None.\n",
    "    filled : bool\n",
    "        Fill ellipses with color when True, adjusting transparency to\n",
    "        indicate relative weights.\n",
    "    axis : matplotlib axis or None\n",
    "        Plot axis where the ellipse collection should be drawn. Uses the\n",
    "        current default axis when None.\n",
    "    \"\"\"\n",
    "    # Calculate the ellipse angles and bounding boxes using SVD.\n",
    "    U, s, _ = np.linalg.svd(C)\n",
    "    angles = np.degrees(np.arctan2(U[:, 1, 0], U[:, 0, 0]))\n",
    "    widths, heights = 2 * nsigmas * np.sqrt(s.T)\n",
    "    # Initialize colors.\n",
    "    color = colorConverter.to_rgba(color)\n",
    "    if filled:\n",
    "        # Use transparency to indicate relative weights.\n",
    "        ec = np.tile([color], (len(w), 1))\n",
    "        ec[:, -1] *= w\n",
    "        fc = np.tile([color], (len(w), 1))\n",
    "        fc[:, -1] *= w ** 2\n",
    "    # Data limits must already be defined for axis.transData to be valid.\n",
    "    axis = axis or plt.gca()\n",
    "    if outline is not None:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=4,\n",
    "            transOffset=axis.transData, facecolors='none', edgecolors=outline))\n",
    "    if filled:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=2,\n",
    "            transOffset=axis.transData, facecolors=fc, edgecolors=ec))\n",
    "    else:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=2.5,\n",
    "            transOffset=axis.transData, facecolors='none', edgecolors=color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_pairplot(data, w, mu, C, limits=None, entropy=False):\n",
    "    \"\"\"Display 2D projections of a Gaussian mixture model fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas DataFrame\n",
    "        N samples of D-dimensional data.\n",
    "    w : array\n",
    "        1D array of K relative weights for each ellipse. Must sum to one.\n",
    "    mu : array\n",
    "        Array of shape (K, 2) giving the 2-dimensional centroids of\n",
    "        each ellipse.\n",
    "    C : array\n",
    "        Array of shape (K, 2, 2) giving the 2 x 2 covariance matrix for\n",
    "        each ellipse.\n",
    "    limits : array or None\n",
    "        Array of shape (D, 2) giving [lo,hi] plot limits for each of the\n",
    "        D dimensions. Limits are determined by the data scatter when None.\n",
    "    \"\"\"\n",
    "    colnames = data.columns.values\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    if entropy:\n",
    "        n_components = len(w)\n",
    "        # Pick good colors to distinguish the different clusters.\n",
    "        cmap = ListedColormap(\n",
    "            sns.color_palette('husl', n_components).as_hex())\n",
    "        # Calculate the relative probability that each sample belongs to each cluster.\n",
    "        # This is equivalent to fit.predict_proba(X)\n",
    "        lnprob = np.zeros((n_components, N))\n",
    "        for k in range(n_components):\n",
    "            lnprob[k] = scipy.stats.multivariate_normal.logpdf(X, mu[k], C[k])\n",
    "        lnprob += np.log(w)[:, np.newaxis]\n",
    "        prob = np.exp(lnprob)\n",
    "        prob /= prob.sum(axis=0)\n",
    "        prob = prob.T\n",
    "        # Assign each sample to its most probable cluster.\n",
    "        labels = np.argmax(prob, axis=1)\n",
    "        color = cmap(labels)\n",
    "        if n_components > 1:\n",
    "            # Calculate the relative entropy (0-1) as a measure of cluster assignment ambiguity.\n",
    "            relative_entropy = -np.sum(prob * np.log(prob), axis=1) / np.log(n_components)\n",
    "            color[:, :3] *= (1 - relative_entropy).reshape(-1, 1)        \n",
    "    # Build a pairplot of the results.\n",
    "    fs = 5 * min(D - 1, 3)\n",
    "    fig, axes = plt.subplots(D - 1, D - 1, sharex='col', sharey='row',\n",
    "                             squeeze=False, figsize=(fs, fs))\n",
    "    for i in range(1, D):\n",
    "        for j in range(D - 1):\n",
    "            ax = axes[i - 1, j]\n",
    "            if j >= i:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            # Plot the data in this projection.\n",
    "            if entropy:\n",
    "                ax.scatter(X[:, j], X[:, i], s=5, c=color, cmap=cmap)\n",
    "                draw_ellipses(\n",
    "                    w, mu[:, [j, i]], C[:, [[j], [i]], [[j, i]]],\n",
    "                    color='w', outline='k', filled=False, axis=ax)\n",
    "            else:\n",
    "                ax.scatter(X[:, j], X[:, i], s=10, alpha=0.3, c='k', lw=0)\n",
    "                draw_ellipses(\n",
    "                    w, mu[:, [j, i]], C[:, [[j], [i]], [[j, i]]],\n",
    "                    color='red', outline=None, filled=True, axis=ax)\n",
    "            # Overlay the fit components in this projection.\n",
    "            # Add axis labels and optional limits.\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(colnames[i])\n",
    "                if limits: ax.set_ylim(limits[i])\n",
    "            if i == D - 1:\n",
    "                ax.set_xlabel(colnames[j])\n",
    "                if limits: ax.set_xlim(limits[j])\n",
    "    plt.subplots_adjust(hspace=0.02, wspace=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple wrapper that uses KMeans to initialize the relative probabilities to all be either zero or one, based on each sample's cluster assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_fit(data, n_components, nsteps, init='random', seed=123):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    p = np.zeros((n_components, N))\n",
    "    if init == 'kmeans':\n",
    "        # Use KMeans to divide the data into clusters.\n",
    "        fit = cluster.KMeans(n_clusters=n_components, random_state=gen, n_init=10).fit(data)\n",
    "        # Initialize the relative weights using cluster membership.\n",
    "        # The initial weights are therefore all either 0 or 1.\n",
    "        for k in range(n_components):\n",
    "            p[k, fit.labels_ == k] = 1\n",
    "    else:\n",
    "        # Assign initial relative weights in quantiles of the first feature.\n",
    "        # This is not a good initialization strategy, but shows how well\n",
    "        # GMM converges from a poor starting point.\n",
    "        x0 = X[:, 0]\n",
    "        edges = np.percentile(x0, np.linspace(0, 100, n_components + 1))\n",
    "        for k in range(n_components):\n",
    "            quantile = (edges[k] <= x0) & (x0 <= edges[k + 1])\n",
    "            p[k, quantile] = 1.\n",
    "    # Normalize relative weights.\n",
    "    p /= p.sum(axis=0)\n",
    "    # Perform an initial M step to initialize the component params.\n",
    "    w, mu, C = M_step(X, p)\n",
    "    # Loop over iterations.\n",
    "    for i in range(nsteps):\n",
    "        p = E_step(X, w, mu, C)\n",
    "        w, mu, C = M_step(X, p)\n",
    "    # Plot the results.\n",
    "    GMM_pairplot(data, w, mu, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this out on the 3D `blobs_data` and notice that it converges close to the correct solution after 8 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_data('https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/data/blobs_data.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_data = pd.read_hdf(locate_data('blobs_data.hf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0)\n",
    "GMM_fit(blobs_data, 3, nsteps=4)\n",
    "GMM_fit(blobs_data, 3, nsteps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is even faster if you use KMeans to initialize the relative weights (which is why most implementations do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0, init='kmeans')\n",
    "GMM_fit(blobs_data, 3, nsteps=1, init='kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A density estimator should provide a probability density function $P(\\vec{x})$ that is normalized over its feature space $\\vec{x}$\n",
    "$$\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) = 1 \\; .\n",
    "$$\n",
    "In this problem you will verify this normalization for KDE using two different numerical approaches for the integral.\n",
    "\n",
    "First, implement the function below to accept a 1D KDE fit object and estimate its normalization integral using the trapezoid rule with the specified grid. *Hint: the `np.trapz` function will be useful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grid_normalization(fit, xlo, xhi, ngrid):\n",
    "    \"\"\"Check 1D denstity estimator fit result normlization using grid quadrature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.KernelDensity fit object\n",
    "        Result of fit to 1D dataset.\n",
    "    xlo : float\n",
    "        Low edge of 1D integration range.\n",
    "    xhi : float\n",
    "        High edge of 1D integration range.\n",
    "    ngrid : int\n",
    "        Number of equally spaced grid points covering [xlo, xhi],\n",
    "        including both end points.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']).values)\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 5), 3) == 1.351\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 10), 3) == 1.019\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 20), 3) == 0.986\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x2']).values)\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 5), 3) == 1.108\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 10), 3) == 0.993\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 20), 3) == 0.971\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x1']).values)\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 5), 3) == 1.311\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 10), 3) == 0.954\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 20), 3) == 1.028\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 100), 3) == 1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to estimate a multidimensional fit normalization using [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration):\n",
    "$$\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) \\simeq \\frac{V}{N_{mc}}\\, \\sum_{j=1}^{N_{mc}} P(\\vec{x}_j) = V \\langle P\\rangle \\; ,\n",
    "$$\n",
    "where the $\\vec{x}_j$ are uniformly distributed over the integration domain and $V$ is the integration domain volume. Note that `trapz` gives more accurate results for a fixed number of $P(\\vec{x})$ evaluations, but MC integration is much easier to generalize to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mc_normalization(fit, xlo, xhi, nmc, seed=123):\n",
    "    \"\"\"Check denstity estimator fit result normlization using MC integration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.KernelDensity fit object\n",
    "        Result of fit to arbitrary dataset of dimension D.\n",
    "    xlo : array\n",
    "        1D array of length D with low limits of integration domain along each dimension.\n",
    "    xhi : array\n",
    "        1D array of length D with high limits of integration domain along each dimension.\n",
    "    nmc : int\n",
    "        Number of random MC integration points within the domain to use.\n",
    "    \"\"\"\n",
    "    xlo = np.asarray(xlo)\n",
    "    xhi = np.asarray(xhi)\n",
    "    assert xlo.shape == xhi.shape\n",
    "    assert np.all(xhi > xlo)\n",
    "    D = len(xlo)\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # Use gen.uniform() in your solution, not gen.rand(), for consistent random numbers.\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A correct solution should pass these tests.\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']).values)\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10), 3) == 1.129\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 100), 3) == 1.022\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 1000), 3) == 1.010\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10000), 3) == 0.999\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x2']).values)\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10), 3) == 1.754\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 100), 3) == 1.393\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 1000), 3) == 0.924\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10000), 3) == 1.019\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.values)\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10), 3) == 2.797\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 100), 3) == 0.613\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 1000), 3) == 1.316\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10000), 3) == 1.139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "© Copyright 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
