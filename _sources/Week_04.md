# <span style="color: blue;"><b>Bayesian Statistics II</b></span>

## *Overview*
You will learn the basics of Bayesian Statistics

## *Goals*
* Learn about Baysian Model Selection
* Learn about Variational Inference
* Learn about Optimization and Stochastic Gradient Descent
* Learn about the Cross Validation method of model training, validation and testing


## *Lecture Materials*
* [Slides](https://docs.google.com/presentation/d/18bft9_CiBLjjBy0MHvT_vN7E95kfakvhm_7d7WKHXyY/edit?usp=sharing)
* {doc}`lectures/ModelSelection`
* {doc}`lectures/VariationalInference`
* {doc}`lectures/Optimization`
* {doc}`lectures/CrossValidation`

## *Homework Assignment*
* {doc}`homework/Homework_04`

## *Supplemental Readings*
* [C. Maes, <i>An introduction to the theory of Markov processes mostly for physics students</i>](https://fys.kuleuven.be/itf/staff/christ/files/pdf/pub/markovlectures2015.pdf)
* [Example of dependence without correlation](https://en.wikipedia.org/wiki/Uncorrelated_random_variables#Example_of_dependence_without_correlation)
* [Conditional Independence](https://en.wikipedia.org/wiki/Conditional_independence)
* [Inverse Problem](https://en.wikipedia.org/wiki/Inverse_problem)
* [Brownian Motion](https://en.wikipedia.org/wiki/Brownian_motion)
* [Hamiltonian Mechanics](https://en.wikipedia.org/wiki/Hamiltonian_mechanics)
* [Cannonical Distribution](https://en.wikipedia.org/wiki/Canonical_ensemble)
* [Hamiltonian MC](http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html)
* [Autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)
* [Convex Functions](https://en.wikipedia.org/wiki/Convex_function)
* [Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen's_inequality)
* [Finite Difference Equations](https://en.wikipedia.org/wiki/Finite_difference)
* [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)
* [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function)
* [Nelder-Mead method](https://en.wikipedia.org/wiki/Nelderâ€“Mead_method)
* [Conjugate Gradient Method](https://en.wikipedia.org/wiki/Conjugate_gradient_method)
* [Newton's CG method](https://en.wikipedia.org/wiki/Newton's_method_in_optimization)
* [Powell's method](https://en.wikipedia.org/wiki/Powell's_method)
* [BFGS method](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)
* [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
* [Adam Optimizer](https://arxiv.org/abs/1412.6980)
* [Softmax Function](https://en.wikipedia.org/wiki/Softmax_function)