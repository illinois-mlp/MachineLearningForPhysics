{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 01: Introduction to Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess\n",
    "from sklearn import cluster, decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for Getting, Loading and Locating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_data(url: str):\n",
    "    local_path = './tmp_data'\n",
    "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
    "    rc = None\n",
    "    while rc is None:\n",
    "      line = p.stderr.readline().strip('\\n')\n",
    "      if len(line) > 0:\n",
    "        print(line)\n",
    "      rc = p.poll()\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.einsum` to evaluate the tensor expression $g^{il} \\Gamma^m_{ki} x^k$ which arises in [contravariant derivatives in General Relativity](https://en.wikipedia.org/wiki/Christoffel_symbols#Covariant_derivatives_of_tensors).  Note we are using the GR convention that repeated indices (k,l) are summed over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_expr(g, Gamma, x, D=4):\n",
    "    \"\"\"Evaluate the tensor expression above.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g : array\n",
    "        Numpy array of shape (D, D)\n",
    "    Gamma : array\n",
    "        Numpy array of shape (D, D, D)\n",
    "    x : array\n",
    "        Numpy array of shape (D,)\n",
    "    D : int\n",
    "        Dimension of input tensors.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Numpy array of shape (D, D) that evaluates the tensor expression.\n",
    "    \"\"\"\n",
    "    assert g.shape == (D, D)\n",
    "    assert Gamma.shape == (D, D, D)\n",
    "    assert x.shape == (D,)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "g = np.arange(4 ** 2).reshape(4, 4)\n",
    "Gamma = np.arange(4 ** 3).reshape(4, 4, 4)\n",
    "x = np.arange(4)\n",
    "y = tensor_expr(g, Gamma, x)\n",
    "assert np.array_equal(\n",
    "    y,\n",
    "    [[ 1680,  3984,  6288,  8592], [ 1940,  4628,  7316, 10004],\n",
    "     [ 2200,  5272,  8344, 11416], [ 2460,  5916,  9372, 12828]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal (aka Gaussian) distribution is one of the fundamental probability densities that we will encounter often.\n",
    "\n",
    "Implement the function below using `np.random.multivariate_normal` to generate random samples from an arbitrary multidimensional normal distribution, for a specified random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal(mu, C, n, seed=123):\n",
    "    \"\"\"Generate random samples from a normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : array\n",
    "        1D array of mean values of length N.\n",
    "    C : array\n",
    "        2D array of covariances of shape (N, N). Must be a positive-definite matrix.\n",
    "    n : int\n",
    "        Number of random samples to generate.\n",
    "    seed : int\n",
    "        Random number seed to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        2D array of shape (n, N) where each row is a random N-dimensional sample.\n",
    "    \"\"\"\n",
    "    assert len(mu.shape) == 1, 'mu must be 1D.'\n",
    "    assert C.shape == (len(mu), len(mu)), 'C must be N x N.'\n",
    "    assert np.all(np.linalg.eigvals(C) > 0), 'C must be positive definite.'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "mu = np.array([-1., 0., +1.])\n",
    "C = np.identity(3)\n",
    "C[0, 1] = C[1, 0] = -0.9\n",
    "Xa = generate_normal(mu, C, n=500, seed=1)\n",
    "Xb = generate_normal(mu, C, n=500, seed=1)\n",
    "Xc = generate_normal(mu, C, n=500, seed=2)\n",
    "assert np.array_equal(Xa, Xb)\n",
    "assert not np.array_equal(Xb, Xc)\n",
    "X = generate_normal(mu, C, n=2000, seed=3)\n",
    "assert np.allclose(np.mean(X, axis=0), mu, rtol=0.001, atol=0.1)\n",
    "assert np.allclose(np.cov(X, rowvar=False), C, rtol=0.001, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a generated 3D dataset using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d():\n",
    "    mu = np.array([-1., 0., +1.])\n",
    "    C = np.identity(3)\n",
    "    C[0, 1] = C[1, 0] = -0.9\n",
    "    X = generate_normal(mu, C, n=2000, seed=3)\n",
    "    df = pd.DataFrame(X, columns=('x0', 'x1', 'x2'))\n",
    "    sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about [correlation and covariance](https://en.wikipedia.org/wiki/Covariance_and_correlation), then implement the function below to create a 2x2 covariance matrix given values of $\\sigma_x$, $\\sigma_y$ and the correlation coefficient $\\rho$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_covariance(sigma_x, sigma_y, rho):\n",
    "    \"\"\"Create and return the 2x2 covariance matrix specified by the input args.\n",
    "    \"\"\"\n",
    "    assert (sigma_x > 0) and (sigma_y > 0), 'sigmas must be > 0.'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.array_equal(create_2d_covariance(1., 1.,  0.0), [[1.,  0.], [ 0., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1.,  0.0), [[4.,  0.], [ 0., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1.,  0.5), [[4.,  1.], [ 1., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1., -0.5), [[4., -1.], [-1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell that uses your `create_2d_covariance` and `generate_normal` functions to compare the 2D normal distributions with $\\rho = 0$ (blue), $\\rho = +0.9$ (red) and $\\rho = -0.9$ (green):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rhos():\n",
    "    mu = np.zeros(2)\n",
    "    sigma_x, sigma_y = 2., 1.\n",
    "    for rho, cmap in zip((0., +0.9, -0.9), ('Blues', 'Reds', 'Greens')):\n",
    "        C = create_2d_covariance(sigma_x, sigma_y, rho)\n",
    "        X = generate_normal(mu, C, 10000)\n",
    "        sns.kdeplot(x=X[:, 0], y=X[:, 1], cmap=cmap)\n",
    "    plt.xlim(-4, +4)\n",
    "    plt.ylim(-2, +2)\n",
    "        \n",
    "compare_rhos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm) is used to implement many machine learning methods, including several we have already studied like K-means and soon to be studied (e.g. factor analysis and weighted PCA.)\n",
    "\n",
    "The basic idea of EM is to optimize a goal function that depends on two disjoint sets of parameters by alternately updating one set and then the other, using a scheme that is guaranteed to improve the goal function (although generally to a local rather than global optimum). The alternating updates are called the E-step and M-step.\n",
    "\n",
    "The K-means is one of the simplest uses of EM, so is a good way to get some hands-on experience.\n",
    "\n",
    "Implement the function below to perform a K-means E-step. Hint: you might find [np.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) and [np.argmin](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.argmin.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, centers):\n",
    "    \"\"\"Perform a K-means E-step.\n",
    "    \n",
    "    Assign each sample to the cluster with the nearest center, using the\n",
    "    Euclidean norm to measure distance between a sample and a cluster center.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    centers : array with shape (n, D)\n",
    "        Centers of the the n clusters in D dimensions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    integer array with shape (N,)\n",
    "        Cluster index of each sample, in the range 0 to n-1.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    n = len(centers)\n",
    "    assert centers.shape[1] == D\n",
    "    indices = np.empty(N, int)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = gen.normal(size=(100, 2))\n",
    "centers = np.array([[0., 0.], [0., 10.]])\n",
    "X[50:] += centers[1]\n",
    "indices = E_step(X, centers)\n",
    "assert np.all(indices[:50] == 0)\n",
    "assert np.all(indices[50:] == 1)\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = gen.normal(size=(20, 2))\n",
    "centers = gen.uniform(size=(5, 2))\n",
    "indices = E_step(X, centers)\n",
    "assert np.array_equal(indices, [4, 1, 4, 4, 1, 0, 1, 0, 2, 1, 2, 4, 0, 1, 0, 1, 0, 1, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to perform a K-means M-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(X, indices, n):\n",
    "    \"\"\"Perform a K-means M-step.\n",
    "    \n",
    "    Calculate the center of each cluster as the geometric mean of its assigned samples.\n",
    "    \n",
    "    The centers of any clusters without any assigned samples should be set to the origin.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    indices : integer array with shape (N,)\n",
    "        Cluster index of each sample, in the range 0 to n-1.\n",
    "    n : int\n",
    "        Number of clusters.  Must be <= N.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array with shape (n, D)\n",
    "        Centers of the the n clusters in D dimensions.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert indices.shape == (N,)\n",
    "    assert n <= N\n",
    "    centers = np.zeros((n, D))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() \n",
    "        \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = np.ones((20, 2))\n",
    "indices = np.zeros(20, int)\n",
    "centers = M_step(X, indices, 1)\n",
    "assert np.all(centers == 1.)\n",
    "\n",
    "n = 5\n",
    "indices = gen.randint(n, size=len(X))\n",
    "centers = M_step(X, indices, n)\n",
    "assert np.all(centers == 1.)\n",
    "\n",
    "X = gen.uniform(size=X.shape)\n",
    "centers = M_step(X, indices, n)\n",
    "assert np.allclose(\n",
    "    np.round(centers, 3),\n",
    "    [[ 0.494,  0.381], [ 0.592,  0.645],\n",
    "     [ 0.571,  0.371], [ 0.234,  0.634],\n",
    "     [ 0.250,  0.386]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented the core of the K-means algorithm.  Try it out with this simple wrapper, which makes a scatter plot of the first two columns after each iteration. The sklearn wrapper combines the result of several random starting points and has other refinements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_fit(data, n_clusters, nsteps, seed=123):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    assert n_clusters <= N\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # Pick random samples as the initial centers.\n",
    "    shuffle = gen.permutation(N)\n",
    "    centers = X[shuffle[:n_clusters]]\n",
    "    # Perform an initial E step to assign samples to clusters.\n",
    "    indices = E_step(X, centers)\n",
    "    # Loop over iterations.\n",
    "    for i in range(nsteps):\n",
    "        centers = M_step(X, indices, n_clusters)\n",
    "        indices = E_step(X, centers)\n",
    "    # Plot the result.\n",
    "    cmap = np.array(sns.color_palette())\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=cmap[indices % len(cmap)])\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker='+', c='k', s=400, lw=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out on some randomly generated 2D data with 3 separate clusters (using the handy [make_blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X, _ = make_blobs(n_samples=500, n_features=2, centers=[[-3,-3],[0,3],[3,-3]], random_state=gen)\n",
    "data = pd.DataFrame(X, columns=('x0', 'x1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple test, you should find a good solution after two iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_fit(data, n_clusters=3, nsteps=0);\n",
    "KMeans_fit(data, n_clusters=3, nsteps=1);\n",
    "KMeans_fit(data, n_clusters=3, nsteps=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 4</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_data('https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/data/spectra_data.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_data = pd.read_hdf(locate_data('spectra_data.hf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA method of dimensionality reduction first calculates an exact linear decomposition (up to round off error),\n",
    "then trims rows and columns to the desired number of latent variables.  In this problem, you will explore how PCA is implemented. The tricky linear algebra is already implemented in [numpy.linalg](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html), but it is still a challenge to keep all the notation and conventions self consistent.\n",
    "\n",
    "The input data $X$ is provided as an $N\\times D$ (samples, features) matrix. In the following we assume that each feature is centered on zero (otherwise, calculate and subtract the $\\mu_j$, then add them back later),\n",
    "\n",
    "$$ \\Large\n",
    "\\mu_j = \\sum_i X_{ij} = 0 \\;.\n",
    "$$\n",
    "\n",
    "There are three equivalent methods for performing the initial exact decomposition:\n",
    "\n",
    "1. Calculate the $D\\times D$ [sample covariance matrix](https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Sample_covariance)\n",
    "\n",
    "$$ \\Large\n",
    "C \\equiv \\frac{1}{N-1}\\,X^T X \\;.\n",
    "$$\n",
    "\n",
    "then find its eigenvalue decomposition:\n",
    "\n",
    "$$ \\large\n",
    "C = Q^T \\Lambda Q\n",
    "$$\n",
    "\n",
    "where $\\Lambda$ is a diagonal $D\\times D$ matrix of eigenvalues and the rows of the orthogonal $D\\times D$ matrix $Q$ are the corresponding eigenvectors.\n",
    "\n",
    "2. Calculate the $N\\times N$ matrix of dot products between samples:\n",
    "\n",
    "$$ \\Large\n",
    "D \\equiv \\frac{1}{N-1}\\,X X^T \\;,\n",
    "$$\n",
    "\n",
    "then find its eigenvalue decomposition, where now $Q$ and $\\Lambda$ are $N\\times N$ matrices.\n",
    "\n",
    "3. Find the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular-value_decomposition) of $X$\n",
    "\n",
    "$$ \\Large\n",
    "X = U S V \\quad \\Rightarrow \\quad C = \\frac{1}{N-1}\\,V^T S^2 V \\;,\n",
    "$$\n",
    "\n",
    "where $S$ is a diagonal $K\\times K$ matrix of *singular values*, $U$ is $N\\times K$ and $V$ is $K\\times D$, with\n",
    "$K = \\min(N, D)$. The notation above is chosen to match [np.linalg.svd](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html) which you will use below.\n",
    "\n",
    "The computational cost of each method depends differently on the values of $N$ and $D$, so the most efficient method will depend on the shape of the input data $X$. There are also numerical considerations: the matrices $C$ and $D$ should be [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix) in order to guarantee positive eigenvalues, but this will not be true for $C$ if $N < D$ or for $D$ if $N > D$.\n",
    "\n",
    "Implement the function below to calculate the eigenvectors and eigenvalues of a square input matrix using a suitable function from [np.linalg](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html).  The results should be sorted in order of decreasing eigenvalues, as needed by PCA. Hint: `M[::-1]` reverses the rows of a 2D array `M` (more info [here](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#indexing-slicing-and-iterating))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigensolve(M):\n",
    "    \"\"\"Calculate eigenvectors and eigenvalues of a square symmetric matrix.\n",
    "    \n",
    "    Results are sorted by decreasing eigenvalue. The rows (not columns) of the\n",
    "    returned eigenvector array are the normalized eigenvectors of M.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M : 2D array\n",
    "        N x N symmetric square matrix to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple of arrays (eigenvalues, eigenvectors) with eigenvalues decreasing and\n",
    "        eigenvector[i] corresponding to eigenvalue[i].  Eigenvalues should have the\n",
    "        shape (N,) and eigenvectors should have the shape (N, N).\n",
    "    \"\"\"\n",
    "    assert len(M.shape) == 2\n",
    "    nrows, ncols = M.shape\n",
    "    assert nrows == ncols\n",
    "    assert np.all(M.T == M)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to check work\n",
    "def checkEigens(evals: np.ndarray, evecs: np.ndarray, covariance: np.ndarray, knownEvals: np.ndarray, knownEvecs: np.ndarray):\n",
    "    assert np.allclose(covariance, evecs.T.dot(np.diag(evals).dot(evecs)))\n",
    "    assert np.allclose(\n",
    "        np.round(evals, 5),\n",
    "        knownEvals)\n",
    "    assert np.allclose(\n",
    "        np.round(np.abs(evecs), 3),\n",
    "        np.abs(knownEvecs)\n",
    "    )\n",
    "    #Accounting for direction\n",
    "    for calcRow, knownRow in zip(np.round(evecs, 3), knownEvecs):\n",
    "        assert np.allclose(calcRow, knownRow) or np.allclose(-1 * calcRow, knownRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "C = np.diag(np.arange(1., 5.))\n",
    "evals, evecs = eigensolve(C)\n",
    "\n",
    "checkEigens(evals, evecs, C, \n",
    "    knownEvals=[4, 3, 2, 1],\n",
    "    knownEvecs=[[ 0.,  0.,  0.,  1.],\n",
    "     [ 0.,  0.,  1.,  0.],\n",
    "     [ 0.,  1.,  0.,  0.],\n",
    "     [ 1.,  0.,  0.,  0.]]\n",
    ")\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "N, D = 4, 3\n",
    "X = gen.uniform(size=(N, D))\n",
    "X -= np.mean(X, axis=0)\n",
    "C = np.dot(X.T, X) / (N - 1)\n",
    "evals, evecs = eigensolve(C)\n",
    "checkEigens(evals, evecs, C,\n",
    "    knownEvals=[ 0.08825,  0.0481 ,  0.01983],\n",
    "    knownEvecs=[[-0.787, -0.477,  0.391],\n",
    "     [-0.117,  0.737,  0.665],\n",
    "     [-0.606,  0.478, -0.636]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below to calculate the same quantities using the [SVD method](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html) directly on $X$ instead of solving the eigenvalue problem for the sample covariance.  Hint: pay attention to the `full_matrices` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdsolve(X):\n",
    "    \"\"\"Calculate eigenvectors and eigenvalues of the sample covariance of X.\n",
    "\n",
    "    Results are sorted by decreasing eigenvalue. The rows (not columns) of the\n",
    "    returned eigenvector array are the normalized eigenvectors of M.\n",
    "\n",
    "    Uses the SVD method directly on X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 2D array\n",
    "        N x D matrix to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple of arrays (eigenvalues, eigenvectors) with eigenvalues decreasing and\n",
    "        eigenvector[i] corresponding to eigenvalue[i].  Eigenvalues should have the\n",
    "        shape (K,) and eigenvectors should have the shape (K, D) with K=min(N, D).\n",
    "    \"\"\"\n",
    "    assert len(X.shape) == 2\n",
    "    N, D = X.shape\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "N, D = 4, 3\n",
    "X = gen.uniform(size=(N, D))\n",
    "X -= np.mean(X, axis=0)\n",
    "evals, evecs = svdsolve(X)\n",
    "C = np.dot(X.T, X) / (N - 1)\n",
    "checkEigens(evals, evecs, C,\n",
    "    knownEvals= [ 0.08825,  0.0481 ,  0.01983],\n",
    "    knownEvecs= [[-0.787, -0.477,  0.391],\n",
    "     [ 0.117, -0.737, -0.665],\n",
    "     [-0.606,  0.478, -0.636]]\n",
    ")\n",
    "\n",
    "N, D = 3, 4\n",
    "X = gen.uniform(size=(N, D))\n",
    "X -= np.mean(X, axis=0)\n",
    "evals, evecs = svdsolve(X)\n",
    "C = np.dot(X.T, X) / (N - 1)\n",
    "# Works on Google Colab (Jan '25)\n",
    "checkEigens(evals, evecs, C,\n",
    "    knownEvals= [ 0.23688,  0.03412,  0.     ],\n",
    "    knownEvecs= [[ 0.368, 0.874,  0.316, -0.041],\n",
    "     [-0.752, 0.178,  0.313, -0.553],\n",
    "     [-0.516, 0.422, -0.496,  0.556]]\n",
    ")\n",
    "# Note: I have seen some platform dependence to the numerics in the above assess. Specifically, the third row in knownEvecs gives something different on my mac and I have to use the one below for the assert to pass.\n",
    "#checkEigens(evals, evecs, C,\n",
    "#    knownEvals= [ 0.23688,  0.03412,  0.     ],\n",
    "#    knownEvecs= [[ 0.368, 0.874,  0.316, -0.041],\n",
    "#     [-0.752, 0.178,  0.313, -0.553],\n",
    "#     [-0.521, 0.417, -0.472,  0.576]]\n",
    "#)\n",
    "# If you run into trouble like this, include the line for the grader:\n",
    "#print(evals,evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the eigenvectors found by the two methods might differ by an overall sign, but both sets of eigenvectors are orthonormal, so equally valid.\n",
    "\n",
    "The following simple driver code shows how to build a PCA fit from your functions (but the [sklearn driver](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) has a lot more options):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_fit(data, n_components=2):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    print('N,D = {},{}'.format(N, D))\n",
    "    K = min(N, D)\n",
    "    assert n_components <= K\n",
    "    # Subtract mean value of each feature.\n",
    "    mu = np.mean(X, axis=0)\n",
    "    Xc = X - mu\n",
    "    # Select the method based on N, D.\n",
    "    if N > 2 * D:\n",
    "        print('Using method 1')\n",
    "        evals, M = eigensolve(np.dot(Xc.T, Xc) / (N - 1))\n",
    "        assert evals.shape == (D,) and M.shape == (D, D)\n",
    "    elif D > 2 * N:\n",
    "        print('Using method 2')\n",
    "        evals, M = eigensolve(np.dot(Xc, Xc.T) / (N - 1))\n",
    "        assert evals.shape == (N,) and M.shape == (N, N)\n",
    "        # Eigenvectors are now M = U.T of the SVD.  Convert to M = V.\n",
    "        # Use abs(evals) since smallest values might be < 0 due to numerical errors.\n",
    "        M = np.dot(np.dot(np.diag(np.abs(evals) ** -0.5), M), Xc) / np.sqrt(N - 1)\n",
    "    else:\n",
    "        print('Using method 3')\n",
    "        evals, M = svdsolve(Xc)\n",
    "        assert evals.shape == (K,) and M.shape == (K, D)\n",
    "    # Calculate Y such that Xc = Y M.\n",
    "    Y = np.dot(Xc, M.T)\n",
    "    # Trim to latent variable subspace.\n",
    "    Y = Y[:, :n_components]\n",
    "    M = M[:n_components]\n",
    "    # Calculate reconstructed samples.\n",
    "    Xr = np.dot(Y, M) + mu\n",
    "    # Plot some samples and their reconstructions.\n",
    "    for i,c in zip((0, 6, 7), sns.color_palette()):\n",
    "        plt.plot(X[i], '.', c=c, ms=5)\n",
    "        plt.plot(Xr[i], '-', c=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this driver in each regime by varying the number of features used from `spectra_data` with $N$, $D$ = 200, 500:\n",
    "- $N \\gg D$: method 1\n",
    "- $N \\ll D$: method 2\n",
    "- $N \\simeq D$: method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_fit(spectra_data.iloc[:, 190:230], n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_fit(spectra_data, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_fit(spectra_data.iloc[:, 120:320], n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below to compare clusters found in a full dataset with those found in a reduced dataset (lower-dimension latent space). Use KMeans for clustering and PCA to obtained the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clusters(data, n_clusters, n_components, seed=123):\n",
    "    \"\"\"Compare clusters in the full vs reduced feature space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas DataFrame\n",
    "        Dataset to analyze of shape (N, D).\n",
    "    n_clusters : int\n",
    "        Number of clusters to find using KMeans.\n",
    "    n_components : int\n",
    "        Number of dimensions of the reduced latent variable space\n",
    "        to calculate using PCA.\n",
    "    seed : int\n",
    "        Random number seed used for reproducible KMeans and PCA.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple (labels1, labels2) of 1D integer arrays of length N,\n",
    "        with values 0,1,...,(n_clusters-1).\n",
    "    \"\"\"\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to test your function and comment (in the last markdown cell) on how the full vs reduced clusters compare and whether there is an advantage to clustering in reduced dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1, labels2 = compare_clusters(spectra_data, 4, 2)\n",
    "fig, ax = plt.subplots(4, 2, figsize=(8, 12))\n",
    "for i in range(4):\n",
    "    sel1 = np.where(labels1 == i)[0]\n",
    "    sel2 = np.where(labels2 == i)[0]\n",
    "    for j in range(4):\n",
    "        ax[i, 0].plot(spectra_data.iloc[sel1[j]], 'r.', ms=5)\n",
    "        ax[i, 1].plot(spectra_data.iloc[sel2[j]], 'b.', ms=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing your results above, comment in the empty markdown cell below on how the full vs reduced clusters compare and whether there is any potential advantage to clustering in reduced dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AnsStart.svg\" width=200 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-mlp/MachineLearningForPhysics/main/img/AnsEnd.svg\" width=200 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "Â© Copyright 2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
